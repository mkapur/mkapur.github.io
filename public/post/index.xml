<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Maia Sosa Kapur</title>
    <link>https://mkapur.netlify.app/post/</link>
      <atom:link href="https://mkapur.netlify.app/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Jul 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mkapur.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://mkapur.netlify.app/post/</link>
    </image>
    
    <item>
      <title>The Simulation Pinball Machine</title>
      <link>https://mkapur.netlify.app/post/simulation-pinball/</link>
      <pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/simulation-pinball/</guid>
      <description>
&lt;script src=&#34;https://mkapur.netlify.app/post/simulation-pinball/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-pinball-machine&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The pinball machine&lt;/h1&gt;
&lt;p&gt;Simulation is the bread-and-butter of my worklife. I use it to create data for research questions, to explore the behavior of the models I build to assess fishery populations, and to convince myself whether certain truisms are really true. I don’t hope to give a full review of fisheries simulation here, but wanted to make a note about how two of the key “switches” we regularly use within the simulation context to represent process error and fishing impacts interact.&lt;/p&gt;
&lt;p&gt;Specifically, I think of a well-built simulation like a pinball machine: the ball is your state-space self running downhill (through time), and the various switches/lights/buttons are all the parameters and model structures that you’ve established in hopes of shooting that ball on a nice, high-scoring trajectory. A good simulation will reliably 1) follow the same trajectory each time things are run the same way, and 2) only vary among trajectories in the manner you’ve asked it to.&lt;/p&gt;
&lt;p&gt;This is easier said than done, if you haven’t given careful thought to how to represent uncertainty in your simulation framework. There also is a nuanced difference between the type of simulation you’d build for &lt;em&gt;forecasting&lt;/em&gt; (aka projecting) a population, versus &lt;em&gt;bootstrapping&lt;/em&gt; (or “resampling”) a historical population. There’s also what I consider a hybrid of the two, where you will use the data &amp;amp; parameters corresponding to a historical population to simulate a new, historical trajectory, which is otherwise independent of the original data.&lt;/p&gt;
&lt;p&gt;This post concerns the fisheries scientest more-or-less fluent in Stock Synthesis but also eager to generate their own simulated pouplations (i.e., in TMB). I will admit up-front that I have not much experience with &lt;code&gt;ss3sim&lt;/code&gt; but will be discussing some principles of Synthesis population generation here. Please tag me on Twitter or email me below if you spot any gross misrepresentations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background Reading&lt;/h1&gt;
&lt;p&gt;Some helpful and can’t-miss resources on this topic are the following:&lt;/p&gt;
&lt;p&gt;Methot, R.D., Wetzel, C.R., 2013. Stock synthesis: A biological and statistical framework for fish stock assessment and fishery management. Fish. Res. 142, 86–99. &lt;a href=&#34;https://doi.org/10.1016/j.fishres.2012.10.012&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.fishres.2012.10.012&lt;/a&gt;, &lt;strong&gt;Particularly&lt;/strong&gt; the section on p94 regarding the bootstrap routine. This paragraph makes it clear that if you’re invoking the bootstrap functionality within SS (where a new data file gets written at the bottom of &lt;code&gt;data.ss_new&lt;/code&gt;), these bootstrapped data series are dependent on the &lt;em&gt;expected&lt;/em&gt; values from your original assessment model. This is done purposefully, to not propagate data conflicts, autocorrelations, and/or residual patterns in your original data into the new simulated data. However, it’s pretty important that your original model actually fits the data pretty well, if you expect to turn around and use that original model to do some sort of re-estimation on the bootstrapped data. Related to the discussion below, this paper mentions that folks will often combine the SS3 Bootstrap routine with other sampling methods to introduce both process and observation error into their simulations (see below).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This &lt;a href=&#34;https://www.google.com/search?q=tmb+simulate&amp;amp;rlz=1C1CHBF_enUS814US814&amp;amp;oq=tmb+simulate&amp;amp;aqs=chrome..69i57j69i64.2880j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34;&gt;simple&lt;/a&gt; overview&lt;/strong&gt; of &lt;code&gt;TMB::simulate()&lt;/code&gt;, where the authors remind us the effects of estimation error even in a dead-simple linear regression model. This is why a perfectly-specified estimation model is not going to exactly recover your index of abundance! Rick himself has a few thoughts about why the distribution of parameter estimates from bootstrapped models is narrower than one might expect from the original data on the vLab site &lt;a href=&#34;https://vlab.noaa.gov/web/stock-synthesis/public-forums/-/message_boards/message/5517207&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the old-ish paper that inspired this blog post:
Magnusson, A., Punt, A.E. and Hilborn, R. (2013), Measuring uncertainty in fisheries stock assessment: the delta method, bootstrap, and MCMC. Fish and Fisheries, 14: 325-342. &lt;a href=&#34;https://doi.org/10.1111/j.1467-2979.2012.00473.x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/j.1467-2979.2012.00473.x&lt;/a&gt;. This paper does a great job talking through how to represent uncertainty in simulated data. Figure 2 in this paper is what got me thinking about the various buttons of the pinball machine, and how changing them can provide subtly different outcomes in your simulated populations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quick-refresherglossary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quick refresher/glossary&lt;/h1&gt;
&lt;p&gt;Important to our discussion today, especially in the context of state-space model simulation, are the ideas of &lt;em&gt;process&lt;/em&gt; vs &lt;em&gt;observation&lt;/em&gt; error. My non-technical way of thinking about this is a paraphrase of Andre’s explanation:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Process error&lt;/em&gt; is the vague, multi-forced, often unexplained system variability that makes the world diverse, stochastic and beautiful. The fact that some recruitment years are big and others are small – the basic idea behind &lt;a href=&#34;https://maiakapur.netlify.app/post/recdevs/how-rec-devs/&#34;&gt;recruitment deviates&lt;/a&gt; – is often equated with process error. It’s random (but might follow an underlying distribution). &lt;strong&gt;Process error is the variation you would see if you could observe the system perfectly&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Observation error is basically the goofs introduced by your faulty instruments, your sleepy technician, your failings as a mortal human. It’s the discrepancy between the truth in the water and what you write down in your lab notebook because of your inability to take perfect measurements; this is a simple fact of reality (see: Uncertainty Principle). &lt;strong&gt;Observation error is the variation obtained as a result of imperfect tools&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Other key words I’m using today:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simulation&lt;/strong&gt;: Inventing data in a systematic way, using pre-set functional forms and statistical distributions to represent a set of natural processes and the collection of information from them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Forecasting/Projecting&lt;/strong&gt;: Taking information (parameters, time series) from an existing model (such as an assessment model), and marching that population forward in time. Requires, at minimum, some idea of how that population will reproduce (recruitment deviates) and die (natural mortality, and perhaps catches) in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;: Fisheries folks use this somewhat loosely to refer to the process of generating new datasets, typically over a historical time period, from an existing model. Generally, these approaches look at your parameter distributions, the expected values from your datasets, and how you’ve specified error within them to create a new batch of datasets and the accompanying population. This is subtly different from forecasting, because we don’t necessarily need to tell the model anything about the future; it can instead use the historical recruitment deviates and catch information to drive the population – but more on this shortly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;le-confusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Le Confusion&lt;/h1&gt;
&lt;p&gt;Let’s take a look at Figure 2 from Magnusson et. al. The text explains that these populations were simulating using totally random recruitment deviations, which you can see on the top-right:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;FIG2.PNG&#34; alt=&#34;Magnusson et. al. 2013&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Magnusson et. al. 2013
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Simulating recruitment deviates, either for the past or future, from a lognormal distribution with &lt;span class=&#34;math inline&#34;&gt;\(\sigma_r\)&lt;/span&gt; from your base model is a good call. This is how we introduce &lt;em&gt;process error&lt;/em&gt; into the system, by pretending that the ups &amp;amp; downs in population productivity happen at random, with a certain level of spread. (Technically, a correct way to do this is to simulate &lt;span class=&#34;math inline&#34;&gt;\(\tilde R_y \sim N(-\sigma_R^2/2,\sigma_R))\)&lt;/span&gt; so that the resultant vector is bias-corrected). Lots of cool studies have been and will be done by examining what happens if that process error (those rec-devs) are instead influenced by, say, a temperature index. But anyways, Arni’s paper does it the random way.&lt;/p&gt;
&lt;p&gt;Now, if you had set up a random cloud of recruitment deviates, how would you expect the SSB trajectory to look? (&lt;a href=&#34;https://maiakapur.netlify.app/post/generals/how-studied-general/&#34;&gt;Phd Generals&lt;/a&gt; question alert). You might be inclined to say “it would depend on the catches”. That is true in an absolute sense, and thinking carefully about this dependency will help you understand the different ways of setting up your simulation to do what you’d like.&lt;/p&gt;
&lt;p&gt;I titled this section “Le Confusion” because an inattentive viewer of Fig. 2 (cough) might not get why the SSB trajectory on the bottom-right looks like a smooth, bendy trend, given the fact that the rec devs are a random cloud. Shouldn’t it be a cloud as well?&lt;/p&gt;
&lt;p&gt;After all (brute coding ahead):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## population setup
sigmaR &amp;lt;- 0.4
init_pop &amp;lt;- 1000; 
rec_mult &amp;lt;- 0.2 ## let&amp;#39;s pretend 20% of the population reproduces
natM &amp;lt;- 0.2

## make fifty, 50-year population trajectories
pop_big &amp;lt;- matrix(NA, nrow = 50, ncol = 50)
for(i in 1:50){
  rec_devs &amp;lt;- rnorm(50,mean = -sigmaR^2/2,sigmaR) ## simulate our devs as a cloud
  pop = NULL; pop[1] &amp;lt;- init_pop ## store a population with initial size 1000
  for(y in 2:50){
    recruits &amp;lt;- exp(rec_devs[y])*pop[y-1]*rec_mult## in lieu of an SRR, just a simple function of current biomass
    pop[y] &amp;lt;- exp(-natM)*(pop[y-1]+recruits) ## pretending they die after spawning
  } ## end years
  pop_big[,i] &amp;lt;- pop
} ## end pop replicate

## do the same thing but without recdevs
pop_big_zero &amp;lt;- matrix(NA, nrow = 50, ncol = 50)
for(i in 1:50){
  rec_devs &amp;lt;- rep(0, 50)
  pop = NULL; pop[1] &amp;lt;- init_pop 
  for(y in 2:50){
    recruits &amp;lt;- exp(rec_devs[y])*pop[y-1]*rec_mult
    pop[y] &amp;lt;- exp(-natM)*(pop[y-1]+recruits) 
  } ## end years
  pop_big_zero[,i] &amp;lt;- pop
} ## end pop replicate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like we’d expect, the rec-devs introduce variety both through time and among population replicates:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;staticunnamed-chunk-3-1.png&#34; alt=&#34;Simulated Populations with random rec-devs (left) or rec devs = 0 (right)&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Simulated Populations with random rec-devs (left) or rec devs = 0 (right)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;That looks pretty different from the figure in the paper – and you might suspect it has something to do with fishery removals. So let’s introduce fishery removals in a careful way.
First we can specify a time series of known catches. This is probably what you have within your basic Stock Synthesis model you’d like to simulate with, or in the &lt;code&gt;data&lt;/code&gt; list passed to your TMB objective function. When you invoke &lt;code&gt;TMB::SIMULATE()&lt;/code&gt; given a rec-dev vector and known catches, or ask SS to bootstrap a population with a fixed rec-dev vector, this is what you’re getting. Notice how below I back out the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; rate in order to realize the catches exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## build the catches
known_catch &amp;lt;- c(sort(runif(25, 1,50 )), sort(runif(25, 1, 40), decreasing = T))/5

## make fifty, 50-year population trajectories
pop_big &amp;lt;-rec_devs &amp;lt;-  Frates &amp;lt;- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] &amp;lt;- rnorm(50,mean = -sigmaR^2/2,sigmaR) 
  
  pop[1] &amp;lt;- init_pop 
  Frates[1,] &amp;lt;-  known_catch[1]/pop[1]
  for(y in 2:50){
    Frates[y,i] = known_catch[y-1]/pop[y-1]
    recruits &amp;lt;- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] &amp;lt;- exp(-(natM))*(pop[y-1]+recruits)*(1- Frates[y,i]) 
  } ## end years
  pop_big[,i] &amp;lt;- pop
} ## end pop replicate&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;staticunnamed-chunk-5-1.png&#34; alt=&#34;Simulated population with catches known exactly&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Simulated population with catches known exactly
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;le-difference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Le Difference&lt;/h1&gt;
&lt;p&gt;The population size trends from the above example do decline, but they are all over the place! (They don’t go up and down systematically). It’s responding to your random rec dev vector you created, and the underlying harvest rate which &lt;em&gt;adjusts&lt;/em&gt; to take the known catches from your population. &lt;strong&gt;If you pass a random vector of recruitment devs to Stock Synthesis with a time series of historical catch, the SSB timeseries derived from your bootstrapped dataset will vary along with your devs&lt;/strong&gt;. It won’t look like your original population series, and might be quite high/low in given years. This could be just what you want! But it’s not what’s in Arni’s paper, which is distinct because there the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;s are instead known exactly, and landings are allowed to vary. Let’s show what happens with that small change, noting how much less variability you see in the SSB trend:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## specify the Fs -- 
known_F &amp;lt;- known_catch/apply(pop_big,1,mean)

## make fifty, 50-year population trajectories
pop_big &amp;lt;-rec_devs &amp;lt;-  catches&amp;lt;- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] &amp;lt;- rnorm(50,mean = -sigmaR/2,sigmaR) 
  
  pop[1] &amp;lt;- init_pop 
  catches[1,] &amp;lt;-  known_F[1]*init_pop
  for(y in 2:50){
    recruits &amp;lt;- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] &amp;lt;- exp(-(natM))*(pop[y-1]+recruits)*(1-known_F[y])
    catches[y,i] &amp;lt;- known_F[y]*(pop[y-1]+recruits)
  } ## end years
  pop_big[,i] &amp;lt;- pop
} ## end pop replicate&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;staticunnamed-chunk-7-1.png&#34; alt=&#34;Simulated population with F known exactly&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Simulated population with F known exactly
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;le-workaround&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Le workaround&lt;/h1&gt;
&lt;p&gt;This is all well and good – but what if you (like me) are on a time crunch, perhaps aren’t super familiar with &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0092725&#34;&gt;&lt;code&gt;ss3sim&lt;/code&gt;&lt;/a&gt;, and/or don’t have time to code up a version of your model which accepts fixed inputs for &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; (instead of solving for them)? In the Synthesis framework, to my knowledge, we’re pretty much stuck with treating the vector of catches in the &lt;code&gt;dat&lt;/code&gt; file as known, so the model &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;s are indeed going to vary with each population replicate. A way to generate a smooth band a la Magnusson at al is to pass a random vector of recruitment deviates that are instead &lt;strong&gt;centered at the annual values from the original model&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec_dev_mean &amp;lt;-  rnorm(50,mean = -sigmaR^2/2,sigmaR) 
## make fifty, 50-year population trajectories
pop_big &amp;lt;-rec_devs &amp;lt;-  catches&amp;lt;- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] &amp;lt;- rnorm(50,mean = rec_dev_mean-sigmaR^2/2,sigmaR) ## devs are now mean-centered, still bias corrected
  
  pop[1] &amp;lt;- init_pop 
  Frates[1,] &amp;lt;- known_catch/init_pop
  for(y in 2:50){
    Frates[y,i] = known_catch[y-1]/pop[y-1]
    recruits &amp;lt;- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] &amp;lt;- exp(-(natM))*(pop[y-1]+recruits)*(1- Frates[y,i])
  } ## end years
  pop_big[,i] &amp;lt;- pop
} ## end pop replicate&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-9&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;staticunnamed-chunk-9-1.png&#34; alt=&#34;Simulated population with recdevs recentered&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Simulated population with recdevs recentered
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we’re able to realize the known catches exactly while preserving the historical population trend to a greater degree than before. Do use this with caution; you are treating the model outputs (recruitment devs from your original assessment) as a new input, so inheriting any trends in the process error your original model might have missed.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Length-based Processes in an Age-Structured Model</title>
      <link>https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age-conv/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age-conv/</guid>
      <description>
&lt;script src=&#34;https://mkapur.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is in the category of “stuff I thought I understood” until I didn’t.&lt;/p&gt;
&lt;p&gt;Let’s say you have an age-based model with integer-year bins &lt;code&gt;a&lt;/code&gt; thru &lt;code&gt;A+&lt;/code&gt;. The model is going to keep track of the dynamics of the fish population as they step through each age, but (annoyingly, perhaps) some of your modeled processes are &lt;em&gt;length&lt;/em&gt;-based. This can occur when you are working with lots of different fleets, for which gear selectivity is better specified by length, or even the simple inclusion of our favorite &lt;span class=&#34;math inline&#34;&gt;\(w_{l} = a l^b\)&lt;/span&gt; calculation, which will require a notion of a fish length-at-age to play nicely within an age-structured model.&lt;/p&gt;
&lt;p&gt;Converting these length-based estimates (or quantities) into age-based estimates introduces a problem because of &lt;strong&gt;uncertainty in length-at-age&lt;/strong&gt;, in other words, the fact that fish of length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; could be of various ages &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. Let’s make up some data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## check out my post &amp;quot;Why do we use ln(19)?&amp;quot; for more info on this syntax:
logistic &amp;lt;- function(bin, b50, b75){
  selage &amp;lt;- 1/(1+exp(-(log(15))*(bin-b50)/(b75-b50)))
  return(selage)
} 

age_bins &amp;lt;- 0:50
length_bins &amp;lt;- 1:65
sigma_length &amp;lt;- 7.16

## a logistic, length-based selectivity curve
length_selex &amp;lt;- sapply(length_bins, logistic, b50 = 30, b75 = 52)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s our selectivity curve; there’s a vertical line at &lt;code&gt;l = 28cm&lt;/code&gt; which we’ll use for reference throughout this post.
&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
So let’s say we’re interested in the selectivity at &lt;code&gt;l = 28cm&lt;/code&gt;. In a length-based population dynamics model, we’d be set to just return the selectivity at &lt;code&gt;l = 28cm&lt;/code&gt; for any fish within that respective bin (in 1-cm bins, fish lengths greater than 27cm and less than or equal to 28cm would all be assigned this value.)&lt;/p&gt;
&lt;p&gt;However, in an age-based model, we are instead interested in translating this curve into the expected selectivity-at-&lt;em&gt;age&lt;/em&gt;. And given age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, a fish could be a broad range of lengths &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;, making it less clear which value we would “read off” the blue curve above. This variation is caused by the variable &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, which I’ve set at ~7cm – this reflects the standard deviation in length-at-age for our stock.&lt;/p&gt;
&lt;p&gt;Here’s how it looks in practice. First, we can generate our vonB-based expected length-at-age (here I assume &lt;span class=&#34;math inline&#34;&gt;\(t_0\)&lt;/span&gt; is 0, and am plotting the observed variation for 500 datasets, which should approximate the value for &lt;code&gt;sigma_length&lt;/code&gt; specified above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vonb &amp;lt;- function(age, linf, kappa, sigma_length){
  len &amp;lt;- linf*(1-exp(-kappa*(age)))+rnorm(1,0,sigma_length)
}

## simulate some datasets
length_at_age = matrix(NA, nrow = length(age_bins), ncol = 500)
for(i in 1:500){
  length_at_age[,i] &amp;lt;- sapply(age_bins, vonb, linf = 44,
                              kappa = 0.15, sigma_length)
}
length_at_age &amp;lt;- bind_cols(age = age_bins, 
                           meanL = rowMeans(data.frame(length_at_age)),
                           sdL = apply(data.frame(length_at_age),1,sd))%&amp;gt;%
  mutate(lci = meanL-1.96*sdL, uci = meanL+1.96*sdL)

ggplot(length_at_age,
       aes(x = age_bins)) +
  geom_line(lwd = 1.1, col = &amp;#39;grey&amp;#39;, aes(y = meanL)) +
  geom_ribbon(aes(ymin = lci, ymax = uci), fill = alpha(&amp;#39;grey&amp;#39;,0.3))+
  geom_hline(yintercept = 28, linetype = &amp;#39;dotted&amp;#39;)+
  labs(x = &amp;#39;age&amp;#39;, y= &amp;#39;length&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notice how the horizontal line at 28cm corresponds to multiple ages within the confidence interval?&lt;/strong&gt; How do we account for the fact that there is a chance that the the selectivity applicable to a given age can be characterized by the selectivity at several lengths, and how do we weight the applicability of that selectivity value given what we know about growth? Enter &lt;span class=&#34;math inline&#34;&gt;\(\psi_{l,a}\)&lt;/span&gt;, the probability of being in length bin &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, a.k.a. the normal distribution &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; of size-at-age.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\psi_{al} &amp;amp;= \begin{array}
   &amp;amp; \Phi (1,\tilde l_{a}, \sigma)) &amp;amp; if \space l = 0cm \\
   \Phi (l+1,\tilde l_{a}, \sigma)) - \Phi (l,\tilde l, \sigma))&amp;amp; if \space 0cm, &amp;lt; l &amp;lt; L+ \\
   1-\Phi (l,\tilde l_{a}, \sigma)) &amp;amp; if \space l = L+\\
\end{array}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is the length bin in question, &lt;span class=&#34;math inline&#34;&gt;\(\tilde l\)&lt;/span&gt; is the observed length-at-age, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the variation in growth. If we simply used a “lookup” method (which I have certainly done in a pinch!), and assigned the selectivity at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; to the selectivity at the expected length at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, we’d be introducing bias.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## this function returns the distribution of expected length-at-age 
## given a bin and observed length-at-age
getP_al &amp;lt;- function(len_bin, len_obs, length_sigma){
  pal = NA
  if(len_bin == 1){
    pal &amp;lt;- pnorm(len_bin, len_obs, length_sigma)
  } else if (len_bin &amp;gt; min(length_bins) &amp;amp; len_bin &amp;lt; max(length_bins)){
        pal &amp;lt;- pnorm(len_bin+1, len_obs, length_sigma) - 
          pnorm(len_bin, len_obs, length_sigma)
  } else{
        pal &amp;lt;- 1-pnorm(len_bin, len_obs, length_sigma)
  }
  pal
}
PAL &amp;lt;- matrix(NA, nrow = length(age_bins), ncol =  length(length_bins))
for(a in age_bins){
  for(l in length_bins){
    ## instead of using means, could re-do vonB with error turned off
    PAL[a,l] &amp;lt;- getP_al(len_bin=l,len_obs=length_at_age$meanL[a],sigma_length )
  }
}
longPal &amp;lt;- data.frame(PAL) %&amp;gt;%
  mutate(age_bins) %&amp;gt;%
  reshape2::melt(id = &amp;#39;age_bins&amp;#39;) %&amp;gt;% 
  mutate(len =as.numeric(gsub(&amp;quot;X&amp;quot;,&amp;quot;&amp;quot;,variable))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a subset of what &lt;span class=&#34;math inline&#34;&gt;\(\psi_{al}\)&lt;/span&gt; looks like for ages 5-10. Notice (again) how length = 28cm crosses multiple age curves. The good news is, &lt;span class=&#34;math inline&#34;&gt;\(\psi_{al}\)&lt;/span&gt; has given us a sense of the relative probability of being age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; given length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. (It looks like &lt;span class=&#34;math inline&#34;&gt;\(l = 28cm\)&lt;/span&gt; is most likely a fish of age 7 or 8).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
To put this all together, we need to compute the dot-product of the selectivity-at-length and the expected distribution of length-at-age.&lt;/p&gt;
&lt;p&gt;In math, we’re doing this:
&lt;span class=&#34;math display&#34;&gt;\[
Sel_a = \sum_l Sel_l \cdot \psi_{al}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which is really just a cheap way of approximating the correct estimator of the expected selectivity of an animal at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Sel_a = \int_L Sel_L  \psi_{aL} dL
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In words: for each length bin &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;, multiply the selectivity at that length times the probability of being at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; given length &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;. Sum these quantities across all length bins for each age bin.&lt;/p&gt;
&lt;p&gt;In pictures: let’s say we are interested in &lt;span class=&#34;math inline&#34;&gt;\(a = 5\)&lt;/span&gt;. We are going to multiply each point on the length-selectivity by its corresponding point (by color and ID number) on the probability of length-at-age-5 plot, then sum the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In code (still using &lt;span class=&#34;math inline&#34;&gt;\(a = 5\)&lt;/span&gt; as an example):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sel_age &amp;lt;- merge(longPal,data.frame(cbind(length_selex,length_bins)),
                 by.x = &amp;#39;len&amp;#39;, by.y = &amp;#39;length_bins&amp;#39;) %&amp;gt;%
  ## inner product for each age-length combo
  mutate(prod1 = length_selex*value) %&amp;gt;% 
  group_by(age_bins, len) %&amp;gt;%
  summarise(dotProd = sum(prod1)) %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(age_bins) %&amp;gt;%
  summarise(dotProdsum = sum(dotProd))

ggplot(sel_age,
       aes(x = age_bins, y = dotProdsum)) +
  geom_line(lwd = 1.1, col = &amp;#39;grey22&amp;#39;) +
  labs(x = &amp;#39;age&amp;#39;, y= &amp;#39;proportion selected&amp;#39;, 
       main = &amp;#39;Converted from length-at-age&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/2022-05-02-len-age-conv/len-age_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I Keep Up With the Literature</title>
      <link>https://mkapur.netlify.app/post/how-i-keep-up-with-the-literature/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/how-i-keep-up-with-the-literature/</guid>
      <description>&lt;p&gt;My lab recently had a meeting where we discussed the nuts &amp;amp; bolts of performing a peer-review for a journal article, which got everyone talking about their preferred methods of reading a scientific paper. Luckily, we aren&amp;rsquo;t in bio-medicine or particle physics, but the volume of articles that emerge monthly in our field is still large, and it takes strategy to efficiently extract and scrutinize the information from a scientific article.&lt;/p&gt;
&lt;p&gt;One of my favorite specimens of PhD advice comes from the &lt;a href=&#34;http://www.chemistry-blog.com/wp-content/uploads/2010/06/Gassman-and-Meyers.pdf&#34;&gt;Gassman and Meyers memos&lt;/a&gt;, which posess the perfect melange of condecision, tough love and &amp;ldquo;kids these days&amp;rdquo; angst &amp;ndash; the advice, nonetheless, is great. It sticks because it stings a bit. Gassman writes:&lt;/p&gt;
&lt;blockquote&gt;It is a consistent observation on my part that people have more ideas about their research when they are writing their thesis than at any other time. This is generally due to the fact that they are finally doing the literature work they should have done early in their thesis work only at the time that they are trying to complete their dissertations. -- P.G. Gassman&lt;/blockquote&gt;
So, don&#39;t let that be you. Here is how I approach the literature, which has proved helpful for the last few years.
&lt;ol&gt;
	&lt;li&gt;Have a method to centralize new articles. I used the tips on the &lt;a href=&#34;https://fraserlab.com/2013/09/28/The-Fraser-Lab-method-of-following-the-scientific-literature/&#34;&gt;Fraser Lab website&lt;/a&gt; to set up a Feedly account (there are lots of other options) and subscribed to the four main journals in my field, plus Science, Nature, and PNAS. The algorithm learns your preferences quite quickly, and your queue will show articles that fit your niche after you &#39;check&#39; them off.&lt;/li&gt;
	&lt;li&gt;Check your centralizer every morning, and save the articles whose abstracts seem relevant as soon as you see them. It&#39;s fine to save articles on-the-fly, but I recommend scheduling &#39;deep reading&#39; periods of no less than one hour.&lt;/li&gt;
	&lt;li&gt;Go beyond highlighting/sticky notes. I use a set of spreadsheets, stored in Google Drive and sorted by topic, to ensure that I&#39;m actually engaged with what I&#39;m reading. They all follow the same format. One example  comes from my spreadsheet for studies of spatial model structure and general model mis-specification. The headings are hard to read in the screenshot, but they are &lt;strong&gt;Citation, Species, Mis-specification, Methods, Performance Measures, Conclusions, Similarities and Differences.&lt;/strong&gt; These can be tailored to your field or project, but the last two are the most crucial. In those columns, I compare the study to my thesis work, making it easy as I write my introductions/discussions to state who has done similar studies, and where the research gaps lie.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How Do Recruitment Deviates Work?</title>
      <link>https://mkapur.netlify.app/post/recdevs/how-rec-devs/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/recdevs/how-rec-devs/</guid>
      <description>


&lt;p&gt;For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. &lt;a href=&#34;https://doi.org/10.1139/f2011-092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1139/f2011-092&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I want to jot down the practical implementation behind the recruitment deviate for those working in Stock Synthesis or building their own models from scratch. The main topics to discuss are 1) The meaning of a recruitment deviate (abbreviated here to “rec-dev”), 2) how it plays in your dynamics equations, and 3) keeping an eye out for lognormal bias correction.&lt;/p&gt;
&lt;div id=&#34;what-is-a-rec-dev&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a rec-dev?&lt;/h1&gt;
&lt;p&gt;A recruitment deviate is how much a given year’s recruitment &lt;em&gt;deviates&lt;/em&gt; from what you’d expect given a stock-recruit relationship (in this post I’ll use the Beverton-Holt as an example). The idea is that there could be many reasons for a stock to produce a number of recruits which is different from the deterministic value defined by the current stock size (among other parameters). Reasons for this deviation could be environmental, and are generally unexplained – which is why the time series of rec devs is often interpreted as process error in your system (all the extra natural noise our model can’t capture).&lt;/p&gt;
&lt;p&gt;Let’s quickly refresh what we’re talking about when we say the “mean” or expected recruitment level. Again, using the Bev-Holt as an example:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
R_{expected} = \frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is steepness, or the expected proportion of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; anticipated at &lt;span class=&#34;math inline&#34;&gt;\(0.2SSB_0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SSB_0\)&lt;/span&gt; are virgin recruitment and spawning biomass, respectively.&lt;/p&gt;
&lt;p&gt;If we make up some numbers for these parameters and plug them in, the resultant relationship might look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool. Now let’s consider that we have some notion of error around this deterministic value; this error could arise from uncertainty in the parameters of the stock recruit curve. Here, the shaded zone is the 95% confidence interval around our expected recruitment. 95% of recruitment values in a given year at a given SB should fall within that zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given this error, your observed recruitment in a given year may &lt;em&gt;deviate&lt;/em&gt; from the black curve somewhat. The new observed recruitments are shown as gold points; the line separating each point from the expected value is the raw value of the deviate itself.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviate at high biomass is going to be much larger than the deviate at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is diverging from expectation. For that reason, we are interested in modeling the errors, or deviates, as &lt;em&gt;lognormally distributed&lt;/em&gt;. Equation-wise, here’s what that looks like. Note that &lt;span class=&#34;math inline&#34;&gt;\(R_{det}\)&lt;/span&gt; is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{expected,y} = R_{det,y}exp(dev_y-\sigma^2/2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that &lt;span class=&#34;math inline&#34;&gt;\(-\sigma^2/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;refreshing-the-lognormal-distribution.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Refreshing the lognormal distribution.&lt;/h1&gt;
&lt;p&gt;If our devs are normally distributed with mean zero, &lt;span class=&#34;math inline&#34;&gt;\(exp(\mu = 0)\)&lt;/span&gt; is &lt;em&gt;lognormally&lt;/em&gt; distributed, shown in blue below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X ~ N(\mu,\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y = e^X\)&lt;/span&gt;, the expected value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, and the expected value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; will be &lt;span class=&#34;math inline&#34;&gt;\(e^{\mu+\theta/2}\)&lt;/span&gt;, &lt;em&gt;not&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{\mu}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(exp(X)\)&lt;/span&gt; is 1) never less than zero and 2) pretty asymmetrical. What this means mathematically is that the mean of &lt;span class=&#34;math inline&#34;&gt;\(exp(X)\)&lt;/span&gt; &lt;em&gt;is in fact not the same&lt;/em&gt; as &lt;span class=&#34;math inline&#34;&gt;\(exp(mean(X))\)&lt;/span&gt;. Confirm this for yourself by testing the following. They’ve been added to the plots above as vertical dashed lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(X) ## should be close to 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01505518&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(X)) ## should be close to 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9850576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(exp(X)) ## uh-oh! It&amp;#39;s way bigger...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.626412&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mu+sigma/2) ##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.648721&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-correction-to-the-rescue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bias correction to the rescue&lt;/h1&gt;
&lt;p&gt;Back to the recruitment example, we can pretend that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; here is our vector of recruitment deviates. We set the mean for &lt;span class=&#34;math inline&#34;&gt;\(dev_y\)&lt;/span&gt; to 0 in hopes that, on an “average” year, we’ll be multiplying &lt;span class=&#34;math inline&#34;&gt;\(R_{det,y}\)&lt;/span&gt; (deterministic recruitment) by &lt;span class=&#34;math inline&#34;&gt;\(exp(0) = 1\)&lt;/span&gt;, and get the expected value back. However, because we’ve discovered that &lt;span class=&#34;math inline&#34;&gt;\(exp(dev_y)\)&lt;/span&gt; is not symmetrical, we need to perform a &lt;em&gt;bias correction&lt;/em&gt; to confirm that we &lt;em&gt;approximately&lt;/em&gt; return the deterministic value when &lt;span class=&#34;math inline&#34;&gt;\(dev_y\)&lt;/span&gt; is zero. In other words, we are adjusting how we back-transform the deviates to confirm that the mean of those deviates is roughly one, on a normal scale.&lt;/p&gt;
&lt;p&gt;Check it out:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(X)) ## should be close to 1, as above&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9850576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9864687&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above).&lt;/p&gt;
&lt;p&gt;Imagining we had a time series of data which encompassed all the different &lt;span class=&#34;math inline&#34;&gt;\(SB\)&lt;/span&gt; values corresponding to the gold points above, we could plot the deviates through time (I randomly reordered the values):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So now, the horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year &lt;em&gt;regardless of the stock size at hand&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why do we use ln(19)?</title>
      <link>https://mkapur.netlify.app/post/ln19/why-ln19/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/ln19/why-ln19/</guid>
      <description>


&lt;p&gt;Have you ever noticed that sneaky &lt;span class=&#34;math inline&#34;&gt;\(log(19)\)&lt;/span&gt; in your logistic equation for maturity or selectivity? What is up with that?&lt;/p&gt;
&lt;p&gt;Let’s take the example of a basic selectivity-at-age function, which uses the parameters &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{95}\)&lt;/span&gt;. These represent the ages where a fish has a probability 50% or 95% chance of being captured. &lt;em&gt;Read that again carefully.&lt;/em&gt; I took these parameters for granted, but the meaning of them is intertwined with the use of &lt;span class=&#34;math inline&#34;&gt;\(ln(19)\)&lt;/span&gt;, and if they change, so does the logged value.&lt;/p&gt;
&lt;p&gt;Here’s a selectivity plot with some made up numbers. The selectivity function I’m using is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_a = \langle 1+exp(-log(19)*\frac{a-a_{50}}{a_{95}-a_{50}}) \rangle ^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ln19/2021-01-25-ln19_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note how nicely these values extend between zero and one (on the y axis)? That’s because we’re really interested in working with the odds of being captured. Based on our parameter definitions, we need to ensure that the logistic function equals 0.95 when &lt;span class=&#34;math inline&#34;&gt;\(a = a_{95}\)&lt;/span&gt; (and similarly equals 0.5 at &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt;). To achieve this, we recognize the exponent of the log odds equals the odds ratio. You can think of the odds ratio as the probability of your event (getting captured) happening over the probability of it not happening. We can leverage this to force the logistic equation to scale the way we want.&lt;/p&gt;
&lt;p&gt;Since we’ve already defined &lt;span class=&#34;math inline&#34;&gt;\(a_{95}\)&lt;/span&gt; as the age at which we have a 95% probability of getting captured, the log-odds can be defined as &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{0.95}{0.05})\)&lt;/span&gt;. This could be rewritten as &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{19/20}{1/20})\)&lt;/span&gt; and simplifies to &lt;span class=&#34;math inline&#34;&gt;\(log(19)\)&lt;/span&gt;. This ensures that when &lt;span class=&#34;math inline&#34;&gt;\(a = a_{95}\)&lt;/span&gt;, the equation up top indeed returns 0.95, which is the logistic function evaluated at an odds ratio of 95%.&lt;/p&gt;
&lt;p&gt;Given this syntax you’ll see how we would never really need to correct for &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt;, since the odds ratio of two events with 50% probability is actually zero (plug in 0.5 to see for yourself).&lt;/p&gt;
&lt;p&gt;Two notes:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Sometimes to ease model fitting, we will simplify the above equation by substituting &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; in place of the denominator (&lt;span class=&#34;math inline&#34;&gt;\(a_{95} - a_{50}\)&lt;/span&gt;), or estimate &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\frac{ln(19)}{a_{95} - a_{50}}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’d want to use the something like &lt;span class=&#34;math inline&#34;&gt;\(a_{75}\)&lt;/span&gt;, you’d need to update the log odds via &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{0.75}{0.25})\)&lt;/span&gt; or ~ &lt;span class=&#34;math inline&#34;&gt;\(log(3)\)&lt;/span&gt;. Note that this ensures that you are getting a 75% selectivity at the corresponding age, but provides a distinct curve from before.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ln19/2021-01-25-ln19_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I studied for my PhD General Exam</title>
      <link>https://mkapur.netlify.app/post/generals/how-studied-general/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/generals/how-studied-general/</guid>
      <description>


&lt;p&gt;I am finally a PhD Candidate! You may want to check out my &lt;a href=&#34;https://maiakapur.netlify.app/post/quals/how-studied-quals&#34;&gt;“How I studied for my PhD Quals”&lt;/a&gt; post, which detailed my prep for a 5-day written examination I took in December. The General exam in my program is more about investigating 1) your preparedness and thinking regarding your proposed dissertation project, and 2) any gaps in your knowledge that should be addressed between now and your defense. This latter point was fairly disconcerting for me, as it’s hard to know if you’re doing well on a test designed to expose your shortcomings. Also, unlike the Qualifying exam, the format is an all-in-one, three-hour oral bonanza where there is no way to &lt;del&gt;scream into a paper bag&lt;/del&gt; take breaks/regroup after a particularly hard question. In any case, I passed, so here are some tips that can hopefully be generalizable to others taking exams in a similar format. I’ve split these into prep tips and in-the-moment advice.&lt;/p&gt;
&lt;div id=&#34;in-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In Preparation &lt;br&gt;&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplify the guessing game&lt;/strong&gt;. Studying for this exam is a little harder because your committee may not give you any readings or additional topics to read about, leaving you to think you should know the entirety of your scientific field. Rather, your #1 priority should be to demonstrate you grasp the &lt;em&gt;essential concepts, assumptions and methods&lt;/em&gt; YOU will use in your disseration work, and have a notion of how your results will &lt;em&gt;fit into the literature at large&lt;/em&gt;. For that reason, I spent about 85% of my time making my proposal presentation clear and thorough, and prepared for questions like “How do you expect X to play into your results?”, “Have you considered using Y method?”, and “How is this different from work by Z?”. You’ll notice these are &lt;em&gt;open ended&lt;/em&gt; and designed to check that you aren’t just following a recipe for your study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The 15% basis&lt;/strong&gt; I studied on the unempirical assumption that only 15% of the committee’s confidence in my ability to complete my project would be based on my ability to respond to technical &amp;amp; conceptual questions about my field. I convinced myself that the &lt;em&gt;overwhelming factor which will convince them that I’m ready to progress on my dissertation&lt;/em&gt; is, well, the demonstrated progress I’ve actually made to this point (writing a proposal counts, too!). In other words, the committee ~is not going to~ did not fail me because I [did] get nervous, blank out, or can’t perfectly recall a concept in the moment, if I’ve otherwise demonstrated I can produce research and find answers when I need them. However, there are some “tricks” towards approaching that 15%, which I share in point #3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The 101 Slide &amp;amp; The “Warrants Future Research”&lt;/strong&gt;. I recognized that the questions people fear getting – the sudden-death, “gotcha” questions regarding a specific paper/equation/idea in your field, are less terrifying if you think of them the way the faculty think of them. I find such questions invariably fall into one of two categories.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;The first I call the “&lt;strong&gt;101 Slide&lt;/strong&gt;”. These are questions about basic concepts in your field which a 101 (introductory) course may have dedicated a lecture or lab exercise to – and that is the degree of response they are looking for! A classic my adviser throws around is, “How do you get the highest fishing yield from a cohort?”. Your brain will instantly rat-wheel around all the things that could lead one to not know the maximum yield…but the answer, literally, is “fish when the cohort biomass is maximized.” Exactly the response that would appear on an undergrad lecture slide! &lt;strong&gt;Full disclosure: I blanked out on a 101 question&lt;/strong&gt; fairly early in my exam and was pretty sure it was a fatal mistake. Depsite all the knowledge I had about this phenomenon, I still fell prey to the anxiety-brain-wipe…and survived. So you will, too 😄&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“&lt;strong&gt;Warrants future research&lt;/strong&gt;” are questions likely to appear in the discussion part of (the faculty’s) recent papers, and they are not expecting you to have a definitive answer. Questions beginning with “have you thought about…” or “what is the possibility…” are hints that the topic falls into this category and they’re asking you to spectulate. &lt;strong&gt;When in doubt, it looks good to start mentioning extant work on that topic&lt;/strong&gt;; the faculty may guide you towards which paper(s) they felt were conclusive, or be satisfied to know that you are aware it is an area of ongoing research. A strong response would suggest what experiments could be done to reduce speculation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The trick, obviously, regarding point 3 is knowing which category the question falls into given the topic. You should be generally familiar with open research questions in your field, emphasizing those &lt;em&gt;related directly to your project&lt;/em&gt; and &lt;em&gt;pet interests of your faculty members&lt;/em&gt;. For example, my disseration doesn’t do a lot of environmental modeling explicitly, but I am aware that there is an open debate regarding the influence of climate (environment) vs biomass on determining recruitment. I can name the folks involved in this work and what their general findings have been, and some of the challenges to implementing such research in our own context.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-the-moment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In the Moment&lt;/h1&gt;
&lt;p&gt;Here are some pointers – but know that nothing can calm you down come exam day like knowing you have put in the right amount of preparation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In my adviser’s words: “Don’t be chaotic”. This means don’t panic. Repeat the question back once asked to buy yourself some time.&lt;/li&gt;
&lt;li&gt;Any amount of silence feels way longer to you than it does to them. It looks better to pause and choose your words carefully than to ramble and hope you step on a right-answer mine.&lt;/li&gt;
&lt;li&gt;DO NOT overhydrate beforehand. You will need to be abiotic for three hours.&lt;/li&gt;
&lt;li&gt;Keep in mind you have probably read WAY more papers in depth in the last ~2 months than the faculty. Also keep in mind that while they could readily write down important equations/concepts that they &lt;em&gt;regularly work with&lt;/em&gt;, they really don’t know everything and likely could not perfectly answer every question asked by the others in the room if the roles were reversed. This isn’t about being arrogant, it’s about being realistic in what actually makes for a succesful scientist.&lt;/li&gt;
&lt;li&gt;Assume they want you to pass and move on. Failing looks bad on your advisor, ultimately – so believe that this is a group that wants to see you suceeed.&lt;/li&gt;
&lt;li&gt;“I don’t know” is a valid response. If you are afraid the question falls into the category of 3a (101 Slide) and you really draw a blank, you could choose to mention how you do/do not think it super relevant to your project, hence giving a bit of cover for perhaps why you didn’t review it in depth. Silence is better than rambling here.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Algebra for Equilibrium Recruitment</title>
      <link>https://mkapur.netlify.app/post/recalg/some-rec-algebra/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/recalg/some-rec-algebra/</guid>
      <description>


&lt;p&gt;I often find myself revisiting the various “re-arrangements” of the Beverton-Holt stock-recruitment relationship, particularly when moving between equilibrium quantities. I wanted to centralize some algebra for myself and anyone else who uses this relationship regularly.&lt;/p&gt;
&lt;div id=&#34;the-bev-holt-srr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bev-Holt SRR&lt;/h2&gt;
&lt;p&gt;Let’s start with some working definitions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array} 
{rr}
R_0 \quad virgin, \quad unfished \quad recruitment \\ SBPR_0 \quad virgin, \quad unfished \quad spawner \quad biomass-per-recruit \\ SSB_0 \quad virgin, \quad unfished \quad spawning \quad biomass \\ SBPR_F \quad  \quad biomass-per-recruit \quad at \quad F \neq 0 \\ SSB_F \quad spawning \quad biomass  \quad at \quad F \neq 0 \\ YPR_F \quad yield-per-recruit \quad at \quad F \neq 0 \\ \end{array} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First, let’s illustrate how one strings these quantities together in an age-structured model &lt;em&gt;assuming a stock-recruit relationship&lt;/em&gt; to derive equilibrium quantities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-per-recruit-quantities-at-various-f&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield-per-Recruit quantities at various F&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(YPR\)&lt;/span&gt; at your &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; value of interest, and also calculate these values when &lt;span class=&#34;math inline&#34;&gt;\(F = 0\)&lt;/span&gt;. This will vary slightly depending on the setup of your model, but in pseudo-math: &lt;span class=&#34;math inline&#34;&gt;\(YPR = \large \Sigma_a \small Fs_aN_aw_a\)&lt;/span&gt;, Where &lt;span class=&#34;math inline&#34;&gt;\(N_a\)&lt;/span&gt; is a proportional expected number-at-age, &lt;span class=&#34;math inline&#34;&gt;\(w_a\)&lt;/span&gt; is the expected weight-at-age, and &lt;span class=&#34;math inline&#34;&gt;\(s_a\)&lt;/span&gt; is fishery selectivity. &lt;span class=&#34;math display&#34;&gt;\[
SBPR = \large \Sigma_a \small E_aN_aw_a
\]&lt;/span&gt; Here &lt;span class=&#34;math inline&#34;&gt;\(E_a\)&lt;/span&gt; is some measure of your population’s fecundity at age. The resultant &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt; is going to vary with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(N_a\)&lt;/span&gt; will decline faster when &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is greater than zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;srr-in-terms-of-sbpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SRR in terms of SBPR&lt;/h2&gt;
&lt;p&gt;2a) Look at your stock-recruitment relationship. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are parameters of the Beverton-Holt SRR. We also leverage the definition of equilibrium stock spawning biomass as the product of unfished SBPR and Recruitment. You can confirm this makes sense by describing the equation with words: the virgin/unfished spawning biomass is the expected spawning biomass for one virgin/unfished recruit times the number of recruits. The same relationship will apply for equilibrium biomass. You will need to pick a number (or estimate) &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array} {rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
SSB_0 = SBPR_0 \times R_0 \\
SSB_{eq} = SBPR_{eq} \times R_{eq} \\
\end{array} 
\]&lt;/span&gt; 2b) Re-arrange the SRR to be in terms of &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt;. We do this by substitution. &lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
SSB_{eq} = SBPR_{eq} \times R_{eq} \\
R_{eq} =  \frac{SBPR_{eq} \times R_{eq} }{\alpha + \beta ( SBPR_{eq} \times R_{eq} )} \\
R_{eq}(\alpha + \beta ( SBPR_{eq} \times R_{eq} )) =  SBPR_{eq} \times R_{eq}  \\
\alpha + \beta ( SBPR_{eq} \times R_{eq} ) =  SBPR_{eq}   \\
\beta ( SBPR_{eq} \times R_{eq} ) =  SBPR_{eq} - \alpha  \\
 R_{eq}  = \frac{ SBPR_{eq} - \alpha}{\beta \times SBPR_{eq} }  \\
\end{array} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;alpha-beta-in-terms-of-steepness-recruitment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Alpha &amp;amp; Beta in terms of steepness &amp;amp; recruitment&lt;/h3&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sweet, now we need a nice definition for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Spoiler: these only vary by &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; and steepness, &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, which is defined as the expected recruitment at 20% of &lt;span class=&#34;math inline&#34;&gt;\(SSB_0\)&lt;/span&gt; (hence the 0.2s). Here’s the derivation of those values. Recall that &lt;span class=&#34;math inline&#34;&gt;\(SSB_0 = SBPR_0 \times R_0\)&lt;/span&gt;, meaning &lt;span class=&#34;math inline&#34;&gt;\(SBPR_0\)&lt;/span&gt; can be substituted for &lt;span class=&#34;math inline&#34;&gt;\(\frac{SSB_0}{R_0}\)&lt;/span&gt;. Let’s start with beta:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}
{cc}
R_0 = \frac{SSB_0}{\alpha + \beta SSB_0} &amp;amp; original \\
\alpha + \beta SSB_0 = \frac{SSB_0}{R_0} &amp;amp; rearrange, \quad sub \\
\alpha =SBPR_0 - \beta SSB_0 &amp;amp; alpha \\
hR_0 = \frac{0.2SSB_0}{\alpha + 0.2 \beta SSB_0} &amp;amp; h @ 0.2SSB_0  \\
hR_0 = \frac{0.2SSB_0}{SBPR_0 - \beta SSB_0 + 0.2 \beta SSB_0} &amp;amp; substitute  \\
hR_0 = \frac{0.2SSB_0}{SBPR_0 - 0.8 \beta SSB_0} &amp;amp; simplify \\
hR_0SBPR_0 - 0.8hR_0\beta SSB_0= 0.2SSB_0 &amp;amp; multiply \quad denom\\
hSSB_0 - 0.8hR_0\beta SSB_0= 0.2SSB_0 &amp;amp; sub \quad SSB_0 = \tilde S R_0\\
h - 0.8hR_0\beta = 0.2 &amp;amp; div \quad SSB_0 \\
5h - 4hR_0\beta = 1 &amp;amp; mult \quad 5 \\
\frac{5h-1}{4hR_0} = \beta &amp;amp; solve \quad \beta \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And now for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, starting at the third equation from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}
{cc}
\alpha =SBPR_0 - \beta SSB_0 &amp;amp; from \quad above \\
\alpha =SBPR_0 -  \frac{5h-1}{4hR_0}SSB_0 &amp;amp; sub \quad \beta \\
\alpha =SBPR_0 -  \frac{5h-1}{4hR_0}SBPR_0 R_0 &amp;amp; sub \quad SSB_0 = SBPR_0 R_0  \\
\alpha =SBPR_0 -  \frac{5h-1}{4h}SBPR_0 &amp;amp; cancel \quad R_0  \\
\alpha =SBPR_0 \langle1-  \frac{5h-1}{4h}\rangle &amp;amp; factor  \\
\alpha = SBPR_0 \langle \frac{1-h}{4h}\rangle &amp;amp; rearrange   \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To recap, &lt;span class=&#34;math inline&#34;&gt;\(R_{eq}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SBPR_{eq}\)&lt;/span&gt; are both varying with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, but alpha and beta only change with &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(SBPR_{F=0}\)&lt;/span&gt; is fixed based on the life history of your species.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-round-stock-synthesis-recruitment-syntax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus Round: Stock Synthesis Recruitment Syntax&lt;/h2&gt;
&lt;p&gt;Ever wonder where the Beverton-Holt syntax in the Stock Synthesis manual comes from? It is simply the substitution &amp;amp; rearrangement of our derived values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; into the original stock-recruit relationship. The “trick” (not really) is just leaving the equation for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in terms of virigin biomass and recruitment.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
R_F = \frac{SSB_F}{\frac{SSB_0}{R_0} \langle \frac{1-h}{4h}\rangle + \frac{5h-1}{4hR_0} SSB_F} \quad substitute \\
R_F = \frac{SSB_F 4hR_0}{SSB_0(1-h)+ SSB_F(5h-1)} \quad flip \quad denom \\
\end{array} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-and-surplus-production-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield and Surplus Production Curves&lt;/h2&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We are almost done. We have equations for equilibrium recruitment, and the definitions for the parameters of that equation. Note that equilibrium recruitment &lt;em&gt;will change&lt;/em&gt; with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(SPBR_F\)&lt;/span&gt; changes, whereas alpha and beta will not (this is important later). We can now write down equations for our resultant equilibrium spawning biomass, and equilibrium yields: &lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
SSB_{eq} = R_{eq} \times {SBPR_F} \\
Yield_{eq} = R_{eq} \times {YPR_F}
\end{array} 
\]&lt;/span&gt; The “yield curve” is simply obtained by re-calculating these values across a range of Fs and plotting the results against input &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and/or resultant &lt;span class=&#34;math inline&#34;&gt;\(SSB\)&lt;/span&gt;. The height of the resultant dome is dependent on the value of &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; used, and the skewness in the biomass is related to selectivity. For the yield curve specifically, the peak occurs at the point just before natural mortality eclipses somatic growth, and the descending limb will be more “bendy” if you are selecting fish far before they mature. Another way of saying this is that the descending limb will accordingly change in height depending at the age of first capture.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My ggplot themes manifesto</title>
      <link>https://mkapur.netlify.app/post/ggplot-themes-manifesto/my-ggplot-themes-manifesto/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/ggplot-themes-manifesto/my-ggplot-themes-manifesto/</guid>
      <description>


&lt;p&gt;A few months ago I tweeted about the value of creating custom ggplot2 themes for easily synchronizing the look of all your figs for a given talk or manuscript. Some folks take this to another level and effectively have a personal aesthetic (see what I did there) that they use across their work, which I find compelling. I am obviously not a graphic designer, but I love beautiful plots, and wanted to consolidate some of the best resources + starter tips for others here. These range from out-of-the-box default themes within the ggplot2 package itself, to those made by others, to resources you can use to make your own more (or less) from scratch.&lt;/p&gt;
&lt;div id=&#34;straight-up-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1 Straight up ggplot()&lt;/h1&gt;
&lt;p&gt;This is the ggplot2 default. Can we just agree that the grey background with nursery colors is not cute? I think Hadley did this on purpose to encourage us to find alternatives.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(ggplot2)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are some themes that come pre-loaded in the ggplot2 package that you can invoke without installing anything else. They seem to follow the rule of subtraction – fewer gridlines, less borders, you get the idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theme_bw&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2 theme_bw()&lt;/h1&gt;
&lt;p&gt;Already loads better without that nasty grey background.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theme_minimal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3 theme_minimal()&lt;/h1&gt;
&lt;p&gt;I actually prefer the bounding box to the gridlines, but some folks find this spacious and clea&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next up are R packages on CRAN and/or Github. If you are like me, you need a pretty compelling reason to download someone’s random Github package purely for plotting needs – doubly so if the function of interest is buried within a massive package with a different purpose (I’m looking at you, oceanography). That said, here are some that I think are definitely worth adding to your toolkit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ggsidekicktheme_sleek&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4 ggsidekick::theme_sleek() &lt;/h1&gt;
&lt;p&gt;To quote my friend/colleague, “Sean Anderson is the GOAT”. He has brought the law of subtraction to the max with this one, which is great for presentations and homework assignments. I only wish the axis and legend text were larger (but more on how one could fix that below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remotes::install_github(&amp;quot;seananderson/ggsidekick&amp;quot;) ## for later R versions 
library(ggsidekick) 
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggsidekick::theme_sleek()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;hrbrthemestheme_modern_rc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5 hrbrthemes::theme_modern_rc()&lt;/h2&gt;
&lt;p&gt;Installing this is a heavier lift due to the required fonts, but I really like the light/dark possibilities here and the emphasis on nice typefaces. Would recommend for presentations exclusively. The one issue I have is that the dark version does not automatically lighten the box borders, something I have tried to fix in my personal versions (see below).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ggthemes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6 ggthemes&lt;/h1&gt;
&lt;p&gt;This is maintained by the ggplot2 folks and has lots of useful themes worth exploring. My favorite is solarized, which also has a light/dark option. I typically use the solarized-dark css from xaringanthemer for my slides, so popping in &lt;code&gt;ggthemes::theme_solarized_2(light = FALSE)&lt;/code&gt; guarantees my figures will match nicely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggthemes::theme_solarized_2() 
p2 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggthemes::theme_solarized_2(light = FALSE)

library(patchwork)

p1/p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ggthemes also has a theme called “solid” which is literally just a background and nothing else, a la sparklines, if you are…chic?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(mtcars)
ggplot(mtcars, aes(x = mpg, y = qsec)) +
  geom_line(lwd = 1.1) +
  ggthemes::theme_solid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-beyond-your-own-themespackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going beyond: your own themes/packages&lt;/h1&gt;
&lt;p&gt;I mentioned the issue of avoiding package-burden if you’re just interested in a nice plotting function. Relatedly, sometimes another person has already made a cool ggtheme that &lt;em&gt;would&lt;/em&gt; be perfect, &lt;em&gt;if only&lt;/em&gt; the font were bigger/colors brighter/legend placed differently. That leads me to the most involved, yet most rewarding approach, which is to write and save your own ggplot themes! I have compiled all my faves into a new package called &lt;code&gt;kaplot&lt;/code&gt;, which includes the original &lt;code&gt;theme_black&lt;/code&gt; and &lt;code&gt;theme_mk()&lt;/code&gt; which I had placed on &lt;code&gt;kaputils&lt;/code&gt; previously. Since kaputils is more of a simulation and data science toolkit (for me), I wanted a lighterweight pacakge with only my plotting stuff. The material ranges from mostly original to downright plagarized, and basically involves me tweaking pre-existing themes to be exactly as I’d like them. Going forward I will probably make themes in there specific to given publications.&lt;/p&gt;
&lt;p&gt;If you aren’t ready to commit to a full package, I recommend editing a basic ggplot template (try &lt;a href = &#34;https://gist.github.com/jslefche/eff85ef06b4705e6efbc&#34;&gt;theme_black() here &lt;/a&gt;) and simply source()ing that script when you make your plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-new-plotting-package-kaplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7-9: My new plotting package, &lt;code&gt;kaplot&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Here are some highlights.&lt;/p&gt;
&lt;div id=&#34;kaplottheme_solarized_mk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;kaplot::theme_solarized_mk()&lt;/h2&gt;
&lt;p&gt;My tweak to ggthemes which ensures that legend key backgrounds are not white. I use the dark version of this (light = FALSE) for presentations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kaplot)

p1 &amp;lt;- ggplot(mtcars, aes(x = mpg, y = hp, color = factor(gear))) +
  geom_boxplot() +
  kaplot::theme_solarized_mk(base_size = 14, light = TRUE)

p2 &amp;lt;- ggplot(mtcars, aes(x = mpg, y = hp, color = factor(gear))) +
  geom_boxplot() +
  kaplot::theme_solarized_mk(base_size = 14, light = FALSE)

p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](theme_solarized_mk.png) --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaplottheme_mk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;kaplot::theme_mk()&lt;/h2&gt;
&lt;p&gt;I have now implemented &lt;a href = &#34;https://community.rstudio.com/t/adding-ggplot-themes-and-color-palettes-to-a-package/2418/3&#34;&gt;this rather complicated method &lt;/a&gt; to override the default color palette to the &lt;a href = &#34;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/&#34;&gt; cbbPalette&lt;/a&gt;, which is colorblind friendly and greyscale-compatible. The whole goal is to reduce the number of lines I need to write for each figure. See below for a final note on color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) +
  geom_boxplot() + 
  kaplot::theme_mk()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](theme_mk.png) --&gt; ## kaplot::foundational() I kind of like old-school figures that look like an engineering professor made them, like the one below. I put out a call on twitter to sort out how this can be done! Stay tuned… &lt;img src=&#34;4642.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;taste-the-rainbow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Taste the rainbow&lt;/h2&gt;
&lt;p&gt;Aside from cbbPalette, I have several go-to color palettes from across the web which I regularly use. Fortunately the viridis() pal is default with ggplot2, but there were a couple others I wanted to bring in automatically. These include &lt;a href = &#34;https://github.com/jakelawlor/PNWColors&#34;&gt;Jake Lawlor’s PNWColors&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Now the palettes are pre-loaded and can be called just like viridis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(ToothGrowth, aes(x = len, y = dose, fill = supp)) + 
  geom_boxplot() + 
  kaplot::theme_mk() + 
  kaplot::scale_fill_starfish() 
p2 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + kaplot::theme_mk() + 
  kaplot::scale_fill_sunset() 
p3 &amp;lt;- ggplot(mtcars[1:9,], aes(x = mpg, y = qsec, fill = rownames(mtcars[1:9,]))) + geom_bar(stat=&amp;#39;identity&amp;#39;) + 
  kaplot::theme_mk() + 
  kaplot::scale_fill_ipsum() 

p4 &amp;lt;- ggplot(USArrests, aes(x = UrbanPop, y = Murder, fill = UrbanPop)) + geom_bar(stat=&amp;#39;identity&amp;#39;) + 
  kaplot::theme_mk() + 
  scale_fill_viridis_c()

(p1  |p2) /(p3  |p4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](starfish_mk.png) --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-rectangular bounding boxes with the sf package</title>
      <link>https://mkapur.netlify.app/post/sf-non-rect/sf-non-rect/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/sf-non-rect/sf-non-rect/</guid>
      <description>


&lt;p&gt;This is in the “so I don’t forget” category.&lt;/p&gt;
&lt;p&gt;For plotting, I wanted to clip an irregular shapefile (namely the EEZ off BC, Canada) based on a sloped line transect – meaning the simple approach of &lt;code&gt;sf_crop()&lt;/code&gt; and passing the bounds of a rectangle were insufficient. &lt;a href = &#34;https://r-spatial.github.io/sf/reference/st.html&#34;&gt; This link helped.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I accomplished this once I recognize that one can define a bounding box using a matrix with coordinates at each of the vertices of the desired shape; the terminal entry in the matrix should match the first, effectively outlining the shape you’d like to make. Keep in mind that the coordinates should be on the same scale (even pre-projection) as the shapefile which is going to be clipped with this polygon – hence the + 360 calls.&lt;/p&gt;
&lt;p&gt;On the left is the EEZ; on the right show the two different clips I’d like to make (blue and white). In the code, the blue and white triangles are the p11 and p12 objects, respectively. They haven’t yet been projected or changed into polygons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(sf) 
require(ggplot2)
require(dplyr)

load(&amp;quot;eez_nepac_regions.rda&amp;quot;)
regions &amp;lt;- eez_nepac_regions

xmin = -140 + 360
xmax = -125 + 360 
ymin = 45 
ymax = 55 
xdomain &amp;lt;- seq(xmin,xmax, length.out = 100) 
outerLower &amp;lt;- matrix(c(xmin, ymin,xmax,ymin,xmax,ymax,xmin, ymin), ncol=2, byrow=TRUE) 
pts = list(outerLower) 
pl1 = st_polygon(pts)

## note the point-switching
outerUpper &amp;lt;- matrix(c(xmin, ymin,xmin, ymax,xmax,ymax,xmin, ymin), ncol=2, byrow=TRUE) 
pts = list(outerUpper)
p12 = st_polygon(pts)

BCreg &amp;lt;- regions %&amp;gt;% filter(Region_Name == &amp;#39;British Columbia&amp;#39;)

par(mfrow = c(1,2))
plot(BCreg$geometry)
plot(pl1, col = &amp;#39;gold&amp;#39;)
plot(p12, add = TRUE, col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/sf-non-rect/2020-05-01-Non-Rectangular-Bounding-Boxes-with-the-sf-Package_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The clip step actually uses &lt;code&gt;sf::st_intersection(&lt;/code&gt;). To simplify things I pass the first argument as a subset of all the regions that includes the shapefile for only BC’s EEZ. This can then be plotted within ggplot2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(ggplot2)
BCangle_geom &amp;lt;- sf::st_sfc(list(p12), crs = &amp;quot;+proj=longlat +datum=WGS84 +no_defs&amp;quot;) 
BCangle_shape &amp;lt;- st_sf(BCangle_geom) 
B1 &amp;lt;- st_intersection(BCreg,BCangle_shape) 

BCangle_geom &amp;lt;- sf::st_sfc(list(pl1), crs = &amp;quot;+proj=longlat +datum=WGS84 +no_defs&amp;quot;) 
BCangle_shape &amp;lt;- st_sf(BCangle_geom) 
B2 &amp;lt;- st_intersection(BCreg,BCangle_shape)

require(marmap)
BCbath &amp;lt;- getNOAA.bathy(lon1 = -140, lon2 = -120,
                        lat1 = 45, lat2 = 65, resolution = 4)

splitLat &amp;lt;- function(sample_x = 25, xmin, xmax, ymin, ymax){
  ydiff = ymax - ymin
  xdiff = xmax - xmin
  int = ymin - ydiff/xdiff * xmin
  # split_y &amp;lt;- (ydiff/xdiff)*sample_x + int
  return(list(ydiff, xdiff, int))
}
xmin = -140
xmax = -125 
ydif = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[1]]
xdif = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[2]]
int = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[3]]

p1 &amp;lt;- ggplot(data = regions) +
  geom_sf(data = B1, fill = &amp;quot;gold&amp;quot;, alpha = 0.5, color = NA) +
  geom_sf(data = B2, fill= &amp;quot;blue&amp;quot;, alpha = 0.5, color = NA) +
  geom_sf(lwd = 1, col = &amp;#39;black&amp;#39;, fill = NA) +
  coord_sf(xlim = c(220, 240), ylim = c(30, 65)) +
  labs(x =&amp;quot;&amp;quot;,y=&amp;quot;&amp;quot;)+
  theme_classic(base_size = 14)
p2 &amp;lt;- autoplot(BCbath, geom=c(&amp;quot;r&amp;quot;, &amp;quot;c&amp;quot;), colour=&amp;quot;white&amp;quot;, size=0.05) + 
  scale_fill_etopo() +
  geom_abline(slope = ydif/xdif, intercept = int, col = &amp;#39;red&amp;#39;, lwd = 1.1) +
  
  labs(x =&amp;quot;&amp;quot;,y=&amp;quot;&amp;quot;)+
  theme_classic(base_size = 14)
require(patchwork)
p1 |p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/sf-non-rect/2020-05-01-Non-Rectangular-Bounding-Boxes-with-the-sf-Package_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bathy maps at right were made with marmap.&lt;/p&gt;
&lt;p&gt;Other crops throughout the domain, which rely only on purely vertical OR horizontal lines, can be accomplished using a simple call to &lt;code&gt;sf::st_crop()&lt;/code&gt;. This takes the arguments of lat-long borders of a rectangle.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse for simple CPUE standardizations</title>
      <link>https://mkapur.netlify.app/post/tidycpue/tidycpue/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/tidycpue/tidycpue/</guid>
      <description>


&lt;p&gt;My first CPUE standardization code was several hundred lines long. By hand, I fit individual glm models with different combinations of predictor variables, laboriously checking the AIC values of each added step and starting a new chunk of tested models based on the best one. I knew even then that some of this work could have been automated using something like MuMIn::dredge(), but that did not inspect the order of predictors added and interaction terms, among other things.&lt;/p&gt;
&lt;p&gt;From here at &lt;code&gt;rstudio::conf(2020)&lt;/code&gt; where I’m happy to be attending as a Diversity Scholar, I whipped up a tidyverse solution to these qualms. Max Kuhn et al have extensive dox explaining how this very basic (i.e. fixed effects) framework can be re-framed into a mixed-effects or stan setup. The code below is effectively a tenth of what I did before, and automates the checking of many models specified via fit_with using or ignoring interactive terms. At the end I use purrr::map to calculate AIC on all at once, extract and plot outputs. I load a few packages up top but really you could do this with fewer if you plotted in base and had your own AIC function.&lt;/p&gt;
&lt;p&gt;If you look into the cpue_fits object, you get a neat summary of every model specified, no for-loops required&lt;/p&gt;
&lt;p&gt;Disclaimer: this data is totally fake, and the point here isn’t whether or not the model makes the most sense from a CPUE standpoint. I fudged it a bit so the CPUE actually tracks fairly well with catch.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../tidycpue.png&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Tidy CPUE
require(lunar)
require(MuMIn)
require(ggplot2)
require(modelr)
require(tidyverse)
require(parsnip)

## generate fake data ----
landings &amp;lt;-  data.frame(expand.grid(YEAR = seq(1956,2015,1),  
                                    VESSEL = c(&amp;#39;Ariel&amp;#39;,&amp;#39;Jasmine&amp;#39;,&amp;#39;Cinderella&amp;#39;,&amp;#39;Elsa&amp;#39;))) %&amp;gt;%
  arrange(.,YEAR) %&amp;gt;%
  mutate(LBS = c(rnorm(0.5*nrow(.),44,4),rnorm(0.1*nrow(.),24,4),
                 rnorm(0.1*nrow(.),34,4),rnorm(0.3*nrow(.),30,4)),
         HOOKS = c(rnorm(0.5*nrow(.),65,4),rnorm(0.1*nrow(.),55,4),
                   rnorm(0.1*nrow(.),70,4),rnorm(0.3*nrow(.),80,4)),
         WIND_SPEED = c(rnorm(0.5*nrow(.),8.5,2),rnorm(0.5*nrow(.),5,4)),
         MOON_PHASE = rep(lunar::lunar.8phases,nrow(.)/8),
         MEAN_TEMP = c(rnorm(0.5*nrow(.),14,4),rnorm(0.1*nrow(.),40,4),
                       rnorm(0.1*nrow(.),20,4),rnorm(0.3*nrow(.),25,4)))

## modelr::fit_with
cpue_fits &amp;lt;- landings %&amp;gt;% 
  modelr::fit_with(lm, formulas(~LBS,
                                base = ~YEAR + VESSEL ,
                                interaction = ~VESSEL * MEAN_TEMP,
                                phase = add_predictors(base, ~MOON_PHASE),
                                full_no_int = add_predictors(base, ~.),
                                full_int = add_predictors(interaction,~.)
))

purrr::map(cpue_fits, parsnip::predict.model_fit, type = &amp;#39;conf_int&amp;#39;)

## extract formula one with the lowest AIC
best_formula &amp;lt;- cpue_fits[[which.min(purrr::map(cpue_fits,AIC))]][&amp;#39;call&amp;#39;][[1]]
best_mod &amp;lt;- linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(formula(best_formula), data = landings)
predict(lm_fit, landings, type = &amp;quot;conf_int&amp;quot;)

## bind predicts and CI to original df
pred_df &amp;lt;- landings %&amp;gt;%
  bind_cols(.,PREDICTS = predict(best_mod, landings)) %&amp;gt;%
  bind_cols(., predict(best_mod ,landings, type = &amp;quot;conf_int&amp;quot;)) 

## clean the DF to get mean annual values

pred_df_clean &amp;lt;- pred_df %&amp;gt;% group_by(YEAR) %&amp;gt;% 
  summarise(meanLBS = mean(LBS), meanCPUE = mean(.pred),
            meanCPUE.lwr = mean(.pred_lower),
            meanCPUE.upr = mean(.pred_upper))


ggplot(pred_df_clean, aes(x = YEAR)) +
  theme_minimal()+
  geom_line(aes(y = meanLBS), col = &amp;#39;black&amp;#39;, lwd = 1.2) +
  geom_point(aes(y = meanCPUE),col = &amp;#39;dodgerblue&amp;#39; ) + 
  geom_errorbar(aes(ymin = meanCPUE.lwr, ymax =meanCPUE.upr),
                col = &amp;#39;dodgerblue&amp;#39; ) +
  labs(x = &amp;#39;Year&amp;#39;, y = &amp;#39;Catch (lbs, black line) or CPUE (blue points and 95%CI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I studied for my PhD Quals</title>
      <link>https://mkapur.netlify.app/post/quals/how-studied-quals/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/quals/how-studied-quals/</guid>
      <description>


&lt;p&gt;Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Acquire materials early&lt;/strong&gt;. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I&#39;d spend on each. For example, my advisor assigned two textbooks -- I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Make a weekly gameplan&lt;/strong&gt;. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a hardcore paper-organization system&lt;/strong&gt;. If you can do this with printed sheets, I&#39;m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;Mendeley&lt;/strong&gt; (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they&#39;re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excel&lt;/strong&gt; (see my &lt;a href = &#34;https://maiakapur.netlify.app/post/how-i-keep-up-with-the-literature/&#34;&gt;How I keep up with the Literature&lt;/a&gt; post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included &amp;quot;claim&amp;quot;, &amp;quot;methods&amp;quot;, &amp;quot;major takeaways&amp;quot; and &amp;quot;larger scale conclusions&amp;quot;, where I connected that paper&#39;s findings with others in that subsection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word&lt;/strong&gt;. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like &amp;quot;What did Sekkar et al. say about X?&amp;quot;. Synthetic questions, such as &amp;quot;what have we learned about the influence of X on Y&amp;quot; or &amp;quot;Are Z considered good metrics of K?&amp;quot; are fruitful and will allow you to connect the readings.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Go beyond&lt;/strong&gt;. One of the best things I did during the exam prep was take a &lt;a href = &#34;https://www.coursera.org/learn/bayesian-statistics&#34;&gt; Coursera (free, online) course in Bayesian statistics&lt;/a&gt;. In add&#39;n to the Bayes textbook I was assigned, the course let me get &amp;quot;tutored&amp;quot; in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn&#39;t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It&#39;s easy to fool yourself that you &amp;quot;get it&amp;quot; after highlighting a textbook (for which the previous steps aren&#39;t as applicable), but MOOCs like Coursera enable you to test yourself in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pace thyself&lt;/strong&gt;. I read a quote last fall that said, &amp;quot;you can either contribute to the literature, or keep up with the literature&amp;quot;. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn&#39;t really &amp;quot;accomplished&amp;quot; anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you&#39;ll always need, and if you don&#39;t do it now, you won&#39;t have any more time when gainfully employed.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
