<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Maia Sosa Kapur</title>
    <link>https://mkapur.netlify.app/post/</link>
      <atom:link href="https://mkapur.netlify.app/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 10 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mkapur.netlify.app/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://mkapur.netlify.app/post/</link>
    </image>
    
    <item>
      <title>How I Keep Up With the Literature</title>
      <link>https://mkapur.netlify.app/post/how-i-keep-up-with-the-literature/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/how-i-keep-up-with-the-literature/</guid>
      <description>&lt;p&gt;My lab recently had a meeting where we discussed the nuts &amp;amp; bolts of performing a peer-review for a journal article, which got everyone talking about their preferred methods of reading a scientific paper. Luckily, we aren&amp;rsquo;t in bio-medicine or particle physics, but the volume of articles that emerge monthly in our field is still large, and it takes strategy to efficiently extract and scrutinize the information from a scientific article.&lt;/p&gt;
&lt;p&gt;One of my favorite specimens of PhD advice comes from the &lt;a href=&#34;http://www.chemistry-blog.com/wp-content/uploads/2010/06/Gassman-and-Meyers.pdf&#34;&gt;Gassman and Meyers memos&lt;/a&gt;, which posess the perfect melange of condecision, tough love and &amp;ldquo;kids these days&amp;rdquo; angst &amp;ndash; the advice, nonetheless, is great. It sticks because it stings a bit. Gassman writes:&lt;/p&gt;
&lt;blockquote&gt;It is a consistent observation on my part that people have more ideas about their research when they are writing their thesis than at any other time. This is generally due to the fact that they are finally doing the literature work they should have done early in their thesis work only at the time that they are trying to complete their dissertations. -- P.G. Gassman&lt;/blockquote&gt;
So, don&#39;t let that be you. Here is how I approach the literature, which has proved helpful for the last few years.
&lt;ol&gt;
	&lt;li&gt;Have a method to centralize new articles. I used the tips on the &lt;a href=&#34;https://fraserlab.com/2013/09/28/The-Fraser-Lab-method-of-following-the-scientific-literature/&#34;&gt;Fraser Lab website&lt;/a&gt; to set up a Feedly account (there are lots of other options) and subscribed to the four main journals in my field, plus Science, Nature, and PNAS. The algorithm learns your preferences quite quickly, and your queue will show articles that fit your niche after you &#39;check&#39; them off.&lt;/li&gt;
	&lt;li&gt;Check your centralizer every morning, and save the articles whose abstracts seem relevant as soon as you see them. It&#39;s fine to save articles on-the-fly, but I recommend scheduling &#39;deep reading&#39; periods of no less than one hour.&lt;/li&gt;
	&lt;li&gt;Go beyond highlighting/sticky notes. I use a set of spreadsheets, stored in Google Drive and sorted by topic, to ensure that I&#39;m actually engaged with what I&#39;m reading. They all follow the same format. One example  comes from my spreadsheet for studies of spatial model structure and general model mis-specification. The headings are hard to read in the screenshot, but they are &lt;strong&gt;Citation, Species, Mis-specification, Methods, Performance Measures, Conclusions, Similarities and Differences.&lt;/strong&gt; These can be tailored to your field or project, but the last two are the most crucial. In those columns, I compare the study to my thesis work, making it easy as I write my introductions/discussions to state who has done similar studies, and where the research gaps lie.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How Do Recruitment Deviates Work?</title>
      <link>https://mkapur.netlify.app/post/recdevs/how-rec-devs/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/recdevs/how-rec-devs/</guid>
      <description>


&lt;p&gt;For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. &lt;a href=&#34;https://doi.org/10.1139/f2011-092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1139/f2011-092&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I want to jot down the practical implementation behind the recruitment deviate for those working in Stock Synthesis or building their own models from scratch. The main topics to discuss are 1) The meaning of a recruitment deviate (abbreviated here to “rec-dev”), 2) how it plays in your dynamics equations, and 3) keeping an eye out for lognormal bias correction.&lt;/p&gt;
&lt;div id=&#34;what-is-a-rec-dev&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a rec-dev?&lt;/h1&gt;
&lt;p&gt;A recruitment deviate is how much a given year’s recruitment &lt;em&gt;deviates&lt;/em&gt; from what you’d expect given a stock-recruit relationship (in this post I’ll use the Beverton-Holt as an example). The idea is that there could be many reasons for a stock to produce a number of recruits which is different from the deterministic value defined by the current stock size (among other parameters). Reasons for this deviation could be environmental, and are generally unexplained – which is why the time series of rec devs is often interpreted as process error in your system (all the extra natural noise our model can’t capture).&lt;/p&gt;
&lt;p&gt;Let’s quickly refresh what we’re talking about when we say the “mean” or expected recruitment level. Again, using the Bev-Holt as an example:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
R_{expected} = \frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; is steepness, or the expected proportion of &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; anticipated at &lt;span class=&#34;math inline&#34;&gt;\(0.2SSB_0\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SSB_0\)&lt;/span&gt; are virgin recruitment and spawning biomass, respectively.&lt;/p&gt;
&lt;p&gt;If we make up some numbers for these parameters and plug them in, the resultant relationship might look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool. Now let’s consider that we have some notion of error around this deterministic value; this error could arise from uncertainty in the parameters of the stock recruit curve. Here, the shaded zone is the 95% confidence interval around our expected recruitment. 95% of recruitment values in a given year at a given SB should fall within that zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given this error, your observed recruitment in a given year may &lt;em&gt;deviate&lt;/em&gt; from the black curve somewhat. The new observed recruitments are shown as gold points; the line separating each point from the expected value is the raw value of the deviate itself.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviate at high biomass is going to be much larger than the deviate at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is diverging from expectation. For that reason, we are interested in modeling the errors, or deviates, as &lt;em&gt;lognormally distributed&lt;/em&gt;. Equation-wise, here’s what that looks like. Note that &lt;span class=&#34;math inline&#34;&gt;\(R_{det}\)&lt;/span&gt; is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{expected,y} = R_{det,y}exp(dev_y-\sigma^2/2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that &lt;span class=&#34;math inline&#34;&gt;\(-\sigma^2/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;refreshing-the-lognormal-distribution.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Refreshing the lognormal distribution.&lt;/h1&gt;
&lt;p&gt;If our devs are normally distributed with mean zero, &lt;span class=&#34;math inline&#34;&gt;\(exp(\mu = 0)\)&lt;/span&gt; is &lt;em&gt;lognormally&lt;/em&gt; distributed, shown in blue below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X ~ N(\mu,\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y = e^X\)&lt;/span&gt;, the expected value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, and the expected value of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; will be &lt;span class=&#34;math inline&#34;&gt;\(e^{\mu+\theta/2}\)&lt;/span&gt;, &lt;em&gt;not&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(e^{\mu}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(exp(X)\)&lt;/span&gt; is 1) never less than zero and 2) pretty asymmetrical. What this means mathematically is that the mean of &lt;span class=&#34;math inline&#34;&gt;\(exp(X)\)&lt;/span&gt; &lt;em&gt;is in fact not the same&lt;/em&gt; as &lt;span class=&#34;math inline&#34;&gt;\(exp(mean(X))\)&lt;/span&gt;. Confirm this for yourself by testing the following. They’ve been added to the plots above as vertical dashed lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(X) ## should be close to 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.01505518&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(X)) ## should be close to 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9850576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(exp(X)) ## uh-oh! It&amp;#39;s way bigger...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.626412&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mu+sigma/2) ##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.648721&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-correction-to-the-rescue&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bias correction to the rescue&lt;/h1&gt;
&lt;p&gt;Back to the recruitment example, we can pretend that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; here is our vector of recruitment deviates. We set the mean for &lt;span class=&#34;math inline&#34;&gt;\(dev_y\)&lt;/span&gt; to 0 in hopes that, on an “average” year, we’ll be multiplying &lt;span class=&#34;math inline&#34;&gt;\(R_{det,y}\)&lt;/span&gt; (deterministic recruitment) by &lt;span class=&#34;math inline&#34;&gt;\(exp(0) = 1\)&lt;/span&gt;, and get the expected value back. However, because we’ve discovered that &lt;span class=&#34;math inline&#34;&gt;\(exp(dev_y)\)&lt;/span&gt; is not symmetrical, we need to perform a &lt;em&gt;bias correction&lt;/em&gt; to confirm that we &lt;em&gt;approximately&lt;/em&gt; return the deterministic value when &lt;span class=&#34;math inline&#34;&gt;\(dev_y\)&lt;/span&gt; is zero. In other words, we are adjusting how we back-transform the deviates to confirm that the mean of those deviates is roughly one, on a normal scale.&lt;/p&gt;
&lt;p&gt;Check it out:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(X)) ## should be close to 1, as above&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9850576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9864687&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above).&lt;/p&gt;
&lt;p&gt;Imagining we had a time series of data which encompassed all the different &lt;span class=&#34;math inline&#34;&gt;\(SB\)&lt;/span&gt; values corresponding to the gold points above, we could plot the deviates through time (I randomly reordered the values):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/recDevs/2021-01-25-recDevs_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So now, the horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year &lt;em&gt;regardless of the stock size at hand&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why do we use ln(19)?</title>
      <link>https://mkapur.netlify.app/post/ln19/why-ln19/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/ln19/why-ln19/</guid>
      <description>


&lt;p&gt;Have you ever noticed that sneaky &lt;span class=&#34;math inline&#34;&gt;\(log(19)\)&lt;/span&gt; in your logistic equation for maturity or selectivity? What is up with that?&lt;/p&gt;
&lt;p&gt;Let’s take the example of a basic selectivity-at-age function, which uses the parameters &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{95}\)&lt;/span&gt;. These represent the ages where a fish has a probability 50% or 95% chance of being captured. &lt;em&gt;Read that again carefully.&lt;/em&gt; I took these parameters for granted, but the meaning of them is intertwined with the use of &lt;span class=&#34;math inline&#34;&gt;\(ln(19)\)&lt;/span&gt;, and if they change, so does the logged value.&lt;/p&gt;
&lt;p&gt;Here’s a selectivity plot with some made up numbers. The selectivity function I’m using is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_a = \langle 1+exp(-log(19)*\frac{a-a_{50}}{a_{95}-a_{50}}) \rangle ^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ln19/2021-01-25-ln19_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note how nicely these values extend between zero and one (on the y axis)? That’s because we’re really interested in working with the odds of being captured. Based on our parameter definitions, we need to ensure that the logistic function equals 0.95 when &lt;span class=&#34;math inline&#34;&gt;\(a = a_{95}\)&lt;/span&gt; (and similarly equals 0.5 at &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt;). To achieve this, we recognize the exponent of the log odds equals the odds ratio. You can think of the odds ratio as the probability of your event (getting captured) happening over the probability of it not happening. We can leverage this to force the logistic equation to scale the way we want.&lt;/p&gt;
&lt;p&gt;Since we’ve already defined &lt;span class=&#34;math inline&#34;&gt;\(a_{95}\)&lt;/span&gt; as the age at which we have a 95% probability of getting captured, the log-odds can be defined as &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{0.95}{0.05})\)&lt;/span&gt;. This could be rewritten as &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{19/20}{1/20})\)&lt;/span&gt; and simplifies to &lt;span class=&#34;math inline&#34;&gt;\(log(19)\)&lt;/span&gt;. This ensures that when &lt;span class=&#34;math inline&#34;&gt;\(a = a_{95}\)&lt;/span&gt;, the equation up top indeed returns 0.95, which is the logistic function evaluated at an odds ratio of 95%.&lt;/p&gt;
&lt;p&gt;Given this syntax you’ll see how we would never really need to correct for &lt;span class=&#34;math inline&#34;&gt;\(a_{50}\)&lt;/span&gt;, since the odds ratio of two events with 50% probability is actually zero (plug in 0.5 to see for yourself).&lt;/p&gt;
&lt;p&gt;Two notes:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Sometimes to ease model fitting, we will simplify the above equation by substituting &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; in place of the denominator (&lt;span class=&#34;math inline&#34;&gt;\(a_{95} - a_{50}\)&lt;/span&gt;), or estimate &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\frac{ln(19)}{a_{95} - a_{50}}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’d want to use the something like &lt;span class=&#34;math inline&#34;&gt;\(a_{75}\)&lt;/span&gt;, you’d need to update the log odds via &lt;span class=&#34;math inline&#34;&gt;\(log(\frac{0.75}{0.25})\)&lt;/span&gt; or ~ &lt;span class=&#34;math inline&#34;&gt;\(log(3)\)&lt;/span&gt;. Note that this ensures that you are getting a 75% selectivity at the corresponding age, but provides a distinct curve from before.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ln19/2021-01-25-ln19_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I studied for my PhD General Exam</title>
      <link>https://mkapur.netlify.app/post/generals/how-studied-general/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/generals/how-studied-general/</guid>
      <description>


&lt;p&gt;I am finally a PhD Candidate! You may want to check out my &lt;a href=&#34;https://maiakapur.netlify.app/post/quals/how-studied-quals&#34;&gt;“How I studied for my PhD Quals”&lt;/a&gt; post, which detailed my prep for a 5-day written examination I took in December. The General exam in my program is more about investigating 1) your preparedness and thinking regarding your proposed dissertation project, and 2) any gaps in your knowledge that should be addressed between now and your defense. This latter point was fairly disconcerting for me, as it’s hard to know if you’re doing well on a test designed to expose your shortcomings. Also, unlike the Qualifying exam, the format is an all-in-one, three-hour oral bonanza where there is no way to &lt;del&gt;scream into a paper bag&lt;/del&gt; take breaks/regroup after a particularly hard question. In any case, I passed, so here are some tips that can hopefully be generalizable to others taking exams in a similar format. I’ve split these into prep tips and in-the-moment advice.&lt;/p&gt;
&lt;div id=&#34;in-preparation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In Preparation &lt;br&gt;&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Simplify the guessing game&lt;/strong&gt;. Studying for this exam is a little harder because your committee may not give you any readings or additional topics to read about, leaving you to think you should know the entirety of your scientific field. Rather, your #1 priority should be to demonstrate you grasp the &lt;em&gt;essential concepts, assumptions and methods&lt;/em&gt; YOU will use in your disseration work, and have a notion of how your results will &lt;em&gt;fit into the literature at large&lt;/em&gt;. For that reason, I spent about 85% of my time making my proposal presentation clear and thorough, and prepared for questions like “How do you expect X to play into your results?”, “Have you considered using Y method?”, and “How is this different from work by Z?”. You’ll notice these are &lt;em&gt;open ended&lt;/em&gt; and designed to check that you aren’t just following a recipe for your study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The 15% basis&lt;/strong&gt; I studied on the unempirical assumption that only 15% of the committee’s confidence in my ability to complete my project would be based on my ability to respond to technical &amp;amp; conceptual questions about my field. I convinced myself that the &lt;em&gt;overwhelming factor which will convince them that I’m ready to progress on my dissertation&lt;/em&gt; is, well, the demonstrated progress I’ve actually made to this point (writing a proposal counts, too!). In other words, the committee ~is not going to~ did not fail me because I [did] get nervous, blank out, or can’t perfectly recall a concept in the moment, if I’ve otherwise demonstrated I can produce research and find answers when I need them. However, there are some “tricks” towards approaching that 15%, which I share in point #3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The 101 Slide &amp;amp; The “Warrants Future Research”&lt;/strong&gt;. I recognized that the questions people fear getting – the sudden-death, “gotcha” questions regarding a specific paper/equation/idea in your field, are less terrifying if you think of them the way the faculty think of them. I find such questions invariably fall into one of two categories.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;The first I call the “&lt;strong&gt;101 Slide&lt;/strong&gt;”. These are questions about basic concepts in your field which a 101 (introductory) course may have dedicated a lecture or lab exercise to – and that is the degree of response they are looking for! A classic my adviser throws around is, “How do you get the highest fishing yield from a cohort?”. Your brain will instantly rat-wheel around all the things that could lead one to not know the maximum yield…but the answer, literally, is “fish when the cohort biomass is maximized.” Exactly the response that would appear on an undergrad lecture slide! &lt;strong&gt;Full disclosure: I blanked out on a 101 question&lt;/strong&gt; fairly early in my exam and was pretty sure it was a fatal mistake. Depsite all the knowledge I had about this phenomenon, I still fell prey to the anxiety-brain-wipe…and survived. So you will, too 😄&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“&lt;strong&gt;Warrants future research&lt;/strong&gt;” are questions likely to appear in the discussion part of (the faculty’s) recent papers, and they are not expecting you to have a definitive answer. Questions beginning with “have you thought about…” or “what is the possibility…” are hints that the topic falls into this category and they’re asking you to spectulate. &lt;strong&gt;When in doubt, it looks good to start mentioning extant work on that topic&lt;/strong&gt;; the faculty may guide you towards which paper(s) they felt were conclusive, or be satisfied to know that you are aware it is an area of ongoing research. A strong response would suggest what experiments could be done to reduce speculation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The trick, obviously, regarding point 3 is knowing which category the question falls into given the topic. You should be generally familiar with open research questions in your field, emphasizing those &lt;em&gt;related directly to your project&lt;/em&gt; and &lt;em&gt;pet interests of your faculty members&lt;/em&gt;. For example, my disseration doesn’t do a lot of environmental modeling explicitly, but I am aware that there is an open debate regarding the influence of climate (environment) vs biomass on determining recruitment. I can name the folks involved in this work and what their general findings have been, and some of the challenges to implementing such research in our own context.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-the-moment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In the Moment&lt;/h1&gt;
&lt;p&gt;Here are some pointers – but know that nothing can calm you down come exam day like knowing you have put in the right amount of preparation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In my adviser’s words: “Don’t be chaotic”. This means don’t panic. Repeat the question back once asked to buy yourself some time.&lt;/li&gt;
&lt;li&gt;Any amount of silence feels way longer to you than it does to them. It looks better to pause and choose your words carefully than to ramble and hope you step on a right-answer mine.&lt;/li&gt;
&lt;li&gt;DO NOT overhydrate beforehand. You will need to be abiotic for three hours.&lt;/li&gt;
&lt;li&gt;Keep in mind you have probably read WAY more papers in depth in the last ~2 months than the faculty. Also keep in mind that while they could readily write down important equations/concepts that they &lt;em&gt;regularly work with&lt;/em&gt;, they really don’t know everything and likely could not perfectly answer every question asked by the others in the room if the roles were reversed. This isn’t about being arrogant, it’s about being realistic in what actually makes for a succesful scientist.&lt;/li&gt;
&lt;li&gt;Assume they want you to pass and move on. Failing looks bad on your advisor, ultimately – so believe that this is a group that wants to see you suceeed.&lt;/li&gt;
&lt;li&gt;“I don’t know” is a valid response. If you are afraid the question falls into the category of 3a (101 Slide) and you really draw a blank, you could choose to mention how you do/do not think it super relevant to your project, hence giving a bit of cover for perhaps why you didn’t review it in depth. Silence is better than rambling here.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Algebra for Equilibrium Recruitment</title>
      <link>https://mkapur.netlify.app/post/recalg/some-rec-algebra/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/recalg/some-rec-algebra/</guid>
      <description>


&lt;p&gt;I often find myself revisiting the various “re-arrangements” of the Beverton-Holt stock-recruitment relationship, particularly when moving between equilibrium quantities. I wanted to centralize some algebra for myself and anyone else who uses this relationship regularly.&lt;/p&gt;
&lt;div id=&#34;the-bev-holt-srr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bev-Holt SRR&lt;/h2&gt;
&lt;p&gt;Let’s start with some working definitions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array} 
{rr}
R_0 \quad virgin, \quad unfished \quad recruitment \\ SBPR_0 \quad virgin, \quad unfished \quad spawner \quad biomass-per-recruit \\ SSB_0 \quad virgin, \quad unfished \quad spawning \quad biomass \\ SBPR_F \quad  \quad biomass-per-recruit \quad at \quad F \neq 0 \\ SSB_F \quad spawning \quad biomass  \quad at \quad F \neq 0 \\ YPR_F \quad yield-per-recruit \quad at \quad F \neq 0 \\ \end{array} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First, let’s illustrate how one strings these quantities together in an age-structured model &lt;em&gt;assuming a stock-recruit relationship&lt;/em&gt; to derive equilibrium quantities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-per-recruit-quantities-at-various-f&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield-per-Recruit quantities at various F&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(YPR\)&lt;/span&gt; at your &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; value of interest, and also calculate these values when &lt;span class=&#34;math inline&#34;&gt;\(F = 0\)&lt;/span&gt;. This will vary slightly depending on the setup of your model, but in pseudo-math: &lt;span class=&#34;math inline&#34;&gt;\(YPR = \large \Sigma_a \small Fs_aN_aw_a\)&lt;/span&gt;, Where &lt;span class=&#34;math inline&#34;&gt;\(N_a\)&lt;/span&gt; is a proportional expected number-at-age, &lt;span class=&#34;math inline&#34;&gt;\(w_a\)&lt;/span&gt; is the expected weight-at-age, and &lt;span class=&#34;math inline&#34;&gt;\(s_a\)&lt;/span&gt; is fishery selectivity. &lt;span class=&#34;math display&#34;&gt;\[
SBPR = \large \Sigma_a \small E_aN_aw_a
\]&lt;/span&gt; Here &lt;span class=&#34;math inline&#34;&gt;\(E_a\)&lt;/span&gt; is some measure of your population’s fecundity at age. The resultant &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt; is going to vary with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(N_a\)&lt;/span&gt; will decline faster when &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is greater than zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;srr-in-terms-of-sbpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SRR in terms of SBPR&lt;/h2&gt;
&lt;p&gt;2a) Look at your stock-recruitment relationship. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are parameters of the Beverton-Holt SRR. We also leverage the definition of equilibrium stock spawning biomass as the product of unfished SBPR and Recruitment. You can confirm this makes sense by describing the equation with words: the virgin/unfished spawning biomass is the expected spawning biomass for one virgin/unfished recruit times the number of recruits. The same relationship will apply for equilibrium biomass. You will need to pick a number (or estimate) &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array} {rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
SSB_0 = SBPR_0 \times R_0 \\
SSB_{eq} = SBPR_{eq} \times R_{eq} \\
\end{array} 
\]&lt;/span&gt; 2b) Re-arrange the SRR to be in terms of &lt;span class=&#34;math inline&#34;&gt;\(SBPR\)&lt;/span&gt;. We do this by substitution. &lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
SSB_{eq} = SBPR_{eq} \times R_{eq} \\
R_{eq} =  \frac{SBPR_{eq} \times R_{eq} }{\alpha + \beta ( SBPR_{eq} \times R_{eq} )} \\
R_{eq}(\alpha + \beta ( SBPR_{eq} \times R_{eq} )) =  SBPR_{eq} \times R_{eq}  \\
\alpha + \beta ( SBPR_{eq} \times R_{eq} ) =  SBPR_{eq}   \\
\beta ( SBPR_{eq} \times R_{eq} ) =  SBPR_{eq} - \alpha  \\
 R_{eq}  = \frac{ SBPR_{eq} - \alpha}{\beta \times SBPR_{eq} }  \\
\end{array} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;alpha-beta-in-terms-of-steepness-recruitment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Alpha &amp;amp; Beta in terms of steepness &amp;amp; recruitment&lt;/h3&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sweet, now we need a nice definition for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Spoiler: these only vary by &lt;span class=&#34;math inline&#34;&gt;\(R_0\)&lt;/span&gt; and steepness, &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;, which is defined as the expected recruitment at 20% of &lt;span class=&#34;math inline&#34;&gt;\(SSB_0\)&lt;/span&gt; (hence the 0.2s). Here’s the derivation of those values. Recall that &lt;span class=&#34;math inline&#34;&gt;\(SSB_0 = SBPR_0 \times R_0\)&lt;/span&gt;, meaning &lt;span class=&#34;math inline&#34;&gt;\(SBPR_0\)&lt;/span&gt; can be substituted for &lt;span class=&#34;math inline&#34;&gt;\(\frac{SSB_0}{R_0}\)&lt;/span&gt;. Let’s start with beta:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}
{cc}
R_0 = \frac{SSB_0}{\alpha + \beta SSB_0} &amp;amp; original \\
\alpha + \beta SSB_0 = \frac{SSB_0}{R_0} &amp;amp; rearrange, \quad sub \\
\alpha =SBPR_0 - \beta SSB_0 &amp;amp; alpha \\
hR_0 = \frac{0.2SSB_0}{\alpha + 0.2 \beta SSB_0} &amp;amp; h @ 0.2SSB_0  \\
hR_0 = \frac{0.2SSB_0}{SBPR_0 - \beta SSB_0 + 0.2 \beta SSB_0} &amp;amp; substitute  \\
hR_0 = \frac{0.2SSB_0}{SBPR_0 - 0.8 \beta SSB_0} &amp;amp; simplify \\
hR_0SBPR_0 - 0.8hR_0\beta SSB_0= 0.2SSB_0 &amp;amp; multiply \quad denom\\
hSSB_0 - 0.8hR_0\beta SSB_0= 0.2SSB_0 &amp;amp; sub \quad SSB_0 = \tilde S R_0\\
h - 0.8hR_0\beta = 0.2 &amp;amp; div \quad SSB_0 \\
5h - 4hR_0\beta = 1 &amp;amp; mult \quad 5 \\
\frac{5h-1}{4hR_0} = \beta &amp;amp; solve \quad \beta \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And now for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, starting at the third equation from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{array}
{cc}
\alpha =SBPR_0 - \beta SSB_0 &amp;amp; from \quad above \\
\alpha =SBPR_0 -  \frac{5h-1}{4hR_0}SSB_0 &amp;amp; sub \quad \beta \\
\alpha =SBPR_0 -  \frac{5h-1}{4hR_0}SBPR_0 R_0 &amp;amp; sub \quad SSB_0 = SBPR_0 R_0  \\
\alpha =SBPR_0 -  \frac{5h-1}{4h}SBPR_0 &amp;amp; cancel \quad R_0  \\
\alpha =SBPR_0 \langle1-  \frac{5h-1}{4h}\rangle &amp;amp; factor  \\
\alpha = SBPR_0 \langle \frac{1-h}{4h}\rangle &amp;amp; rearrange   \\
\end{array}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To recap, &lt;span class=&#34;math inline&#34;&gt;\(R_{eq}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SBPR_{eq}\)&lt;/span&gt; are both varying with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;, but alpha and beta only change with &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R0\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(SBPR_{F=0}\)&lt;/span&gt; is fixed based on the life history of your species.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-round-stock-synthesis-recruitment-syntax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus Round: Stock Synthesis Recruitment Syntax&lt;/h2&gt;
&lt;p&gt;Ever wonder where the Beverton-Holt syntax in the Stock Synthesis manual comes from? It is simply the substitution &amp;amp; rearrangement of our derived values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; into the original stock-recruit relationship. The “trick” (not really) is just leaving the equation for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in terms of virigin biomass and recruitment.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
R_F = \frac{SSB_F}{\alpha + \beta SSB_F} \quad Bev-Holt\\
R_F = \frac{SSB_F}{\frac{SSB_0}{R_0} \langle \frac{1-h}{4h}\rangle + \frac{5h-1}{4hR_0} SSB_F} \quad substitute \\
R_F = \frac{SSB_F 4hR_0}{SSB_0(1-h)+ SSB_F(5h-1)} \quad flip \quad denom \\
\end{array} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yield-and-surplus-production-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yield and Surplus Production Curves&lt;/h2&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We are almost done. We have equations for equilibrium recruitment, and the definitions for the parameters of that equation. Note that equilibrium recruitment &lt;em&gt;will change&lt;/em&gt; with &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(SPBR_F\)&lt;/span&gt; changes, whereas alpha and beta will not (this is important later). We can now write down equations for our resultant equilibrium spawning biomass, and equilibrium yields: &lt;span class=&#34;math display&#34;&gt;\[ 
\begin{array}
{rr}
SSB_{eq} = R_{eq} \times {SBPR_F} \\
Yield_{eq} = R_{eq} \times {YPR_F}
\end{array} 
\]&lt;/span&gt; The “yield curve” is simply obtained by re-calculating these values across a range of Fs and plotting the results against input &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and/or resultant &lt;span class=&#34;math inline&#34;&gt;\(SSB\)&lt;/span&gt;. The height of the resultant dome is dependent on the value of &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; used, and the skewness in the biomass is related to selectivity. For the yield curve specifically, the peak occurs at the point just before natural mortality eclipses somatic growth, and the descending limb will be more “bendy” if you are selecting fish far before they mature. Another way of saying this is that the descending limb will accordingly change in height depending at the age of first capture.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My ggplot themes manifesto</title>
      <link>https://mkapur.netlify.app/post/ggplot-themes-manifesto/my-ggplot-themes-manifesto/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/ggplot-themes-manifesto/my-ggplot-themes-manifesto/</guid>
      <description>


&lt;p&gt;A few months ago I tweeted about the value of creating custom ggplot2 themes for easily synchronizing the look of all your figs for a given talk or manuscript. Some folks take this to another level and effectively have a personal aesthetic (see what I did there) that they use across their work, which I find compelling. I am obviously not a graphic designer, but I love beautiful plots, and wanted to consolidate some of the best resources + starter tips for others here. These range from out-of-the-box default themes within the ggplot2 package itself, to those made by others, to resources you can use to make your own more (or less) from scratch.&lt;/p&gt;
&lt;div id=&#34;straight-up-ggplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1 Straight up ggplot()&lt;/h1&gt;
&lt;p&gt;This is the ggplot2 default. Can we just agree that the grey background with nursery colors is not cute? I think Hadley did this on purpose to encourage us to find alternatives.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(ggplot2)
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are some themes that come pre-loaded in the ggplot2 package that you can invoke without installing anything else. They seem to follow the rule of subtraction – fewer gridlines, less borders, you get the idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theme_bw&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2 theme_bw()&lt;/h1&gt;
&lt;p&gt;Already loads better without that nasty grey background.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theme_minimal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3 theme_minimal()&lt;/h1&gt;
&lt;p&gt;I actually prefer the bounding box to the gridlines, but some folks find this spacious and clea&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next up are R packages on CRAN and/or Github. If you are like me, you need a pretty compelling reason to download someone’s random Github package purely for plotting needs – doubly so if the function of interest is buried within a massive package with a different purpose (I’m looking at you, oceanography). That said, here are some that I think are definitely worth adding to your toolkit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ggsidekicktheme_sleek&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4 ggsidekick::theme_sleek() &lt;/h1&gt;
&lt;p&gt;To quote my friend/colleague, “Sean Anderson is the GOAT”. He has brought the law of subtraction to the max with this one, which is great for presentations and homework assignments. I only wish the axis and legend text were larger (but more on how one could fix that below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remotes::install_github(&amp;quot;seananderson/ggsidekick&amp;quot;) ## for later R versions 
library(ggsidekick) 
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggsidekick::theme_sleek()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;hrbrthemestheme_modern_rc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5 hrbrthemes::theme_modern_rc()&lt;/h2&gt;
&lt;p&gt;Installing this is a heavier lift due to the required fonts, but I really like the light/dark possibilities here and the emphasis on nice typefaces. Would recommend for presentations exclusively. The one issue I have is that the dark version does not automatically lighten the box borders, something I have tried to fix in my personal versions (see below).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ggthemes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6 ggthemes&lt;/h1&gt;
&lt;p&gt;This is maintained by the ggplot2 folks and has lots of useful themes worth exploring. My favorite is solarized, which also has a light/dark option. I typically use the solarized-dark css from xaringanthemer for my slides, so popping in &lt;code&gt;ggthemes::theme_solarized_2(light = FALSE)&lt;/code&gt; guarantees my figures will match nicely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggthemes::theme_solarized_2() 
p2 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + 
  ggthemes::theme_solarized_2(light = FALSE)

library(patchwork)

p1/p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ggthemes also has a theme called “solid” which is literally just a background and nothing else, a la sparklines, if you are…chic?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(mtcars)
ggplot(mtcars, aes(x = mpg, y = qsec)) +
  geom_line(lwd = 1.1) +
  ggthemes::theme_solid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-beyond-your-own-themespackages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Going beyond: your own themes/packages&lt;/h1&gt;
&lt;p&gt;I mentioned the issue of avoiding package-burden if you’re just interested in a nice plotting function. Relatedly, sometimes another person has already made a cool ggtheme that &lt;em&gt;would&lt;/em&gt; be perfect, &lt;em&gt;if only&lt;/em&gt; the font were bigger/colors brighter/legend placed differently. That leads me to the most involved, yet most rewarding approach, which is to write and save your own ggplot themes! I have compiled all my faves into a new package called &lt;code&gt;kaplot&lt;/code&gt;, which includes the original &lt;code&gt;theme_black&lt;/code&gt; and &lt;code&gt;theme_mk()&lt;/code&gt; which I had placed on &lt;code&gt;kaputils&lt;/code&gt; previously. Since kaputils is more of a simulation and data science toolkit (for me), I wanted a lighterweight pacakge with only my plotting stuff. The material ranges from mostly original to downright plagarized, and basically involves me tweaking pre-existing themes to be exactly as I’d like them. Going forward I will probably make themes in there specific to given publications.&lt;/p&gt;
&lt;p&gt;If you aren’t ready to commit to a full package, I recommend editing a basic ggplot template (try &lt;a href = &#34;https://gist.github.com/jslefche/eff85ef06b4705e6efbc&#34;&gt;theme_black() here &lt;/a&gt;) and simply source()ing that script when you make your plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-new-plotting-package-kaplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7-9: My new plotting package, &lt;code&gt;kaplot&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;Here are some highlights.&lt;/p&gt;
&lt;div id=&#34;kaplottheme_solarized_mk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;kaplot::theme_solarized_mk()&lt;/h2&gt;
&lt;p&gt;My tweak to ggthemes which ensures that legend key backgrounds are not white. I use the dark version of this (light = FALSE) for presentations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kaplot)

p1 &amp;lt;- ggplot(mtcars, aes(x = mpg, y = hp, color = factor(gear))) +
  geom_boxplot() +
  kaplot::theme_solarized_mk(base_size = 14, light = TRUE)

p2 &amp;lt;- ggplot(mtcars, aes(x = mpg, y = hp, color = factor(gear))) +
  geom_boxplot() +
  kaplot::theme_solarized_mk(base_size = 14, light = FALSE)

p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](theme_solarized_mk.png) --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kaplottheme_mk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;kaplot::theme_mk()&lt;/h2&gt;
&lt;p&gt;I have now implemented &lt;a href = &#34;https://community.rstudio.com/t/adding-ggplot-themes-and-color-palettes-to-a-package/2418/3&#34;&gt;this rather complicated method &lt;/a&gt; to override the default color palette to the &lt;a href = &#34;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/&#34;&gt; cbbPalette&lt;/a&gt;, which is colorblind friendly and greyscale-compatible. The whole goal is to reduce the number of lines I need to write for each figure. See below for a final note on color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) +
  geom_boxplot() + 
  kaplot::theme_mk()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](theme_mk.png) --&gt; ## kaplot::foundational() I kind of like old-school figures that look like an engineering professor made them, like the one below. I put out a call on twitter to sort out how this can be done! Stay tuned… &lt;img src=&#34;4642.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;taste-the-rainbow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Taste the rainbow&lt;/h2&gt;
&lt;p&gt;Aside from cbbPalette, I have several go-to color palettes from across the web which I regularly use. Fortunately the viridis() pal is default with ggplot2, but there were a couple others I wanted to bring in automatically. These include &lt;a href = &#34;https://github.com/jakelawlor/PNWColors&#34;&gt;Jake Lawlor’s PNWColors&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Now the palettes are pre-loaded and can be called just like viridis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(ToothGrowth, aes(x = len, y = dose, fill = supp)) + 
  geom_boxplot() + 
  kaplot::theme_mk() + 
  kaplot::scale_fill_starfish() 
p2 &amp;lt;- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, fill = Species)) + 
  geom_boxplot() + kaplot::theme_mk() + 
  kaplot::scale_fill_sunset() 
p3 &amp;lt;- ggplot(mtcars[1:9,], aes(x = mpg, y = qsec, fill = rownames(mtcars[1:9,]))) + geom_bar(stat=&amp;#39;identity&amp;#39;) + 
  kaplot::theme_mk() + 
  kaplot::scale_fill_ipsum() 

p4 &amp;lt;- ggplot(USArrests, aes(x = UrbanPop, y = Murder, fill = UrbanPop)) + geom_bar(stat=&amp;#39;identity&amp;#39;) + 
  kaplot::theme_mk() + 
  scale_fill_viridis_c()

(p1  |p2) /(p3  |p4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/ggplot-themes-manifesto/2020-05-03-my-ggplot-themes-manifesto.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt; &lt;!-- ![](starfish_mk.png) --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Non-rectangular bounding boxes with the sf package</title>
      <link>https://mkapur.netlify.app/post/sf-non-rect/sf-non-rect/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/sf-non-rect/sf-non-rect/</guid>
      <description>


&lt;p&gt;This is in the “so I don’t forget” category.&lt;/p&gt;
&lt;p&gt;For plotting, I wanted to clip an irregular shapefile (namely the EEZ off BC, Canada) based on a sloped line transect – meaning the simple approach of &lt;code&gt;sf_crop()&lt;/code&gt; and passing the bounds of a rectangle were insufficient. &lt;a href = &#34;https://r-spatial.github.io/sf/reference/st.html&#34;&gt; This link helped.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I accomplished this once I recognize that one can define a bounding box using a matrix with coordinates at each of the vertices of the desired shape; the terminal entry in the matrix should match the first, effectively outlining the shape you’d like to make. Keep in mind that the coordinates should be on the same scale (even pre-projection) as the shapefile which is going to be clipped with this polygon – hence the + 360 calls.&lt;/p&gt;
&lt;p&gt;On the left is the EEZ; on the right show the two different clips I’d like to make (blue and white). In the code, the blue and white triangles are the p11 and p12 objects, respectively. They haven’t yet been projected or changed into polygons.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(sf) 
require(ggplot2)
require(dplyr)

load(&amp;quot;eez_nepac_regions.rda&amp;quot;)
regions &amp;lt;- eez_nepac_regions

xmin = -140 + 360
xmax = -125 + 360 
ymin = 45 
ymax = 55 
xdomain &amp;lt;- seq(xmin,xmax, length.out = 100) 
outerLower &amp;lt;- matrix(c(xmin, ymin,xmax,ymin,xmax,ymax,xmin, ymin), ncol=2, byrow=TRUE) 
pts = list(outerLower) 
pl1 = st_polygon(pts)

## note the point-switching
outerUpper &amp;lt;- matrix(c(xmin, ymin,xmin, ymax,xmax,ymax,xmin, ymin), ncol=2, byrow=TRUE) 
pts = list(outerUpper)
p12 = st_polygon(pts)

BCreg &amp;lt;- regions %&amp;gt;% filter(Region_Name == &amp;#39;British Columbia&amp;#39;)

par(mfrow = c(1,2))
plot(BCreg$geometry)
plot(pl1, col = &amp;#39;gold&amp;#39;)
plot(p12, add = TRUE, col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/sf-non-rect/2020-05-01-Non-Rectangular-Bounding-Boxes-with-the-sf-Package_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The clip step actually uses &lt;code&gt;sf::st_intersection(&lt;/code&gt;). To simplify things I pass the first argument as a subset of all the regions that includes the shapefile for only BC’s EEZ. This can then be plotted within ggplot2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(ggplot2)
BCangle_geom &amp;lt;- sf::st_sfc(list(p12), crs = &amp;quot;+proj=longlat +datum=WGS84 +no_defs&amp;quot;) 
BCangle_shape &amp;lt;- st_sf(BCangle_geom) 
B1 &amp;lt;- st_intersection(BCreg,BCangle_shape) 

BCangle_geom &amp;lt;- sf::st_sfc(list(pl1), crs = &amp;quot;+proj=longlat +datum=WGS84 +no_defs&amp;quot;) 
BCangle_shape &amp;lt;- st_sf(BCangle_geom) 
B2 &amp;lt;- st_intersection(BCreg,BCangle_shape)

require(marmap)
BCbath &amp;lt;- getNOAA.bathy(lon1 = -140, lon2 = -120,
                        lat1 = 45, lat2 = 65, resolution = 4)

splitLat &amp;lt;- function(sample_x = 25, xmin, xmax, ymin, ymax){
  ydiff = ymax - ymin
  xdiff = xmax - xmin
  int = ymin - ydiff/xdiff * xmin
  # split_y &amp;lt;- (ydiff/xdiff)*sample_x + int
  return(list(ydiff, xdiff, int))
}
xmin = -140
xmax = -125 
ydif = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[1]]
xdif = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[2]]
int = splitLat(sample_x = 25, xmin, xmax, ymin, ymax)[[3]]

p1 &amp;lt;- ggplot(data = regions) +
  geom_sf(data = B1, fill = &amp;quot;gold&amp;quot;, alpha = 0.5, color = NA) +
  geom_sf(data = B2, fill= &amp;quot;blue&amp;quot;, alpha = 0.5, color = NA) +
  geom_sf(lwd = 1, col = &amp;#39;black&amp;#39;, fill = NA) +
  coord_sf(xlim = c(220, 240), ylim = c(30, 65)) +
  labs(x =&amp;quot;&amp;quot;,y=&amp;quot;&amp;quot;)+
  theme_classic(base_size = 14)
p2 &amp;lt;- autoplot(BCbath, geom=c(&amp;quot;r&amp;quot;, &amp;quot;c&amp;quot;), colour=&amp;quot;white&amp;quot;, size=0.05) + 
  scale_fill_etopo() +
  geom_abline(slope = ydif/xdif, intercept = int, col = &amp;#39;red&amp;#39;, lwd = 1.1) +
  
  labs(x =&amp;quot;&amp;quot;,y=&amp;quot;&amp;quot;)+
  theme_classic(base_size = 14)
require(patchwork)
p1 |p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://mkapur.netlify.app/post/sf-non-rect/2020-05-01-Non-Rectangular-Bounding-Boxes-with-the-sf-Package_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bathy maps at right were made with marmap.&lt;/p&gt;
&lt;p&gt;Other crops throughout the domain, which rely only on purely vertical OR horizontal lines, can be accomplished using a simple call to &lt;code&gt;sf::st_crop()&lt;/code&gt;. This takes the arguments of lat-long borders of a rectangle.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse for simple CPUE standardizations</title>
      <link>https://mkapur.netlify.app/post/tidycpue/tidycpue/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/tidycpue/tidycpue/</guid>
      <description>


&lt;p&gt;My first CPUE standardization code was several hundred lines long. By hand, I fit individual glm models with different combinations of predictor variables, laboriously checking the AIC values of each added step and starting a new chunk of tested models based on the best one. I knew even then that some of this work could have been automated using something like MuMIn::dredge(), but that did not inspect the order of predictors added and interaction terms, among other things.&lt;/p&gt;
&lt;p&gt;From here at &lt;code&gt;rstudio::conf(2020)&lt;/code&gt; where I’m happy to be attending as a Diversity Scholar, I whipped up a tidyverse solution to these qualms. Max Kuhn et al have extensive dox explaining how this very basic (i.e. fixed effects) framework can be re-framed into a mixed-effects or stan setup. The code below is effectively a tenth of what I did before, and automates the checking of many models specified via fit_with using or ignoring interactive terms. At the end I use purrr::map to calculate AIC on all at once, extract and plot outputs. I load a few packages up top but really you could do this with fewer if you plotted in base and had your own AIC function.&lt;/p&gt;
&lt;p&gt;If you look into the cpue_fits object, you get a neat summary of every model specified, no for-loops required&lt;/p&gt;
&lt;p&gt;Disclaimer: this data is totally fake, and the point here isn’t whether or not the model makes the most sense from a CPUE standpoint. I fudged it a bit so the CPUE actually tracks fairly well with catch.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../tidycpue.png&#34; /&gt;

&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Tidy CPUE
require(lunar)
require(MuMIn)
require(ggplot2)
require(modelr)
require(tidyverse)
require(parsnip)

## generate fake data ----
landings &amp;lt;-  data.frame(expand.grid(YEAR = seq(1956,2015,1),  
                                    VESSEL = c(&amp;#39;Ariel&amp;#39;,&amp;#39;Jasmine&amp;#39;,&amp;#39;Cinderella&amp;#39;,&amp;#39;Elsa&amp;#39;))) %&amp;gt;%
  arrange(.,YEAR) %&amp;gt;%
  mutate(LBS = c(rnorm(0.5*nrow(.),44,4),rnorm(0.1*nrow(.),24,4),
                 rnorm(0.1*nrow(.),34,4),rnorm(0.3*nrow(.),30,4)),
         HOOKS = c(rnorm(0.5*nrow(.),65,4),rnorm(0.1*nrow(.),55,4),
                   rnorm(0.1*nrow(.),70,4),rnorm(0.3*nrow(.),80,4)),
         WIND_SPEED = c(rnorm(0.5*nrow(.),8.5,2),rnorm(0.5*nrow(.),5,4)),
         MOON_PHASE = rep(lunar::lunar.8phases,nrow(.)/8),
         MEAN_TEMP = c(rnorm(0.5*nrow(.),14,4),rnorm(0.1*nrow(.),40,4),
                       rnorm(0.1*nrow(.),20,4),rnorm(0.3*nrow(.),25,4)))

## modelr::fit_with
cpue_fits &amp;lt;- landings %&amp;gt;% 
  modelr::fit_with(lm, formulas(~LBS,
                                base = ~YEAR + VESSEL ,
                                interaction = ~VESSEL * MEAN_TEMP,
                                phase = add_predictors(base, ~MOON_PHASE),
                                full_no_int = add_predictors(base, ~.),
                                full_int = add_predictors(interaction,~.)
))

purrr::map(cpue_fits, parsnip::predict.model_fit, type = &amp;#39;conf_int&amp;#39;)

## extract formula one with the lowest AIC
best_formula &amp;lt;- cpue_fits[[which.min(purrr::map(cpue_fits,AIC))]][&amp;#39;call&amp;#39;][[1]]
best_mod &amp;lt;- linear_reg() %&amp;gt;% set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(formula(best_formula), data = landings)
predict(lm_fit, landings, type = &amp;quot;conf_int&amp;quot;)

## bind predicts and CI to original df
pred_df &amp;lt;- landings %&amp;gt;%
  bind_cols(.,PREDICTS = predict(best_mod, landings)) %&amp;gt;%
  bind_cols(., predict(best_mod ,landings, type = &amp;quot;conf_int&amp;quot;)) 

## clean the DF to get mean annual values

pred_df_clean &amp;lt;- pred_df %&amp;gt;% group_by(YEAR) %&amp;gt;% 
  summarise(meanLBS = mean(LBS), meanCPUE = mean(.pred),
            meanCPUE.lwr = mean(.pred_lower),
            meanCPUE.upr = mean(.pred_upper))


ggplot(pred_df_clean, aes(x = YEAR)) +
  theme_minimal()+
  geom_line(aes(y = meanLBS), col = &amp;#39;black&amp;#39;, lwd = 1.2) +
  geom_point(aes(y = meanCPUE),col = &amp;#39;dodgerblue&amp;#39; ) + 
  geom_errorbar(aes(ymin = meanCPUE.lwr, ymax =meanCPUE.upr),
                col = &amp;#39;dodgerblue&amp;#39; ) +
  labs(x = &amp;#39;Year&amp;#39;, y = &amp;#39;Catch (lbs, black line) or CPUE (blue points and 95%CI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How I studied for my PhD Quals</title>
      <link>https://mkapur.netlify.app/post/quals/how-studied-quals/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mkapur.netlify.app/post/quals/how-studied-quals/</guid>
      <description>


&lt;p&gt;Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Acquire materials early&lt;/strong&gt;. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I&#39;d spend on each. For example, my advisor assigned two textbooks -- I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Make a weekly gameplan&lt;/strong&gt;. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a hardcore paper-organization system&lt;/strong&gt;. If you can do this with printed sheets, I&#39;m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;strong&gt;Mendeley&lt;/strong&gt; (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they&#39;re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Excel&lt;/strong&gt; (see my &lt;a href = &#34;https://maiakapur.netlify.app/post/how-i-keep-up-with-the-literature/&#34;&gt;How I keep up with the Literature&lt;/a&gt; post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included &amp;quot;claim&amp;quot;, &amp;quot;methods&amp;quot;, &amp;quot;major takeaways&amp;quot; and &amp;quot;larger scale conclusions&amp;quot;, where I connected that paper&#39;s findings with others in that subsection.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Word&lt;/strong&gt;. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like &amp;quot;What did Sekkar et al. say about X?&amp;quot;. Synthetic questions, such as &amp;quot;what have we learned about the influence of X on Y&amp;quot; or &amp;quot;Are Z considered good metrics of K?&amp;quot; are fruitful and will allow you to connect the readings.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Go beyond&lt;/strong&gt;. One of the best things I did during the exam prep was take a &lt;a href = &#34;https://www.coursera.org/learn/bayesian-statistics&#34;&gt; Coursera (free, online) course in Bayesian statistics&lt;/a&gt;. In add&#39;n to the Bayes textbook I was assigned, the course let me get &amp;quot;tutored&amp;quot; in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn&#39;t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It&#39;s easy to fool yourself that you &amp;quot;get it&amp;quot; after highlighting a textbook (for which the previous steps aren&#39;t as applicable), but MOOCs like Coursera enable you to test yourself in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pace thyself&lt;/strong&gt;. I read a quote last fall that said, &amp;quot;you can either contribute to the literature, or keep up with the literature&amp;quot;. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn&#39;t really &amp;quot;accomplished&amp;quot; anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you&#39;ll always need, and if you don&#39;t do it now, you won&#39;t have any more time when gainfully employed.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
