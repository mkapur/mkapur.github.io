[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maia Sosa Kapur",
    "section": "",
    "text": "I am an applied scientist (mathematical statistician) at the Alaska Fisheries Science Center (NOAA, Seattle). I received my PhD from the the School of Aquatic and Fisheries Science at the University of Washington in the Punt Lab. My research was supported by a NMFS-Sea Grant Population Dynamics Fellowship awarded in 2019.\nPreviously I worked as a data scientist in the assessment program at the Joint Institute for Marine and Atmospheric Research at PIFSC (NOAA) in Honolulu and completed my MSc at the Hawaii Institute of Marine Biology. I have also worked in the Florida Keys and studied at the Smithsonian Tropical Research Institute in Bocas Del Toro, Panama while completing my BSc in Environmental Science at UC Berkeley.\nMy current research interests include: how to rapidly develop and deploy AI tools to improve resource management; how complex ecological forcings conspire to bias data and statistical estimation models; and how geospatial tools can inform risk management and decision science tools for connected systems impacted by climate."
  },
  {
    "objectID": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "href": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "title": "Why do we use ln(19)?",
    "section": "",
    "text": "Have you ever noticed that sneaky \\(log(19)\\) in your logistic equation for maturity or selectivity? What is up with that?\nLet’s take the example of a basic selectivity-at-age function, which uses the parameters \\(a_{50}\\) and \\(a_{95}\\). These represent the ages where a fish has a probability 50% or 95% chance of being captured. Read that again carefully. I took these parameters for granted, but the meaning of them is intertwined with the use of \\(ln(19)\\), and if they change, so does the logged value.\nHere’s a selectivity plot with some made up numbers. The selectivity function I’m using is:\n\\[\nS_a = \\langle 1+exp(-log(19)*\\frac{a-a_{50}}{a_{95}-a_{50}}) \\rangle ^{-1}\n\\]\n\n\n\n\n\nNote how nicely these values extend between zero and one (on the y axis)? That’s because we’re really interested in working with the odds of being captured. Based on our parameter definitions, we need to ensure that the logistic function equals 0.95 when \\(a = a_{95}\\) (and similarly equals 0.5 at \\(a_{50}\\)). To achieve this, we recognize the exponent of the log odds equals the odds ratio. You can think of the odds ratio as the probability of your event (getting captured) happening over the probability of it not happening. We can leverage this to force the logistic equation to scale the way we want.\nSince we’ve already defined \\(a_{95}\\) as the age at which we have a 95% probability of getting captured, the log-odds can be defined as \\(log(\\frac{0.95}{0.05})\\). This could be rewritten as \\(log(\\frac{19/20}{1/20})\\) and simplifies to \\(log(19)\\). This ensures that when \\(a = a_{95}\\), the equation up top indeed returns 0.95, which is the logistic function evaluated at an odds ratio of 95%.\nGiven this syntax you’ll see how we would never really need to correct for \\(a_{50}\\), since the odds ratio of two events with 50% probability is actually zero (plug in 0.5 to see for yourself).\nTwo notes:\n\nSometimes to ease model fitting, we will simplify the above equation by substituting \\(\\delta\\) in place of the denominator (\\(a_{95} - a_{50}\\)), or estimate \\(\\delta\\) as \\(\\frac{ln(19)}{a_{95} - a_{50}}\\). (Specifically, if you have the \\(a_{50}\\) and \\(\\delta\\) values from, say, a Stock Synthesis model, you can solve for \\(a_{95}\\) via \\(a_{95} = \\frac{-ln(19)}{\\delta}+a_{50}\\)).\nIf you’d want to use the something like \\(a_{75}\\), you’d need to update the log odds via \\(log(\\frac{0.75}{0.25})\\) or ~ \\(log(3)\\). Note that this ensures that you are getting a 75% selectivity at the corresponding age, but provides a distinct curve from before."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a federal Research Mathematical Statistician at the Alaska Fisheries Science Center (NOAA, Seattle). I received my PhD from the the School of Aquatic and Fisheries Science at the University of Washington in the Punt Lab. My research was supported by a NMFS-Sea Grant Population Dynamics Fellowship awarded in 2019.\nPreviously I worked as an Assessment Specialist at the Joint Institute for Marine and Atmospheric Research at PIFSC (NOAA) in Honolulu and completed my MSc at the Hawaii Institute of Marine Biology. I have also worked in the Florida Keys and studied at the Smithsonian Tropical Research Institute in Bocas Del Toro, Panama while completing my BSc in Environmental Science at UC Berkeley."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Applied Science",
    "section": "",
    "text": "You can find an up-to-date list of my peer-reviewed publications on Google Scholar.\nBecause much of my work involves risk assessment for and informs the management of industrial resources in the United States and international waters, a large body of my research is contained in grey literature: technical memoranda and formal reports. A list of these publications, and links (where available), is provided below.\nRisk and Status Assessments Used for Indusrial Fisheries Management\n\nSpies, I., Kapur, M., Barbeaux, S., Haltuch, M., Hulson, P., Ortiz, I., Spencer, L., and Lowe, S. Assessment of the Pacific Cod stock in the Aleutian Islands. North Pacific Fishery Management Council, 2024. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2024/AIpcod.pdf\nKapur, M. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2024. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2024/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2024. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2024/BSAIflathead.pdf\nKapur, M., Hulson, P. and Williams, B. Assessment of the Pacific Ocean Perch Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2023. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2023/GOApop.pdf\nKapur, M. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2023. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2023/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2023. https://www.npfmc.org/wp-content/PDFdocuments/SAFE/2023/BSAIflathead.pdf\nKapur, M. and Monnahan, C. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/BSAIflathead.pdf\nKapur, M. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/BSAIflathead.pdf\nKapur, M., Lee, Q., Haltuch, M., Gertserva, V., and Hamel, O. Status update of the U.S. sablefish stock in 2021. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. https://www.pcouncil.org/documents/2021/06/g-5-attachment-5-draft-status-of-sablefish-anoplopoma-fimbria-along-the-us-west-coast-in-2021-electronic-only.pdf/ 108 p.\nHaltuch, M., Johnson, K., Tolimieri, N, Kapur, M., and Castillo-Jordán, C. Status of the sablefish stock in U.S. waters in 2019. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. 398 p. https://www.pcouncil.org/wp-content/uploads/2019/10/sablefish_20191022.pdf\nKapur, M., Hamel, O. Catch-only update for China Rockfish in 2019.\nAdams, G. D., Kapur, M., McQuaw, K., Thurner, S., Hamel, O.S., Stephens, A. & Wetzel, C. R. 2019. Stock Assessment Update: Status of Widow Rockfish (Sebastes entomelas) Along the U.S. West Coast in 2019. Pacific Fishery Management Council, Portland, Oregon. https://www.pcouncil.org/wp-content/uploads/2019/10/2019WidowUpdate_final_clean.pdf\nHamel, O. Kapur, M., Catch-only update for Blackgill Rockfish in 2019.\nKapur, M., Fitchett, M., Yau, A., Carvalho F. 2019. 2018 Benchmark Stock Assessment of Main Hawaiian Islands Kona Crab. NOAA Tech Memo. NMFS-PIFSC-77, 114 p. https://doi.org/10.25923/7wf2-f040 Available at: https://www.fisheries.noaa.gov/resource/document/2018-benchmark-stock-assessment-main-hawaiian-islands-kona-crab\nISC, 2019 (contributor). 2019 Benchmark stock assessment and future projections for the Western and Central North Pacific striped marlin. Honolulu, Hawaii, USA. Available at: http://isc.fra.go.jp/pdf/ISC19/ISC19_ANNEX11_Stock_Assessment_Report_for_Striped_Marlin.pdf\nISC, 2018 (contributor). Stock Assessment of Shortfin Mako Shark in the North Pacific Ocean Through 2016 (No. ISC/18/ANNEX/15). Yeosu, Republic of Korea. Available at: http://isc.fra.go.jp/pdf/ISC18/ISC_18_ANNEX_15_Shortfin_Mako_Shark_Stock_Assessment_FINAL.pdf\nLangseth, B., Syslo, J., Yau, A., Kapur, M., Brodziak, J., 2018. Stock assessment for the main Hawaiian Islands Deep 7 bottomfish complex in 2018, with catch projections through 2022. NOAA Tech. Memo. NMFS-PIFSC 69, 217. Available at: https://repository.library.noaa.gov/view/noaa/17252\nWinker, H., Carvalho, F., Kapur, M., Parker, D., Kerwath, S., 2017. JABBA goes IOTC: Just Another Bayesian Biomass Assessment for Indian Ocean Blue shark and swordfish (No. IOTC-2017-WPB15-INF02). Available at: https://www.iotc.org/sites/default/files/documents/2017/10/IOTC-2017-WPM08-11_Rev_1.pdf\n\nTechnical Documents Submitted to Fisheries Management Organizations\n\nICES (contributor), 2024. Joint ICES-SEAwise Workshop to Quality Assure Methods to Incorporate Environmental Factors and Quantifying Ecological Considerations in Management Strategy Evaluation Tools (WKEcoMSE). ICES Scientific Reports. https://doi.org/10.17895/ICES.PUB.26661457\nHamel, O., Punt, A.E., Kapur, M. and Maunder, M. 2023. Natural Mortality: Theory, Estimation, and Application in Fishery Stock Assessment Models. U.S. Department of Commerce, NOAA Processed Report NMFSNWFSC-PR-2023-02. https://doi.org/10.25923/xqv2-2g73\nKapur, M., Connors, B., DeVore, J., Fenske, K., Haltuch, M., Key, M. 2021. Transboundary Sablefish Management Strategy Evaluation (MSE) Workshop Report. Pacific Fishery Management Council, Supplemental Informational Report 5. https://www.pcouncil.org/documents/2021/06/supplemental-informational-report-5-council-sponsors-successful-sablefish-management-strategy-evaluation-workshop.pdf/\nCarvalho, F., Kapur, M., Ijima, H. 2019. Comparison between Preliminary Stock Synthesis Models for the 2019 WCNPO Striped Marlin Stock Assessment. http://isc.fra.go.jp/pdf/BILL/ISC19_BILL_2/ISC19_BILLWG2_WP4.pdf\nFenske, K. H., Berger, A. M., Connors, B., Cope, J.M., Cox, S. P., Haltuch, M. A., Hanselman, D. H., Kapur, M., Lacko, L., Lunsford , C., Rodgveller, C., and Williams, B. 2019. Report on the 2018 International Sablefish Workshop. U.S. Dep. Commer., NOAA Tech. Memo. NMFS-AFSC-387, 107 p. Available at: https://www.afsc.noaa.gov/Publications/AFSC-TM/NOAA-TM-AFSC-387.pdf\nSculley, M., Kapur, M., Yau, A., 2018. Size composition for swordfish Xiphias gladius in the Hawaii-based pelagic longline fishery for 1995-2016 (No. WP-18-003). https://doi.org/10.7289/V5/WP-PIFSC-18-003\nSculley, M., Yau, A., Kapur, M., 2018. Standardization of the swordfish Xiphias gladius catch per unit effort data caught by the Hawaii-based longline fishery from 1994-2016 using generalized linear models. (No. WP-18-001). https://doi.org/10.7289/V5/WP-PIFSC-18-001\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. Performance evaluation of JABBA-Select against an age-structured simulator and estimation model (No. MARAM/IWS/2017/Linefish/P3 Performance). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P3.pdf\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. JABBA-Select: an alternative surplus production model to account for changes in selectivity and relative mortality from multiple fisheries (No. MARAM/IWS/2017/Linefish/P2). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P2.pdf\nKapur, M., Yau, A., 2018. Size Compositions and Sex Ratios of Oceanic Whitetip Sharks and Giant Manta Rays for Longline Fisheries in the Pacific Islands Region (DR-18-012). Honolulu, HI. https://doi.org/10.25923/67hf-bn93\nKapur, M., Brodziak, J.A., Fletcher, E.J., Yau, A.J., 2017. Summary of Life History and Stock Assessment Results for Pacific Blue Marlin, Western and Central North Pacific Striped Marlin, and North Pacific Swordfish. (No. WP-17-004). 10.7289/V5/WP-PIFSC-17-004 Available at: http://www.pifsc.noaa.gov/library/pubs/WP-17-004.pdf\nSculley, M.S., Brodziak, J.A., Yau, A.J., Kapur, M., 2017. An Exploratory Analysis of Trends in Swordfish (Xiphias gladius) Length Composition Data from the Hawaiian Longline Fishery (No. WP-17-002). 10.7289/V5/WP-PIFSC-17-002. Available at: https://repository.library.noaa.gov/view/noaa/14862/noaa_14862_DS1.pdf"
  },
  {
    "objectID": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "href": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "title": "How I studied for my qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a https://www.coursera.org/learn/bayesian-statistics (free, online) course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2021-01-25_recDevs/recdevs.html",
    "href": "posts/2021-01-25_recDevs/recdevs.html",
    "title": "How Do Recruitment Deviations Work?",
    "section": "",
    "text": "For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out: Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. https://doi.org/10.1139/f2011-092.\nHere I want to jot down the practical implementation behind the recruitment deviation. The topics covered here are 1) The meaning of a recruitment deviation (abbreviated here to “rec-dev”), 2) the common mathematical syntax for these values, and 3) the practical meaning of lognormal bias correction. This post will not address advanced implementation topics, such as likelihood penalties and bias correction ramps.\n\nWhat is a rec-dev?\nA recruitment deviation is how much a given year’s recruitment deviates from what you’d expect from average. This could be a static value (e.g., unfished recruitment), or the deterministic value given by a stock-recruit relationship (in this post I’ll use the Beverton-Holt [1957] as an example). The idea is that stock productivity is likely not the same every single year, and we need a flexible method to capture that variation in our model fitting procedures. Reasons for these deviations could be environmental, and are generally unexplained – which is why the time series of rec-devs is often interpreted as process error in your system (extra noise in the system not captured by other processes).\nLet’s quickly refresh what we’re talking about when we say the mean or expected recruitment level. Again, using the Bev-Holt as an example, this equation describes the expected number of recruits given a stock spawning biomass (SSB). (This syntax is what’s commonly used in Stock Synthesis models: check out my post on recruitment algebra for the derivation).\n\\[\nR_{expected} = \\frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)}\n\\]\n\\(h\\) is steepness, or the expected proportion of \\(R_0\\) anticipated at \\(0.2SSB_0\\); \\(R_0\\) and \\(SSB_0\\) are unfished recruitment and unfished spawning biomass, respectively. Given values for these parameters, the “mean” or “expected recruitment” level is what you’d get by plugging in those numbers to the values and reading off the curve:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, the recruitment in a given year may deviate from the black curve. We can visualize these in two ways. On the bottom left, I’ve invented a series of new recruitment values shown as gold points; the dotted line separating each point from the expected value is the raw value of the deviation itself (in numbers). A time series of these values is shown on the right. The blue point on both plots is the highest year – and you can see that since this is plotted in natural (not log) space it’s fairly hard to distinguish the smaller points from one another.\n\n\n\n\n\n\n\n\n\nLet’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviation at high biomass is going to be much larger than the deviation at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year \\(y\\) is diverging from expectation. For that reason, we are interested in modeling the errors, or deviations, as lognormally distributed. Equation-wise, here’s what that looks like. Note that \\(R_{det}\\) is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.\n\\[\nR_{expected,y} = R_{det,y}e^{(dev_y-\\sigma^2/2)}\n\\]\nNow we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that \\(-\\sigma^2/2\\).\n\n\nRefreshing the lognormal distribution.\nCheck out towardsdatascience.com for a good background read on this topic, which is not specific to recruitment deviations but rather a general property of the lognormal distribution.\nIf our deviations vector \\(\\vec r\\) is normally distributed with mean zero, \\(exp(\\vec r)\\) is lognormally distributed, shown in blue below:\n\n\n\n\n\n\n\n\n\nIf \\(r_y \\sim N(\\mu=0,\\sigma=1)\\) and \\(Y = e^{r_y}\\), the expected value of \\(r_y\\) is \\(\\mu\\) (zero), and the expected value of \\(Y\\) will be \\(e^{\\mu+\\sigma^2/2}\\) (1.6487213), not \\(e^{\\mu}\\).\nNote that \\(e^{r_y}\\), the blue values above, are 1) never less than zero and 2) asymmetrically distributed. This means mathematically is that the expected value, or mean, of \\(e^{r_y}\\) (blue dashed line above) is in fact not the same as \\(exp(mean(r_y))\\). Confirm this for yourself below.\n\n## the mean of X should be close to 0\nmean(X) \n\n[1] -0.005742354\n\n## that value exponentiated should therefore be close to 1\nexp(mean(X))\n\n[1] 0.9942741\n\n## yet the mean of the blue distribution is larger than 1...\nmean(exp(X)) \n\n[1] 1.639648\n\n##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)\nexp(mu+sigma^2/2)\n\n[1] 1.648721\n\n\nThis statistical reality makes for a problem if we calculate our annual recruitment by multiplying \\(R_{det}\\) by \\(exp(r_y)\\). If we don’t account for it, we end up with inflated annual recruitments because we would be multiplying by values typically greater than 1.\n\n\nBias correction to the rescue\nThe simple idea here is that we are applying a correction to the deviations so that when a year is “average” we’ll be multiplying \\(R_{det,y}\\) (deterministic recruitment in year y) by \\(exp(0) = 1\\), and get back the mean or expected value (the black curve in the first figure). Because we’ve discovered that \\(exp(r_y)\\) is not symmetrical, we need to perform a bias correction to confirm that we approximately return the \\(R_{det,y}\\) when \\(r_y\\) is zero.\nCheck it out:\n\n\n\n\n\n\n\n\n\n\n## as above, the mean is close to 1 without adjustment\nexp(mean(X)) \n\n[1] 0.9942741\n\n## ...though the actual mean is greater, by a factor of sigma^2/2\nmean(exp(X)) \n\n[1] 1.639648\n\nmean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!\n\n[1] 0.9944967\n\n\nOne last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above). The horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year regardless of the stock size at hand. You can also see that the blue point we saw in the earlier figures was not actually as great of an outlier as the original plot suggested\n\n\n\n\n\n\n\n\n\n\n\nBonus: why this isn’t perfect\nThis came up on my General Exam for my PhD. I was asked to walk through the basic logic of the recruitment deviation and why we apply bias correction. Then I was asked, “why is this wrong?”.\nPart of the answer lies in the fact that this is a conscious decision made by fisheries population dynamicists to make parts of our models behave in the way we want. Specifically, after applying our bias adjustment factor, the resulting recruitment timeseries will have a median value that is less than the deterministic curve shown above. In the Methot & Taylor paper linked up top, they mention that this is sensible since the average recruitment is most representative of a recruitment event’s long-term contribution to the population, “because most of the population biomass comes from the numerous recruits in the upper tail of the lognormal distribution”. If you’re working with a different data type or hope to gain information from the median behavior of this process, such an adjustment is not for you."
  },
  {
    "objectID": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "href": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "title": "Length-based Processes in an Age-Structured Model",
    "section": "",
    "text": "This is in the category of “stuff I thought I understood” until I didn’t.\nLet’s say you have an age-based model with integer-year bins a thru A+. The model is going to keep track of the dynamics of the fish population as they step through each age, but (annoyingly, perhaps) some of your modeled processes are length-based. This can occur when you are working with lots of different fleets, for which gear selectivity is better specified by length, or even the simple inclusion of our favorite \\(w_{l} = a l^b\\) calculation, which will require a notion of a fish length-at-age to play nicely within an age-structured model.\nConverting these length-based estimates (or quantities) into age-based estimates introduces a problem because of uncertainty in length-at-age, in other words, the fact that fish of length \\(l\\) could be of various ages \\(a\\). Let’s make up some data:\n\n## check out my post \"Why do we use ln(19)?\" for more info on this syntax:\nlogistic <- function(bin, b50, b75){\n  selage <- 1/(1+exp(-(log(15))*(bin-b50)/(b75-b50)))\n  return(selage)\n} \n\nage_bins <- 0:50\nlength_bins <- 1:65\nsigma_length <- 7.16\n\n## a logistic, length-based selectivity curve\nlength_selex <- sapply(length_bins, logistic, b50 = 30, b75 = 52)\n\nHere’s our selectivity curve; there’s a vertical line at l = 28cm which we’ll use for reference throughout this post.\n\n\n\n\n\nSo let’s say we’re interested in the selectivity at l = 28cm. In a length-based population dynamics model, we’d be set to just return the selectivity at l = 28cm for any fish within that respective bin (in 1-cm bins, fish lengths greater than 27cm and less than or equal to 28cm would all be assigned this value.)\nHowever, in an age-based model, we are instead interested in translating this curve into the expected selectivity-at-age. And given age \\(a\\), a fish could be a broad range of lengths \\(l\\), making it less clear which value we would “read off” the blue curve above. This variation is caused by the variable \\(\\sigma\\), which I’ve set at ~7cm – this reflects the standard deviation in length-at-age for our stock.\nHere’s how it looks in practice. First, we can generate our vonB-based expected length-at-age (here I assume \\(t_0\\) is 0, and am plotting the observed variation for 500 datasets, which should approximate the value for sigma_length specified above).\n\nvonb <- function(age, linf, kappa, sigma_length){\n  len <- linf*(1-exp(-kappa*(age)))+rnorm(1,0,sigma_length)\n}\n\n## simulate some datasets\nlength_at_age = matrix(NA, nrow = length(age_bins), ncol = 500)\nfor(i in 1:500){\n  length_at_age[,i] <- sapply(age_bins, vonb, linf = 44,\n                              kappa = 0.15, sigma_length)\n}\nlength_at_age <- bind_cols(age = age_bins, \n                           meanL = rowMeans(data.frame(length_at_age)),\n                           sdL = apply(data.frame(length_at_age),1,sd))%>%\n  mutate(lci = meanL-1.96*sdL, uci = meanL+1.96*sdL)\n\nggplot(length_at_age,\n       aes(x = age_bins)) +\n  geom_line(lwd = 1.1, col = 'grey', aes(y = meanL)) +\n  geom_ribbon(aes(ymin = lci, ymax = uci), fill = alpha('grey',0.3))+\n  geom_hline(yintercept = 28, linetype = 'dotted')+\n  labs(x = 'age', y= 'length')\n\n\n\n\nNotice how the horizontal line at 28cm corresponds to multiple ages within the confidence interval? How do we account for the fact that there is a chance that the the selectivity applicable to a given age can be characterized by the selectivity at several lengths, and how do we weight the applicability of that selectivity value given what we know about growth? Enter \\(\\psi_{l,a}\\), the probability of being in length bin \\(l\\) at age \\(a\\), a.k.a. the normal distribution \\(\\phi\\) of size-at-age.\n\\[\n\\begin{align}\n\\psi_{al} &= \\begin{array}\n   & \\Phi (1,\\tilde l_{a}, \\sigma)) & if \\space l = 0cm \\\\\n   \\Phi (l+1,\\tilde l_{a}, \\sigma)) - \\Phi (l,\\tilde l, \\sigma))& if \\space 0cm, < l < L+ \\\\\n   1-\\Phi (l,\\tilde l_{a}, \\sigma)) & if \\space l = L+\\\\\n\\end{array}\n\\end{align}\n\\]\nWhere \\(l\\) is the length bin in question, \\(\\tilde l\\) is the observed length-at-age, and \\(\\sigma\\) is the variation in growth. If we simply used a “lookup” method (which I have certainly done in a pinch!), and assigned the selectivity at age \\(a\\) to the selectivity at the expected length at age \\(a\\), we’d be introducing bias.\n\n## this function returns the distribution of expected length-at-age \n## given a bin and observed length-at-age\ngetP_al <- function(len_bin, len_obs, length_sigma){\n  pal = NA\n  if(len_bin == 1){\n    pal <- pnorm(len_bin, len_obs, length_sigma)\n  } else if (len_bin > min(length_bins) & len_bin < max(length_bins)){\n        pal <- pnorm(len_bin+1, len_obs, length_sigma) - \n          pnorm(len_bin, len_obs, length_sigma)\n  } else{\n        pal <- 1-pnorm(len_bin, len_obs, length_sigma)\n  }\n  pal\n}\nPAL <- matrix(NA, nrow = length(age_bins), ncol =  length(length_bins))\nfor(a in age_bins){\n  for(l in length_bins){\n    ## instead of using means, could re-do vonB with error turned off\n    PAL[a,l] <- getP_al(len_bin=l,len_obs=length_at_age$meanL[a],sigma_length )\n  }\n}\nlongPal <- data.frame(PAL) %>%\n  mutate(age_bins) %>%\n  reshape2::melt(id = 'age_bins') %>% \n  mutate(len =as.numeric(gsub(\"X\",\"\",variable))) \n\nHere’s a subset of what \\(\\psi_{al}\\) looks like for ages 5-10. Notice (again) how length = 28cm crosses multiple age curves. The good news is, \\(\\psi_{al}\\) has given us a sense of the relative probability of being age \\(a\\) given length \\(l\\). (It looks like \\(l = 28cm\\) is most likely a fish of age 7 or 8).\n\n\n\n\n\nTo put this all together, we need to compute the dot-product of the selectivity-at-length and the expected distribution of length-at-age.\nIn math, we’re doing this: \\[\nSel_a = \\sum_l Sel_l \\cdot \\psi_{al}\n\\]\nWhich is really just a cheap way of approximating the correct estimator of the expected selectivity of an animal at age \\(a\\):\n\\[\nSel_a = \\int_L Sel_L  \\psi_{aL} dL\n\\]\nIn words: for each length bin \\(l\\), multiply the selectivity at that length times the probability of being at age \\(a\\) given length \\(l\\). Sum these quantities across all length bins for each age bin.\nIn pictures: let’s say we are interested in \\(a = 5\\). We are going to multiply each point on the length-selectivity by its corresponding point (by color and ID number) on the probability of length-at-age-5 plot, then sum the results.\n\n\n\n\n\n\n\n\n\n\nIn code (still using \\(a = 5\\) as an example):\n\nsel_age <- merge(longPal,data.frame(cbind(length_selex,length_bins)),\n                 by.x = 'len', by.y = 'length_bins') %>%\n  ## inner product for each age-length combo\n  mutate(prod1 = length_selex*value) %>% \n  group_by(age_bins, len) %>%\n  summarise(dotProd = sum(prod1)) %>%\n  ungroup() %>%\n  group_by(age_bins) %>%\n  summarise(dotProdsum = sum(dotProd))\n\nggplot(sel_age,\n       aes(x = age_bins, y = dotProdsum)) +\n  geom_line(lwd = 1.1, col = 'grey22') +\n  labs(x = 'age', y= 'proportion selected', \n       main = 'Converted from length-at-age')"
  },
  {
    "objectID": "posts/2020-01-01-quals/quals.html",
    "href": "posts/2020-01-01-quals/quals.html",
    "title": "How I studied for my PhD qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a free, online Coursera course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "href": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "title": "Leveraging try_again() for error catching in MSEs",
    "section": "",
    "text": "This post is not a full breakdown of how to structure code for Management Strategy Evaluation (MSE) work. I’m certainly not a pro software engineer. Instead, I was pleased to find a clean way to deal with errors in the hierarchical nature of my MSE workflow that may be of use to others, or to future-me.\nHere’s a schematic of what’s happening in my MSE:\n\n\n\n\n\nAs anyone who’s done this type of work before knows, we definitely expect a subset of our simulation runs to ‘fail’ - that is, for the unique simulated data set (which differs here based upon what I’m calling \\(r\\))in year \\(y\\), the estimation procedure doesn’t pass whatever criteria we’ve set (could be convergence, a NaN gradient, etc).\nA simple break won’t help us here, since that would kill the loop mid-run and require us to manually re-start the higher-level function. A tryCatch also seems appealing, but this would still require recursion (since we’d have to embded a call to run_MSE() in the Catch wrapper, within run_MSE() itself…not ideal).\nOur desired behavior would have us hit play once on the run_MSE() function, and trust that any estimation model that fails will be safely handled:\n\n\n\n\n\nI was able to accomplish this elegantly using testthat::try_again(). I simply indicated the number of times I wanted to attempt my run_MSE() function, and included a stop() argument inside check_model() (so that a true error is thrown when the estimator fails).\nWhen testthat::try_again() detects this failure, it restarts the run_MSE() function, which increments along a vector of random seeds, until it hits something functional. This can be adapted to lots of purposes, and one could readily keep track of the identity/quantity of estimators that failed.\nEven better, I can pop this into a one-line call to replicate, which functionally requests a total number of sucesses equal to nReplicates. A key for this to work is to ensure whatever will be incremented along for each call to run_MSE() (in my case, the vector of seeds) is long enough to account for every attempted replicate times the potential number of tries. In the example below, this would be 99 tries x 100 nReplicates = 9900 seeds.\nsim <- replicate(nReplicates, {try_again(99,run_MSE(nprojyrs, spaceID))})"
  },
  {
    "objectID": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "href": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "title": "How I Keep Up With the Literature",
    "section": "",
    "text": "My lab recently had a meeting where we discussed the nuts & bolts of performing a peer-review for a journal article, which got everyone talking about their preferred methods of reading a scientific paper. Luckily, we aren’t in bio-medicine or particle physics, but the volume of articles that emerge monthly in our field is still large, and it takes strategy to efficiently extract and scrutinize the information from a scientific article.\nOne of my favorite specimens of PhD advice comes from the http://www.chemistry-blog.com/wp-content/uploads/2010/06/Gassman-and-Meyers.pdf”, which posess the perfect melange of condecision, tough love and “kids these days” angst – the advice, nonetheless, is great. It sticks because it stings a bit. Gassman writes:\n\nIt is a consistent observation on my part that people have more ideas about their research when they are writing their thesis than at any other time. This is generally due to the fact that they are finally doing the literature work they should have done early in their thesis work only at the time that they are trying to complete their dissertations. – P.G. Gassman\n\nSo, don’t let that be you. Here is how I approach the literature, which has proved helpful for the last few years.\n\n\nHave a method to centralize new articles. I used the tips on the https://fraserlab.com/2013/09/28/The-Fraser-Lab-method-of-following-the-scientific-literature/ website to set up a Feedly account (there are lots of other options) and subscribed to the four main journals in my field, plus Science, Nature, and PNAS. The algorithm learns your preferences quite quickly, and your queue will show articles that fit your niche after you ‘check’ them off.\n\n\nCheck your centralizer every morning, and save the articles whose abstracts seem relevant as soon as you see them. It’s fine to save articles on-the-fly, but I recommend scheduling ‘deep reading’ periods of no less than one hour.\n\n\nGo beyond highlighting/sticky notes. I use a set of spreadsheets, stored in Google Drive and sorted by topic, to ensure that I’m actually engaged with what I’m reading. They all follow the same format. One example comes from my spreadsheet for studies of spatial model structure and general model mis-specification. The headings are hard to read in the screenshot, but they are Citation, Species, Mis-specification, Methods, Performance Measures, Conclusions, Similarities and Differences. These can be tailored to your field or project, but the last two are the most crucial. In those columns, I compare the study to my thesis work, making it easy as I write my introductions/discussions to state who has done similar studies, and where the research gaps lie."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "title": "Ageing Error Matrices - some notes",
    "section": "",
    "text": "I revisit the inclusion of ageing error matrices frequently enough that I wanted to consolidate some notes here. None of this is new information; it is a resource for myself to get things straight, particularly when moving between the Stock Synthesis ageing error syntax and other setups like those used in our bespoke ADMB/TMB models. For this post, \\(a\\) will index true ages, \\(\\tilde a\\) indexes read ages.\nIn this post, I will talk through the math behind the ageing error matrix, and show how one can develop such a matrix (with R code) given parameters concerning the bias and imprecision of their age reads."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "title": "Ageing Error Matrices - some notes",
    "section": "Populating the Matrix",
    "text": "Populating the Matrix\nContinuing our example, let’s pass the bias and imprecision vectors we established above to see how \\(\\mathbf P\\) responds. I’m filling each element of \\(\\mathbf P\\) one at a time. Sometimes you will need to make an assumption about the first age group to avoid a zero.\n\n\n\nYou’ll want to sanity check that the columns sum to one; in other words, are all possible true ages \\(a\\) accounted for under each read age \\(\\tilde a\\)? Normalization, and occasionally some rounding to several decimals, are reasonable steps here.\n\nfor(a_obs in 1:abins){\n  P_aa[,a_obs] <- P_aa[,a_obs]/sum(P_aa[,a_obs])\n}\nall(round(colSums(P_aa))==1)\n\n[1] TRUE\n\n\nNow let’s confirm that the shape of the probabilities looks reasonable. Generally, I expect to see what I call a “slumpy worm”: values with higher, narrow peaks at early ages, slumping to flatter, shallower peaks at older ages. The slumpy worm corresponds to the notion that it’s more likely for a reader to get a precise read on a young fish (it’s easy to count a small handful of annuli), whereas it’s harder to age an older fish (many annuli tend to get blurry and bunched up, but **check with your life history program!*).\nThere should be a spike at the ends for the plus group and age-0, if applicable.\n\n\n\n\n\nAnd here’s what the actual populated matrix looks like:"
  },
  {
    "objectID": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html",
    "href": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html",
    "title": "Unusual Grad School Advice",
    "section": "",
    "text": "I’ve been ambivalent about writing graduate school “advice”, given that 1) I’m barely out of it and 2) tons of others have compiled their own listicles, blogs, articles, and books with more numerous and practical tips than I could ever come up with.\nInstead, I wanted to share a few thoughts about the American graduate school experience, and focus on “unusual” advice – things I haven’t heard much in the discourse.\nHow to approach this list, and perhaps a piece of meta-advice in itself, is that the point is not to make you the world’s best, or even a better, graduate student. The ideas below have an undercurrent of revisiting that which enriches and preserves all aspects of you, aspects that existed before and will continue to exist after your time in school.\nI reserve the right to change my mind.\n\nDon’t abandon the “Ph” part of the “PhD”. Regardless the topic you’re doing your dissertation on, don’t neglect Philosophy. It doesn’t have to be Socrates - but give yourself the gift of reading deep thinkers (used broadly), especially those far afield. Among many, I am blessed to have spent my grad school years in the intellectual company of activists, journalists, artificial intelligence ethicists, climate lawyers, poets, ultra-runners, fantasy novelists, and many others - through their writings. Why? I hesitate to say that engaging regularly with others’ ideas will make you a better writer and critical thinker (it will), but that is not the point. The time you spend uncovering knowledge about your PhD topic should not be at the expense of your deepening relationship to your own ways of knowing, your sense of justice, creativity, and purpose. Do not let these things lie frozen during your 20’s or 30’s; let them grow with you.\nYou are aging. This is not “advice”, exactly, rather a few things to consider on at least a quarterly basis. You have precious few years on this earth. If you allow it to, the stress of your degree will accelerate the aging process. What are you willing to sacrifice on the altar of this degree? Starting a family? Your partnership? Retirement savings? Are you certain that the rewards from completing your degree will merit these sacrifices? The only guarantee is when you graduate you will be a certain number of years older than you are now. What do you want to be doing regularly in service of that future self? What if you were to receive a terminal diagnosis this month, or in the month of your defense; do you still find this endeavor worth it, in that light? Do not view your graduate education as a dalliance, a detour, or a prolonged undergrad experience. These are (likely) the healthiest, freest, most impactful years (financially and otherwise) of your singular life. If that burden discomforts you, find out why.\nEconomize, don’t capitalize. Through my extra-curricular readings, I came upon the idea of internalized capitalism: where we conceive of ourselves as efficiency machines and work to claw back productivity from every possible cranny of our lives. This is obvious in the self-help literature, with endless titles imploring us to wake up earlier, automate our mundane tasks, and hustle ourselves into “successful” oblivion. My time with thinkers old & new taught me that this is a fallacy that benefits the economy but diseases and alienates us. My experience as a mother taught me how ridiculous it is to try to milk “more” from the time given to us in a day. So, resist the temptation to spend endless hours in the office, or to implement gimmicky apps/techniques to “maximize” your productivity. The best way to get things done is to remove distractions, and I recommend you achieve this in as blunt a way as possible. I have my distracting websites fully blocked on my work computer, and there was a time I would unplug my web router at home to ensure progress was made. You will be shocked how much of a frenzy you’re in on a normal workday.\nGet your adviser relationship straight. Endless ink has been spilled about the risks of studying under a toxic PI (this was fortunately not my case). Virtually everyone I know has experienced some degree of emotional distress from their advisor and/or committee relationship, that can linger sometimes years after defending. Why is this the case? Why is it so normal to have “bad bosses” in many other professions (to the point that comedic TV shows, books and social norms are built around this assumption), but folks in the sciences find the experience so damaging and all-consuming? It has to do with structural issues, mainly the fact that under the US system the PI has basically total control over your present and future life and job prospects. This is objectively true and even more crushing if you don’t have family wealth or support, or are on a visa. What you can control is how much emotional power you hand to your supervisor."
  },
  {
    "objectID": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html#populating-the-matrix",
    "href": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html#populating-the-matrix",
    "title": "Ageing Error Matrices - some notes",
    "section": "Populating the Matrix",
    "text": "Populating the Matrix\nContinuing our example, let’s pass the bias and imprecision vectors we established above to see how \\(\\mathbf P\\) responds. I’m filling each element of \\(\\mathbf P\\) one at a time. Sometimes you will need to make an assumption about the first age group to avoid a zero.\n\n\n\nYou’ll want to sanity check that the columns sum to one; in other words, are all possible true ages \\(a\\) accounted for under each read age \\(\\tilde a\\)? Normalization, and occasionally some rounding to several decimals, are reasonable steps here.\n\nfor(a_obs in 1:abins){\n  P_aa[,a_obs] <- P_aa[,a_obs]/sum(P_aa[,a_obs])\n}\nall(round(colSums(P_aa))==1)\n\n[1] TRUE\n\n\nNow let’s confirm that the shape of the probabilities looks reasonable. Generally, I expect to see what I call a “slumpy worm”: values with higher, narrow peaks at early ages, slumping to flatter, shallower peaks at older ages. The slumpy worm corresponds to the notion that it’s more likely for a reader to get a precise read on a young fish (it’s easy to count a small handful of annuli), whereas it’s harder to age an older fish (many annuli tend to get blurry and bunched up, but **check with your life history program!*).\nThere should be a spike at the ends for the plus group and age-0, if applicable.\n\n\n\n\n\nAnd here’s what the actual populated matrix looks like:"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Linking Spatial Processes to Covarying Paramters\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I got a New Job (under duress)\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnusual Grad School Advice\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgeing Error Matrices - some notes\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging try_again() for error catching in MSEs\n\n\n\n\n\n\n\n\n\nMSK\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLength-based Processes in an Age-Structured Model\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Keep Up With the Literature\n\n\n\n\n\n\n\n\n\nMSK\n\n\nFeb 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do Recruitment Deviations Work?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do Recruitment Deviations Work?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we use ln(19)?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD General Exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Algebra for Equilibrium Recruitment\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD qualifying exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html",
    "title": "Unusual Grad School Advice",
    "section": "",
    "text": "I’ve been ambivalent about writing graduate school “advice”, given that 1) I’m barely out of it and 2) tons of others have done so. What could I add?\nInstead, I wanted to share some “unusual” advice – things I haven’t heard much in the discourse. These are more relevant to the US doctoral experience.\nHow to approach this list, and perhaps a piece of meta-advice in itself, is that the point is not to make you the world’s best, or even a better, graduate student. The ideas below have an undercurrent of revisiting that which enriches and preserves all aspects of you, aspects that existed before and will continue to exist after your time in school."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#recall-the-ph-part-of-the-phd",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#recall-the-ph-part-of-the-phd",
    "title": "Unusual Grad School Advice",
    "section": "Recall the “Ph” part of the “PhD”",
    "text": "Recall the “Ph” part of the “PhD”\nRegardless the topic you’re doing your dissertation on, don’t neglect Philosophy. It doesn’t have to be Socrates - but expose yourself to the words and works of deep thinkers (used broadly), especially those far afield. I am blessed to have spent my grad school years in the intellectual company of activists, journalists, artificial intelligence ethicists, climate lawyers, poets, ultra-runners, fantasy novelists, and many others - through their writings, performances, and YouTube videos. Why do this? I hesitate to say that engaging regularly with others’ ideas will make you a better writer and critical thinker (it will), but that is not the point. The time you spend uncovering knowledge about your PhD topic should not be at the expense of your deepening relationship to your own ways of knowing, your sense of justice, creativity, and purpose. Do not let these parts of you lie frozen during your 20’s or 30’s; let them grow with you."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-are-aging",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-are-aging",
    "title": "Unusual Grad School Advice",
    "section": "You are aging",
    "text": "You are aging\nThis is not “advice”, exactly, rather a few things to consider on at least a quarterly basis. You have precious few years on this earth. If you allow it to, the stress of your degree will accelerate the aging process. What are you willing to sacrifice on the altar of this degree? Starting a family? Your partnership? Living somewhere you like? Retirement savings? Are you certain that the rewards from completing your degree will merit these sacrifices? When you graduate, you will be a certain number of years older than you are now. What do you want to be doing regularly in service of that future self? What if you were to receive a terminal diagnosis this month, or in the month of your defense; would you still find this endeavor worth it? Do not view your graduate education as a dalliance, a detour, or a prolonged undergrad experience. These are (likely) the healthiest, freest, most impactful years (financially and otherwise) of your only life. If that burden discomforts you, find out why."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#economize-dont-capitalize",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#economize-dont-capitalize",
    "title": "Unusual Grad School Advice",
    "section": "Economize, don’t capitalize",
    "text": "Economize, don’t capitalize\nThrough my extra-curricular readings, I came upon the idea of internalized capitalism: where we conceive of ourselves as efficiency machines and work to claw back productivity from every cranny of our day. Endless self-help titles implore us to wake up earlier, automate our mundane tasks, and hustle ourselves into “successful” oblivion. My time with thinkers old & new taught me that self-capitalization is a fallacy that benefits the economy but diseases and alienates us. My experience as a mother taught me how ridiculous it is to try to milk “more” from the time given to us in a day. So, resist the temptation to spend endless hours in the office, or to implement gimmicky apps/techniques to maximize your productivity. The best and only way to get things done is to remove distractions, and I recommend you achieve this in as blunt a way as possible. I have my distracting websites fully blocked on my work computer, and there was a time I would unplug my web router at home to ensure progress was made. You will be shocked how much of a frenzy you’re in on a normal workday."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#get-your-adviser-relationship-straight",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#get-your-adviser-relationship-straight",
    "title": "Unusual Grad School Advice",
    "section": "Get your adviser relationship straight",
    "text": "Get your adviser relationship straight\nEndless ink has been spilled about the risks of studying under a toxic PI (this was fortunately not my case). Virtually everyone I know has experienced some degree of emotional distress from their advisor and/or committee relationship, pain that can linger sometimes years after defending. Why is this the case? Why is it so normal to have “bad bosses” in many other professions (to the point that comedic TV shows, books and social norms are built around this assumption), but folks in the sciences find the experience so damaging and all-consuming? It has to do with structural issues, mainly the fact that under the US system the PI has near-total control over your present work-life and job prospects. This is even more crushing if you don’t have family wealth nor support, or are on a visa. What you can control is how much emotional power you hand to your supervisor. Do not allow them to take the place of your parent or inner self-critic. How your boss treats you has no correlation with your worthiness."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#impostor-syndrome-is-a-scam",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#impostor-syndrome-is-a-scam",
    "title": "Unusual Grad School Advice",
    "section": "Impostor syndrome is a scam",
    "text": "Impostor syndrome is a scam\nThis commencement address and article by Reshma Saujani explain: Impostor Syndrome is a myth, a distraction, a “scheme”. By pathologizing our insecurities we waste too much time trying to think our way into a better state of feeling, and more importantly, direct our focus at our own shortcomings instead of the structural problems that work against our own sense of belonging. The very core of professional-level scientific inquiry is based upon an assumption of ignorance. We should feel uninformed, ignorant and curious most of the time, but the impostor syndrome industry wants that healthy awareness of our temporary intellectual shortcomings to metastasize into generalized personal insecurity and doubt. Which becomes yet another problem for us to solve. So, avoid using shorthand for your experiences. Instead of “I’m suffering from Impostor Syndrome” try “I found this problem set really hard, and didn’t feel supported by the professor when I asked for help”. Then you can disentangle the aspects you can control from those that are structural. This also enables us, future mentors, teachers, and scientific professionals, to ask ourselves: how would this institution look if it were built to support someone like me?"
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-finish-graduate-school-when-you-are-ready-to-be-done",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-finish-graduate-school-when-you-are-ready-to-be-done",
    "title": "Unusual Grad School Advice",
    "section": "“You finish graduate school when you are ready to be done”",
    "text": "“You finish graduate school when you are ready to be done”\nA close mentor of mine shared this with me in late 2019 at an international conference. I didn’t really get it at the time, and at first thought it was wrong-headed (i.e., “I’m ready to be done right now, but nobody else agrees!” and “Being ready isn’t the same as the analyses going well…”). In retrospect, they were right. At least 80% of my dissertation progress happened within <30% of the five years I spent in graduate school. The rest of the time, I was working on side projects, dealing with illness, raising my child, taking in family during the pandemic, or performing duties for my main full-time job. The final burst of effort that brought me over the finish line was largely inspired by the looming deadline my work had imposed, and a difficult family situation the preceding fall which reminded me how important it was that I finished, ASAP. The point is that it is a challenge of the US doctoral system that the deadlines and project goals are amorphous and flexible. Most people struggle in that context, and it’s totally unlike the rest of the working world. So, create a hard stop for yourself - “I’m moving in August” (and start making arrangements), “I want to start my family next year” (and start making arrangements), “I’m applying for my dream job in six months” (and start writing the application). This might be the nudge you need to make things happen."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#references",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#references",
    "title": "Unusual Grad School Advice",
    "section": "References",
    "text": "References\nSome books, art, philosophy and things that moved me deeply or simply helped during graduate school\n\nHow to Do Nothing and Saving Time: Discovering a Life Beyond the Clock, both by Jenny Odell. She is an exemplary interdisciplinary thinker and opened my mind to new ways of being.\nRun to be Visible - Lydia Jennings (YouTube) A stunning representation of the difficulties of completing a degree during the pandemic, and the parallels between long-distance running and getting a PhD in a world not built for your success.\nAlma (YouTube) - Instrumental piece by Omar Sosa.\nAlma (YouTube) - Song by El Caribefunk (see translated lyrics if non-Spanish speaking)\nOn Native Land exhibit at the Tacoma Art Museum, particularly Galiano Island by Barbara Boldt (below). This seemingly straightforward landscape contains much about the passage of time and the consequences of sacrifice. Does the water shape the rocks, or do the rocks shape the water? Do the conditions of our life define us, or do we shape conditions to our subconscious selves? Could both be true?\n\n And also: Can equilibrium exist on any real timescale, whether within a human’s life, or the geological life of a beach break? How might this relate to the reliability of a population model that exchanges individuals among areas? I spent many hours on the bench before this painting in March of 2023 while my MSE simulations ran on the server. My dissertation findings regarding “cryptic depletion” (of a vulnerable fish population) was directly informed by my perception of the hidden, holey crags causing the froth at the waterside.\n\nAll books by Cal Newport, particularly “Deep Work” (see minimizing distraction above).\nGabor Maté - Chronic Illness and Stress (YouTube) Our habits program our neuro-physio-immunological experiences. See “You are aging” above."
  },
  {
    "objectID": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html",
    "href": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html",
    "title": "Linking Spatial Processes to Covarying Paramters",
    "section": "",
    "text": "There are lots of real-life processes that we know are driven by the environment, and exhibit covariance amongst the parameters we use to model them. People interested in forecasting are deeply curious about how to best model these processes, but I find that folks (especially in ecology) tend to overthink this stuff. We build highly complex agent-based models to represent relationships between the environment, demographic traits, and population dynamics, without considering simpler ways of representing these relationships. These approaches have their place, but shouldn’t be the default tool when we want to investigate high-level questions regarding management and policy outcomes. Even if we aren’t 100% certain on the mechanism, or even the magnitude, of these connections, building cleaner & quicker simulations is the ticket to knowledge-building.\nIn this post, I discuss an intuitive way for us to work with co-varying process parameters that avoids building a complex mechanistic model, and the considerations when selecting values that are “as extreme as” your underlying spatial/environmental process."
  },
  {
    "objectID": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#multivariate-simulation",
    "href": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#multivariate-simulation",
    "title": "Linking Spatial Processes to Covarying Paramters",
    "section": "Multivariate Simulation",
    "text": "Multivariate Simulation\nThis first step illustrates how we’d generate a “cloud” of values corresponding to \\(W_\\infty\\) (asymptotic weight), \\(k\\) (growth rate) and \\(t_0\\) (age at zero weight, which could be negative), and display the resultant curves. We need to specify \\(mu_\\theta\\), mean values for each of these parameters, and \\(\\Sigma\\), the covariance matrix. It’ll be a square matrix with dimensions given by the length of \\(\\theta\\).\nTo briefly review: \\[\n\\Sigma = \\begin{bmatrix} var(\\theta_1) & Cov(\\theta_2,\\theta_1) &  Cov(\\theta_n,\\theta_1) \\\\\nCov(\\theta_2,\\theta_1) & var(\\theta_2) &  Cov(\\theta_2,\\theta_3)  \\\\\n  Cov(\\theta_3,\\theta_1) &   Cov(\\theta_3,\\theta_2)&  var(\\theta_2)   \\\\\n\\end{bmatrix}\n\\]\nThe diagonal elements \\(var(\\theta_n)\\) represent the variance of an individual parameter. The off-diagonal elements are the covariances: a representation of the directional relationship, or joint variability, of two random variables. Large values mean that the two parameters tend to mirror each other well (if positive) or inversely (if negative). Values closer to zero mean they are less responsive to one another.\nWe can leverage the covariance matrix to ensure that when we generate random draws representing our von B parameters, we’re conserving the biological fact that \\(W_\\infty\\) and \\(k\\) exhibit negative correlation. The example below gives a very explict walkthrough of how these values could be drummed up from scratch; in practice you’d likely use empirical data or a meta-analytic tool like FishLife.\n\nmu_theta &lt;- c(5, 0.3, -0.5) ## Winf, K, t0\nvcov &lt;- diag(c(0.05, 0.05, 0.2)) # Variance-covariance of mu_theta\n\n# Define the variances of each parameter \nvar1 &lt;- 0.05\nvar2 &lt;- 0.05\nvar3 &lt;- 0.05\n\n# Define the covariances\ncov12 &lt;- sqrt(var1 * var2) * -0.2  #  negative correlation\ncov13 &lt;- sqrt(var1 * var3) * 0.1   # Weak positive correlation\ncov23 &lt;- sqrt(var2 * var3) * 0.1   # Weak positive correlation\n\n# Construct the variance-covariance matrix\nvcov_matrix &lt;- matrix(c(var1, cov12, cov13,\n                        cov12, var2, cov23,\n                        cov13, cov23, var3), nrow = 3, byrow = TRUE) \n\n## simulate random draws of the parameters\nsim &lt;- MASS::mvrnorm(1e3, mu_theta, vcov_matrix)"
  },
  {
    "objectID": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#invoking-drift",
    "href": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#invoking-drift",
    "title": "Simulating the Climate and Multivariate Responses",
    "section": "Invoking Drift",
    "text": "Invoking Drift\nNow let’s imagine that we have some environmental forcing on \\(W_\\infty\\) and want to ensure we are working with corresponding values for \\(k\\) and \\(t0\\) in the appropriate “zone” of the multivariate distribution.\nIn statistical terms, it’s straightforward to re-simulate values of \\(\\theta_n\\) conditional on one parameter, \\(\\theta_1\\). We just re-define the new mean and covariance using the values from our matrix:\n\\[\nE(\\theta_n  | \\theta_1 ) \\sim  N(\\mu_{\\theta_n  | \\theta_1 } , \\sigma_{\\theta_n  | \\theta_1 })\n\\]\n\\[\n\\mu_{\\theta_n  | \\theta_1 } = \\mu_{\\theta_n} + cov(\\theta_n  , \\theta_1 ) \\frac{var_{\\theta_n}}{var_{\\theta_1}} (x - \\mu_{\\theta_1})\n\\]\n\\[\n\\sigma_{\\theta_n  | \\theta_1 } = var_{\\theta_n}  \\sqrt{1 - cov(\\theta_n , \\theta_1 )^2 }\n\\]\nWe can either do this by specifying a fixed, specific value of \\(W_\\infty\\), as in the vertical strip of points on the lefthand plot below. Alternatively, we can use the probability contours – recall that any given interval will be defined by an ellipse for 2d data. The latter can be accomplished via mnormt::dmnorm().\nThis is an important distinction, because when we think of ‘extreme’ outcomes in a purely statistical sense, we are really describing extreme curves: realizations of the weight-at-age relationship that are characterized by unlikely combinations of parameters, not necessarily by unusually large values for individual parameters. This can be seen in the ring of red points below; many of them represent pretty typical (average) values of each parameter.\n\n\n\n\n\n\n\n\n\nWith this approach, the researcher now has more explicit control about how to represent the linkage between the spatial (environmental) process and the parameter “lookup”. In either case, the extremity of a climate process (in this case obtained using the marginal distribution) at a certain point can be used to determine either a) the marginal probability of an individual parameter, as in the first option above, or b) the joint probability of two or more parameters, as in the second option.\nThe second option is more thorough – characterizing all the possible combinations that meet your statistical threshold – but the first option might be the better choice for simulation studies because it allows us to represent environmental impacts in an intuitive, directional manner."
  },
  {
    "objectID": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#linking-to-a-spatial-process",
    "href": "posts/2025-03-10-simulating-bivariate/20250311-climatebivariate.html#linking-to-a-spatial-process",
    "title": "Linking Spatial Processes to Covarying Paramters",
    "section": "Linking to a spatial process",
    "text": "Linking to a spatial process\nNow let’s imagine that we have some environmental forcing on \\(W_\\infty\\) and want to ensure we are working with corresponding values for \\(k\\) and \\(t0\\) in the appropriate “zone” of the multivariate distribution.\nIn statistical terms, it’s straightforward to re-simulate values of \\(\\theta_n\\) conditional on one parameter, \\(\\theta_1\\). We just re-define the new mean and covariance using the values from our matrix:\n\\[\nE(\\theta_n  | \\theta_1 ) \\sim  N(\\mu_{\\theta_n  | \\theta_1 } , \\sigma_{\\theta_n  | \\theta_1 })\n\\]\n\\[\n\\mu_{\\theta_n  | \\theta_1 } = \\mu_{\\theta_n} + cov(\\theta_n  , \\theta_1 ) \\frac{var_{\\theta_n}}{var_{\\theta_1}} (x - \\mu_{\\theta_1})\n\\]\n\\[\n\\sigma_{\\theta_n  | \\theta_1 } = var_{\\theta_n}  \\sqrt{1 - cov(\\theta_n , \\theta_1 )^2 }\n\\] Here’s what the map & associated deviations might look like. The red circles are highlighting a specific point in space, and the associated value of the spatial process (on the right). The probability associated with this observation is what we’ll use to determine new values of our parameters.\n\nWe can either do this by specifying a fixed, specific value of \\(W_\\infty\\) corresponding to the quantile of the environmental deviation circled above. This approach is shown in the vertical strip of points on the left-hand plot below. Alternatively, we can use the probability contours – recall that any given interval will be defined by an ellipse for 2d data. The latter can be accomplished via mnormt::dmnorm() and is shown on the right-hand plot below.\nThis is an important distinction, because when we think of ‘extreme’ outcomes in a purely statistical sense, we are really describing extreme curves: realizations of the weight-at-age relationship that are characterized by unlikely combinations of parameters, not necessarily by unusually large values for individual parameters. This can be seen in the ring of red points below; many of them represent pretty typical (average) values of each parameter.\n\n\n\n\n\n\n\n\n\nWith this approach, the researcher now has more explicit control about how to represent the linkage between the spatial (environmental) process and the parameter “lookup”. In either case, the extremity of a climate process (in this case obtained using the marginal distribution) at a certain point can be used to determine either a) the marginal probability of an individual parameter, as in the first option above, or b) the joint probability of two or more parameters, as in the second option.\nThe second option is more thorough – characterizing all the possible combinations that meet your statistical threshold – but the first option might be the better choice for simulation studies because it allows us to represent environmental impacts in an intuitive, directional manner."
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "Media & Talks",
    "section": "",
    "text": "Video: AFSC Science Seminar 2025\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSablefish MSE Editor’s Choice & Featured Story for NOAA Fisheries\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo: Quantitative Seminar 2024\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Interview on the ‘Fisheries Podcast’\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo: Invited talk to the Policy Simulation Library\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJABBA Bayesian Model Makes International News\n\n\n\n\n\n\n\n\n\nDec 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo: Interview with STEMworks HI\n\n\n\n\n\n\n\n\n\nDec 11, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "media/05-12-2024-fisheriespod/fisheriespod.html",
    "href": "media/05-12-2024-fisheriespod/fisheriespod.html",
    "title": "My Interview on the ‘Fisheries Podcast’",
    "section": "",
    "text": "This week The Fisheries Podcast chats with Dr. Maia Sosa Kapur, a Research Mathematical Statistician with the NOAA Alaska Fisheries Science Center. Listen in to hear how Maia found her dream career in stock assessment, what the day-to-day looks like for stock assessment work, cool research projects Maia has led and contributed to (including sablefish research and crab harvest in Native Hawaiian fishponds), and more! If you’ve ever experienced a bit of an identity crisis while pursuing your career, this is the episode for you!\nMain point: There are no prerequisites to succeeding in this field!\nListen Here."
  },
  {
    "objectID": "media/12-11-2015-STEMWorksHI/stemworks.html",
    "href": "media/12-11-2015-STEMWorksHI/stemworks.html",
    "title": "Video: Interview with STEMworks HI",
    "section": "",
    "text": "STEMworks™ helps prepare Hawai’i students for rewarding and sustainable career opportunities and promoting GIS awareness and education. We offer annual GIS training programs for teachers and students in November and at our Hawaii STEM Conference. Students also explore GIS through our annual 6-week STEMworks™ Summer Internship program.\nGeographic Information Systems (GIS) is becoming an integral skill set for the 21st century employee. The Department of Labor estimates that 10 million jobs will utilize GIS technologies over the next 10 years. From government to private industry, GIS is utilized in industries such as agriculture, defense, surveying, retail, marketing, conservation, telecommunications, law enforcement, public health, airports and aviation, education, and much more.\nProfessionals share how they use GIS in their respective fields. Visit the K-12 Hawai’i GIS Hub to stay connected to Hawai’i’s GIS community!"
  },
  {
    "objectID": "media/2024-06-04-sablefishPR/sablefishpr.html",
    "href": "media/2024-06-04-sablefishPR/sablefishpr.html",
    "title": "Sablefish MSE Editor’s Choice & Featured Story for NOAA Fisheries",
    "section": "",
    "text": "Our sablefish MSE was included as a Feature Story on NOAA fisheries – check it out here. The associated publication was also selected for Editor’s Choice by the Canadian Journal of Fisheries and Aquatic Sciences."
  },
  {
    "objectID": "media/12-04-2017-JABBA/jabba.html",
    "href": "media/12-04-2017-JABBA/jabba.html",
    "title": "JABBA Bayesian Model Makes International News",
    "section": "",
    "text": "The impacts of the JABBA model1 are gaining widespread attention, with recent press releases close to home with NOAA Fisheries and as far as South Africa with this feature story from Cape Town News."
  },
  {
    "objectID": "media/12-04-2017-JABBA/jabba.html#footnotes",
    "href": "media/12-04-2017-JABBA/jabba.html#footnotes",
    "title": "JABBA Bayesian Model Makes International News",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWinker, H., Carvalho, F., Kapur, M., 2018. JABBA: Just Another Bayesian Biomass Assessment. Fish. Res. 204, 275–288. https://doi.org/10.1016/j.fishres.2018.03.010↩︎"
  },
  {
    "objectID": "media/2024-05-28-quantsem/quantsem.html",
    "href": "media/2024-05-28-quantsem/quantsem.html",
    "title": "Video: Quantitative Seminar 2024",
    "section": "",
    "text": "This talk describes the work we’re doing with ev-OSMOSE, an agent-based multispecies ecosystem model that accounts for genetic processes under future emissions scenarios."
  },
  {
    "objectID": "media/10-31-2023-PSL/psl.html",
    "href": "media/10-31-2023-PSL/psl.html",
    "title": "Video: Invited talk to the Policy Simulation Library",
    "section": "",
    "text": "Here’s a talk I gave on JABBA to the Policy Simulation Library, a collection of models and other software for public-policy decisionmaking."
  },
  {
    "objectID": "media/2025-04-01-afscSem/quantsem.html",
    "href": "media/2025-04-01-afscSem/quantsem.html",
    "title": "Video: Quantitative Seminar 2024",
    "section": "",
    "text": "This is a joint seminar I gave with my colleage Grant Adams on our efforts to implement data science principles in applying AI/ML tools for demographic projections."
  },
  {
    "objectID": "media/2025-04-01-afscSem/afscsem.html",
    "href": "media/2025-04-01-afscSem/afscsem.html",
    "title": "Video: AFSC Science Seminar 2025",
    "section": "",
    "text": "This is a joint seminar I gave with my colleage Grant Adams on our efforts to implement data science principles in applying AI/ML tools for demographic projections."
  },
  {
    "objectID": "posts/2020-06-03-generals/2020-06-03-genex.html",
    "href": "posts/2020-06-03-generals/2020-06-03-genex.html",
    "title": "How I studied for my PhD General Exam",
    "section": "",
    "text": "I am finally a PhD Candidate! You may want to check out my “How I studied for my PhD Quals” post, which detailed my prep for a 5-day written examination I took in December. The General exam in my program is more about investigating 1) your preparedness and thinking regarding your proposed dissertation project, and 2) any gaps in your knowledge that should be addressed between now and your defense. This latter point was fairly disconcerting for me, as it’s hard to know if you’re doing well on a test designed to expose your shortcomings. Also, unlike the Qualifying exam, the format is an all-in-one, three-hour oral bonanza where there is no way to scream into a paper bag take breaks/regroup after a particularly hard question. In any case, I passed, so here are some tips that can hopefully be generalizable to others taking exams in a similar format. I’ve split these into prep tips and in-the-moment advice.\n\nIn Preparation \n\nSimplify the guessing game. Studying for this exam is a little harder because your committee may not give you any readings or additional topics to read about, leaving you to think you should know the entirety of your scientific field. Rather, your #1 priority should be to demonstrate you grasp the essential concepts, assumptions and methods YOU will use in your disseration work, and have a notion of how your results will fit into the literature at large. For that reason, I spent about 85% of my time making my proposal presentation clear and thorough, and prepared for questions like “How do you expect X to play into your results?”, “Have you considered using Y method?”, and “How is this different from work by Z?”. You’ll notice these are open ended and designed to check that you aren’t just following a recipe for your study.\nThe 15% basis I studied on the unempirical assumption that only 15% of the committee’s confidence in my ability to complete my project would be based on my ability to respond to technical & conceptual questions about my field. I convinced myself that the overwhelming factor which will convince them that I’m ready to progress on my dissertation is, well, the demonstrated progress I’ve actually made to this point (writing a proposal counts, too!). In other words, the committee ~is not going to~ did not fail me because I [did] get nervous, blank out, or can’t perfectly recall a concept in the moment, if I’ve otherwise demonstrated I can produce research and find answers when I need them. However, there are some “tricks” towards approaching that 15%, which I share in point #3.\nThe 101 Slide & The “Warrants Future Research”. I recognized that the questions people fear getting – the sudden-death, “gotcha” questions regarding a specific paper/equation/idea in your field, are less terrifying if you think of them the way the faculty think of them. I find such questions invariably fall into one of two categories.\n\nThe first I call the “101 Slide”. These are questions about basic concepts in your field which a 101 (introductory) course may have dedicated a lecture or lab exercise to – and that is the degree of response they are looking for! A classic my adviser throws around is, “How do you get the highest fishing yield from a cohort?”. Your brain will instantly rat-wheel around all the things that could lead one to not know the maximum yield…but the answer, literally, is “fish when the cohort biomass is maximized.” Exactly the response that would appear on an undergrad lecture slide! Full disclosure: I blanked out on a 101 question fairly early in my exam and was pretty sure it was a fatal mistake. Depsite all the knowledge I had about this phenomenon, I still fell prey to the anxiety-brain-wipe…and survived. So you will, too r emoji::emoji('smile')\n“Warrants future research” are questions likely to appear in the discussion part of (the faculty’s) recent papers, and they are not expecting you to have a definitive answer. Questions beginning with “have you thought about…” or “what is the possibility…” are hints that the topic falls into this category and they’re asking you to spectulate. When in doubt, it looks good to start mentioning extant work on that topic; the faculty may guide you towards which paper(s) they felt were conclusive, or be satisfied to know that you are aware it is an area of ongoing research. A strong response would suggest what experiments could be done to reduce speculation.\n\n\nThe trick, obviously, regarding point 3 is knowing which category the question falls into given the topic. You should be generally familiar with open research questions in your field, emphasizing those related directly to your project and pet interests of your faculty members. For example, my disseration doesn’t do a lot of environmental modeling explicitly, but I am aware that there is an open debate regarding the influence of climate (environment) vs biomass on determining recruitment. I can name the folks involved in this work and what their general findings have been, and some of the challenges to implementing such research in our own context.\n\n\nIn the Moment\nHere are some pointers – but know that nothing can calm you down come exam day like knowing you have put in the right amount of preparation.\n\nIn my adviser’s words: “Don’t be chaotic”. This means don’t panic. Repeat the question back once asked to buy yourself some time.\nAny amount of silence feels way longer to you than it does to them. It looks better to pause and choose your words carefully than to ramble and hope you step on a right-answer mine.\nDO NOT overhydrate beforehand. You will need to be abiotic for three hours.\nKeep in mind you have probably read WAY more papers in depth in the last ~2 months than the faculty. Also keep in mind that while they could readily write down important equations/concepts that they regularly work with, they really don’t know everything and likely could not perfectly answer every question asked by the others in the room if the roles were reversed. This isn’t about being arrogant, it’s about being realistic in what actually makes for a succesful scientist.\nAssume they want you to pass and move on. Failing looks bad on your advisor, ultimately – so believe that this is a group that wants to see you suceeed.\n“I don’t know” is a valid response. If you are afraid the question falls into the category of 3a (101 Slide) and you really draw a blank, you could choose to mention how you do/do not think it super relevant to your project, hence giving a bit of cover for perhaps why you didn’t review it in depth. Silence is better than rambling here."
  },
  {
    "objectID": "posts/2021-01-25_recDevs/recdevs.knit.html",
    "href": "posts/2021-01-25_recDevs/recdevs.knit.html",
    "title": "How Do Recruitment Deviations Work?",
    "section": "",
    "text": "For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out: Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. https://doi.org/10.1139/f2011-092.\nHere I want to jot down the practical implementation behind the recruitment deviation. The topics covered here are 1) The meaning of a recruitment deviation (abbreviated here to “rec-dev”), 2) the common mathematical syntax for these values, and 3) the practical meaning of lognormal bias correction. This post will not address advanced implementation topics, such as likelihood penalties and bias correction ramps.\n\nWhat is a rec-dev?\nA recruitment deviation is how much a given year’s recruitment deviates from what you’d expect from average. This could be a static value (e.g., unfished recruitment), or the deterministic value given by a stock-recruit relationship (in this post I’ll use the Beverton-Holt [1957] as an example). The idea is that stock productivity is likely not the same every single year, and we need a flexible method to capture that variation in our model fitting procedures. Reasons for these deviations could be environmental, and are generally unexplained – which is why the time series of rec-devs is often interpreted as process error in your system (extra noise in the system not captured by other processes).\nLet’s quickly refresh what we’re talking about when we say the mean or expected recruitment level. Again, using the Bev-Holt as an example, this equation describes the expected number of recruits given a stock spawning biomass (SSB). (This syntax is what’s commonly used in Stock Synthesis models: check out my post on recruitment algebra for the derivation).\n\\[\nR_{expected} = \\frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)}\n\\]\n\\(h\\) is steepness, or the expected proportion of \\(R_0\\) anticipated at \\(0.2SSB_0\\); \\(R_0\\) and \\(SSB_0\\) are unfished recruitment and unfished spawning biomass, respectively. Given values for these parameters, the “mean” or “expected recruitment” level is what you’d get by plugging in those numbers to the values and reading off the curve:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, the recruitment in a given year may deviate from the black curve. We can visualize these in two ways. On the bottom left, I’ve invented a series of new recruitment values shown as gold points; the dotted line separating each point from the expected value is the raw value of the deviation itself (in numbers). A time series of these values is shown on the right. The blue point on both plots is the highest year – and you can see that since this is plotted in natural (not log) space it’s fairly hard to distinguish the smaller points from one another.\n\n\n\n\n\n\n\n\n\nLet’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviation at high biomass is going to be much larger than the deviation at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year \\(y\\) is diverging from expectation. For that reason, we are interested in modeling the errors, or deviations, as lognormally distributed. Equation-wise, here’s what that looks like. Note that \\(R_{det}\\) is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.\n\\[\nR_{expected,y} = R_{det,y}e^{(dev_y-\\sigma^2/2)}\n\\]\nNow we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that \\(-\\sigma^2/2\\).\n\n\nRefreshing the lognormal distribution.\nCheck out towardsdatascience.com for a good background read on this topic, which is not specific to recruitment deviations but rather a general property of the lognormal distribution.\nIf our deviations vector \\(\\vec r\\) is normally distributed with mean zero, \\(exp(\\vec r)\\) is lognormally distributed, shown in blue below:\n\n\n\n\n\n\n\n\n\nIf \\(r_y \\sim N(\\mu=0,\\sigma=1)\\) and \\(Y = e^{r_y}\\), the expected value of \\(r_y\\) is \\(\\mu\\) (zero), and the expected value of \\(Y\\) will be \\(e^{\\mu+\\sigma^2/2}\\) (1.6487213), not \\(e^{\\mu}\\).\nNote that \\(e^{r_y}\\), the blue values above, are 1) never less than zero and 2) asymmetrically distributed. This means mathematically is that the expected value, or mean, of \\(e^{r_y}\\) (blue dashed line above) is in fact not the same as \\(exp(mean(r_y))\\). Confirm this for yourself below.\n\n## the mean of X should be close to 0\nmean(X) \n\n[1] -0.005742354\n\n## that value exponentiated should therefore be close to 1\nexp(mean(X))\n\n[1] 0.9942741\n\n## yet the mean of the blue distribution is larger than 1...\nmean(exp(X)) \n\n[1] 1.639648\n\n##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)\nexp(mu+sigma^2/2)\n\n[1] 1.648721\n\n\nThis statistical reality makes for a problem if we calculate our annual recruitment by multiplying \\(R_{det}\\) by \\(exp(r_y)\\). If we don’t account for it, we end up with inflated annual recruitments because we would be multiplying by values typically greater than 1.\n\n\nBias correction to the rescue\nThe simple idea here is that we are applying a correction to the deviations so that when a year is “average” we’ll be multiplying \\(R_{det,y}\\) (deterministic recruitment in year y) by \\(exp(0) = 1\\), and get back the mean or expected value (the black curve in the first figure). Because we’ve discovered that \\(exp(r_y)\\) is not symmetrical, we need to perform a bias correction to confirm that we approximately return the \\(R_{det,y}\\) when \\(r_y\\) is zero.\nCheck it out:\n\n\n\n\n\n\n\n\n\n\n## as above, the mean is close to 1 without adjustment\nexp(mean(X)) \n\n[1] 0.9942741\n\n## ...though the actual mean is greater, by a factor of sigma^2/2\nmean(exp(X)) \n\n[1] 1.639648\n\nmean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!\n\n[1] 0.9944967\n\n\nOne last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above). The horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year regardless of the stock size at hand. You can also see that the blue point we saw in the earlier figures was not actually as great of an outlier as the original plot suggested\n\n\n\n\n\n\n\n\n\n\n\nBonus: why this isn’t perfect\nThis came up on my General Exam for my PhD. I was asked to walk through the basic logic of the recruitment deviation and why we apply bias correction. Then I was asked, “why is this wrong?”.\nPart of the answer lies in the fact that this is a conscious decision made by fisheries population dynamicists to make parts of our models behave in the way we want. Specifically, after applying our bias adjustment factor, the resulting recruitment timeseries will have a median value that is less than the deterministic curve shown above. In the Methot & Taylor paper linked up top, they mention that this is sensible since the average recruitment is most representative of a recruitment event’s long-term contribution to the population, “because most of the population biomass comes from the numerous recruits in the upper tail of the lognormal distribution”. If you’re working with a different data type or hope to gain information from the median behavior of this process, such an adjustment is not for you."
  },
  {
    "objectID": "posts/CopyOf2025-03-10-simulating-bivariate/20250514-howgotjob.html",
    "href": "posts/CopyOf2025-03-10-simulating-bivariate/20250514-howgotjob.html",
    "title": "How I got a New Job (under duress)",
    "section": "",
    "text": "The Context\nI found myself emailing individual versions of this, so decided to make a post. This is not so much about the story of my leaving NOAA but my experience looking for and securing a new job during an uncertain time (and frankly bad job market).\n\n\nSome hard stats\nDon’t let this post make you think I had an easy time. I started job searching “full time” (meaning, in all of my free time) in late January 2025 and will receive my first paycheck in a new role at the end of June. I applied to 85 roles, Received 6 first-round interviews, 4 second-or-more-round interviews, and 3 offers. That’s less than a 4% success rate\n\n\nSoul Searching\nI didn’t want to admit this part was\n\n\nThe Three Pivots\nThere are three basic components to any job: your sector, your domain, and your skillset. Sector is what type of employer you have: federal, private, NGO, academic, freelance, etc. The domain is the topic, or field, in which you operate (I think of this in general terms, like the undergraduate major that would roughly correspond to your field – marine science? chemical engineering?). The skillset is what you’re doing every day in that role. Are you generating insights from data? Communicating with stakeholders?\nWhen searching for a new job, it’s easier to do a single pivot (change one of these things) than to change all three. It’s also easier to do a nearby pivot (Fed to NGO, since both interact with policy and grants).\nSo, if I am a federal (sector) fisheries scientist (domain) conducting statistical analyses (skillset), it’s going to be a tall ask to move to the private sector in fintech as a product manager – it’s not impossible, just a much harder sell.\nNow, if you’re in the boat I was (leaving the Fed), the first component is going to have to change no matter what. So I decided to stick with my skillset (broadly, ‘data science’) and target roles that pivoted from fisheries/marine science to ‘earth sciences’ more broadly. This included energy, sustainability, agroecology, atmospheric science, carbon, climate risk – basically any role where I hoped that the hiring manager(s) would recognize the value of my environmental science degree and understanding of simulation modeling. This increased the\n\n\nTread lightly\nThere’s ubiquitous advice to network like crazy. I don’t think it’s realistic to expect to build meaningful new connections at the same time as engaging in a heavy job search, and anticipate those connections to directly lead to a new hire. I had a dozen ‘new connnection’ conversations with folks at or adjacent to open roles I was excited about. Several of these convos went amazingly well, with one person even saying “I hope you become my colleague!” at the end…and then crickets. A new connection is not going to be able to pass your resume along with any sort of endorsement. At most, they’ll get you a first-round interview. The point is, informal networking “conversations” are useful, but jobs are obtained by applying and interviewing.\nSo, approach the networking task as part of the soul-searching task: to learn more about what’s out there, to get a feel for the personalities in certain roles…but don’t expect a brand-new contact (unless it’s a recruiter who specifically wants to talk to you about a role) to be your bridge into a position."
  }
]