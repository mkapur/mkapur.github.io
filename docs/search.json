[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maia Sosa Kapur",
    "section": "",
    "text": "I am a federal Research Mathematical Statistician at the Alaska Fisheries Science Center (NOAA, Seattle). I received my PhD from the the School of Aquatic and Fisheries Science at the University of Washington in the Punt Lab. My research was supported by a NMFS-Sea Grant Population Dynamics Fellowship awarded in 2019.\nPreviously I worked as an Assessment Specialist at the Joint Institute for Marine and Atmospheric Research at PIFSC (NOAA) in Honolulu and completed my MSc at the Hawaii Institute of Marine Biology. I have also worked in the Florida Keys and studied at the Smithsonian Tropical Research Institute in Bocas Del Toro, Panama while completing my BSc in Environmental Science at UC Berkeley.\nMy current research interests include: application and development of AI tools for scientific management; how evolution and climate change conspire to bias data and assessment outputs; and how spatial statistics can be used to tackle uncertainties in our scientific management."
  },
  {
    "objectID": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "href": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "title": "Why do we use ln(19)?",
    "section": "",
    "text": "Have you ever noticed that sneaky \\(log(19)\\) in your logistic equation for maturity or selectivity? What is up with that?\nLet’s take the example of a basic selectivity-at-age function, which uses the parameters \\(a_{50}\\) and \\(a_{95}\\). These represent the ages where a fish has a probability 50% or 95% chance of being captured. Read that again carefully. I took these parameters for granted, but the meaning of them is intertwined with the use of \\(ln(19)\\), and if they change, so does the logged value.\nHere’s a selectivity plot with some made up numbers. The selectivity function I’m using is:\n\\[\nS_a = \\langle 1+exp(-log(19)*\\frac{a-a_{50}}{a_{95}-a_{50}}) \\rangle ^{-1}\n\\]\n\n\n\n\n\nNote how nicely these values extend between zero and one (on the y axis)? That’s because we’re really interested in working with the odds of being captured. Based on our parameter definitions, we need to ensure that the logistic function equals 0.95 when \\(a = a_{95}\\) (and similarly equals 0.5 at \\(a_{50}\\)). To achieve this, we recognize the exponent of the log odds equals the odds ratio. You can think of the odds ratio as the probability of your event (getting captured) happening over the probability of it not happening. We can leverage this to force the logistic equation to scale the way we want.\nSince we’ve already defined \\(a_{95}\\) as the age at which we have a 95% probability of getting captured, the log-odds can be defined as \\(log(\\frac{0.95}{0.05})\\). This could be rewritten as \\(log(\\frac{19/20}{1/20})\\) and simplifies to \\(log(19)\\). This ensures that when \\(a = a_{95}\\), the equation up top indeed returns 0.95, which is the logistic function evaluated at an odds ratio of 95%.\nGiven this syntax you’ll see how we would never really need to correct for \\(a_{50}\\), since the odds ratio of two events with 50% probability is actually zero (plug in 0.5 to see for yourself).\nTwo notes:\n\nSometimes to ease model fitting, we will simplify the above equation by substituting \\(\\delta\\) in place of the denominator (\\(a_{95} - a_{50}\\)), or estimate \\(\\delta\\) as \\(\\frac{ln(19)}{a_{95} - a_{50}}\\). (Specifically, if you have the \\(a_{50}\\) and \\(\\delta\\) values from, say, a Stock Synthesis model, you can solve for \\(a_{95}\\) via \\(a_{95} = \\frac{-ln(19)}{\\delta}+a_{50}\\)).\nIf you’d want to use the something like \\(a_{75}\\), you’d need to update the log odds via \\(log(\\frac{0.75}{0.25})\\) or ~ \\(log(3)\\). Note that this ensures that you are getting a 75% selectivity at the corresponding age, but provides a distinct curve from before."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a federal Research Mathematical Statistician at the Alaska Fisheries Science Center (NOAA, Seattle). I received my PhD from the the School of Aquatic and Fisheries Science at the University of Washington in the Punt Lab. My research was supported by a NMFS-Sea Grant Population Dynamics Fellowship awarded in 2019.\nPreviously I worked as an Assessment Specialist at the Joint Institute for Marine and Atmospheric Research at PIFSC (NOAA) in Honolulu and completed my MSc at the Hawaii Institute of Marine Biology. I have also worked in the Florida Keys and studied at the Smithsonian Tropical Research Institute in Bocas Del Toro, Panama while completing my BSc in Environmental Science at UC Berkeley."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "You can find an up-to-date list of my peer-reviewed publications on Google Scholar.\nBecause much of my work directly informs the management of industrial fisheries in the United States and international waters, a large body of my research is contained in grey literature: technical memoranda and formal stock assessment reports submitted to regional fishery management organizations. A list of these publications, and links (where available), is provided below.\nStock Assessments Used for Fisheries Management\n\nKapur, M. and Monnohan, C. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/BSAIflathead.pdf\nKapur, M. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/BSAIflathead.pdf\nKapur, M., Lee, Q., Haltuch, M., Gertserva, V., and Hamel, O. Status update of the U.S. sablefish stock in 2021. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. https://www.pcouncil.org/documents/2021/06/g-5-attachment-5-draft-status-of-sablefish-anoplopoma-fimbria-along-the-us-west-coast-in-2021-electronic-only.pdf/ 108 p.\nHaltuch, M., Johnson, K., Tolimieri, N, Kapur, M., and Castillo-Jordán, C. Status of the sablefish stock in U.S. waters in 2019. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. 398 p. https://www.pcouncil.org/wp-content/uploads/2019/10/sablefish_20191022.pdf\nKapur, M., Hamel, O. Catch-only update for China Rockfish in 2019.\nAdams, G. D., Kapur, M., McQuaw, K., Thurner, S., Hamel, O.S., Stephens, A. & Wetzel, C. R. 2019. Stock Assessment Update: Status of Widow Rockfish (Sebastes entomelas) Along the U.S. West Coast in 2019. Pacific Fishery Management Council, Portland, Oregon. https://www.pcouncil.org/wp-content/uploads/2019/10/2019WidowUpdate_final_clean.pdf\nHamel, O. Kapur, M., Catch-only update for Blackgill Rockfish in 2019.\nKapur, M., Fitchett, M., Yau, A., Carvalho F. 2019. 2018 Benchmark Stock Assessment of Main Hawaiian Islands Kona Crab. NOAA Tech Memo. NMFS-PIFSC-77, 114 p. https://doi.org/10.25923/7wf2-f040 Available at: https://www.fisheries.noaa.gov/resource/document/2018-benchmark-stock-assessment-main-hawaiian-islands-kona-crab\nISC, 2019 (contributor). 2019 Benchmark stock assessment and future projections for the Western and Central North Pacific striped marlin. Honolulu, Hawaii, USA. Available at:http://isc.fra.go.jp/pdf/ISC19/ISC19_ANNEX11_Stock_Assessment_Report_for_Striped_Marlin.pdf\nISC, 2018 (contributor). Stock Assessment of Shortfin Mako Shark in the North Pacific Ocean Through 2016 (No. ISC/18/ANNEX/15). Yeosu, Republic of Korea. Available at: http://isc.fra.go.jp/pdf/ISC18/ISC_18_ANNEX_15_Shortfin_Mako_Shark_Stock_Assessment_FINAL.pdf\nLangseth, B., Syslo, J., Yau, A., Kapur, M., Brodziak, J., 2018. Stock assessment for the main Hawaiian Islands Deep 7 bottomfish complex in 2018, with catch projections through 2022. NOAA Tech. Memo. NMFS-PIFSC 69, 217. Available at: https://repository.library.noaa.gov/view/noaa/17252\nWinker, H., Carvalho, F., Kapur, M., Parker, D., Kerwath, S., 2017. JABBA goes IOTC: Just Another Bayesian Biomass Assessment for Indian Ocean Blue shark and swordfish (No. IOTC-2017-WPB15-INF02). Available at: https://www.iotc.org/sites/default/files/documents/2017/10/IOTC-2017-WPM08-11_Rev_1.pdf\n\nTechnical Documents Submitted to Fisheries Management Organizations\n\nHamel, O., Punt, A.E., Kapur, M. and Maunder, M. 2023. Natural Mortality: Theory, Estimation, and Application in Fishery Stock Assessment Models. U.S. Department of Commerce, NOAA Processed Report NMFSNWFSC-PR-2023-02. https://doi.org/10.25923/xqv2-2g73\nKapur, M., Connors, B., DeVore, J., Fenske, K., Haltuch, M., Key, M. 2021. Transboundary Sablefish Management Strategy Evaluation (MSE) Workshop Report. Pacific Fishery Management Council, Supplemental Informational Report 5. https://www.pcouncil.org/documents/2021/06/supplemental-informational-report-5-council-sponsors-successful-sablefish-management-strategy-evaluation-workshop.pdf/\nCarvalho, F., Kapur, M., Ijima, H. 2019. Comparison between Preliminary Stock Synthesis Models for the 2019 WCNPO Striped Marlin Stock Assessment. http://isc.fra.go.jp/pdf/BILL/ISC19_BILL_2/ISC19_BILLWG2_WP4.pdf\nFenske, K. H., Berger, A. M., Connors, B., Cope, J.M., Cox, S. P., Haltuch, M. A., Hanselman, D. H., Kapur, M., Lacko, L., Lunsford , C., Rodgveller, C., and Williams, B. 2019. Report on the 2018 International Sablefish Workshop. U.S. Dep. Commer., NOAA Tech. Memo. NMFS-AFSC-387, 107 p. Available at: https://www.afsc.noaa.gov/Publications/AFSC-TM/NOAA-TM-AFSC-387.pdf\nSculley, M., Kapur, M., Yau, A., 2018. Size composition for swordfish Xiphias gladius in the Hawaii-based pelagic longline fishery for 1995-2016 (No. WP-18-003). https://doi.org/10.7289/V5/WP-PIFSC-18-003\nSculley, M., Yau, A., Kapur, M., 2018. Standardization of the swordfish Xiphias gladius catch per unit effort data caught by the Hawaii-based longline fishery from 1994-2016 using generalized linear models. (No. WP-18-001). https://doi.org/10.7289/V5/WP-PIFSC-18-001\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. Performance evaluation of JABBA-Select against an age-structured simulator and estimation model (No. MARAM/IWS/2017/Linefish/P3 Performance). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P3.pdf\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. JABBA-Select: an alternative surplus production model to account for changes in selectivity and relative mortality from multiple fisheries (No. MARAM/IWS/2017/Linefish/P2). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P2.pdf\nKapur, M., Yau, A., 2018. Size Compositions and Sex Ratios of Oceanic Whitetip Sharks and Giant Manta Rays for Longline Fisheries in the Pacific Islands Region (DR-18-012). Honolulu, HI. https://doi.org/10.25923/67hf-bn93\nKapur, M., Brodziak, J.A., Fletcher, E.J., Yau, A.J., 2017. Summary of Life History and Stock Assessment Results for Pacific Blue Marlin, Western and Central North Pacific Striped Marlin, and North Pacific Swordfish. (No. WP-17-004). 10.7289/V5/WP-PIFSC-17-004 Available at: http://www.pifsc.noaa.gov/library/pubs/WP-17-004.pdf\nSculley, M.S., Brodziak, J.A., Yau, A.J., Kapur, M., 2017. An Exploratory Analysis of Trends in Swordfish (Xiphias gladius) Length Composition Data from the Hawaiian Longline Fishery (No. WP-17-002). 10.7289/V5/WP-PIFSC-17-002. Available at: https://repository.library.noaa.gov/view/noaa/14862/noaa_14862_DS1.pdf"
  },
  {
    "objectID": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "href": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "title": "How I studied for my qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a https://www.coursera.org/learn/bayesian-statistics (free, online) course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2021-01-25_recDevs/recdevs.html",
    "href": "posts/2021-01-25_recDevs/recdevs.html",
    "title": "How Do Recruitment Deviations Work?",
    "section": "",
    "text": "For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out: Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. https://doi.org/10.1139/f2011-092.\nHere I want to jot down the practical implementation behind the recruitment deviation. The topics covered here are 1) The meaning of a recruitment deviation (abbreviated here to “rec-dev”), 2) the common mathematical syntax for these values, and 3) the practical meaning of lognormal bias correction. This post will not address advanced implementation topics, such as likelihood penalties and bias correction ramps.\n\nWhat is a rec-dev?\nA recruitment deviation is how much a given year’s recruitment deviates from what you’d expect from average. This could be a static value (e.g., unfished recruitment), or the deterministic value given by a stock-recruit relationship (in this post I’ll use the Beverton-Holt [1957] as an example). The idea is that stock productivity is likely not the same every single year, and we need a flexible method to capture that variation in our model fitting procedures. Reasons for these deviations could be environmental, and are generally unexplained – which is why the time series of rec-devs is often interpreted as process error in your system (extra noise in the system not captured by other processes).\nLet’s quickly refresh what we’re talking about when we say the mean or expected recruitment level. Again, using the Bev-Holt as an example, this equation describes the expected number of recruits given a stock spawning biomass (SSB). (This syntax is what’s commonly used in Stock Synthesis models: check out my post on recruitment algebra for the derivation).\n\\[\nR_{expected} = \\frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)}\n\\]\n\\(h\\) is steepness, or the expected proportion of \\(R_0\\) anticipated at \\(0.2SSB_0\\); \\(R_0\\) and \\(SSB_0\\) are unfished recruitment and unfished spawning biomass, respectively. Given values for these parameters, the “mean” or “expected recruitment” level is what you’d get by plugging in those numbers to the values and reading off the curve:\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, the recruitment in a given year may deviate from the black curve. We can visualize these in two ways. On the bottom left, I’ve invented a series of new recruitment values shown as gold points; the dotted line separating each point from the expected value is the raw value of the deviation itself (in numbers). A time series of these values is shown on the right. The blue point on both plots is the highest year – and you can see that since this is plotted in natural (not log) space it’s fairly hard to distinguish the smaller points from one another.\n\n\n\n\n\nLet’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviation at high biomass is going to be much larger than the deviation at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year \\(y\\) is diverging from expectation. For that reason, we are interested in modeling the errors, or deviations, as lognormally distributed. Equation-wise, here’s what that looks like. Note that \\(R_{det}\\) is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.\n\\[\nR_{expected,y} = R_{det,y}e^{(dev_y-\\sigma^2/2)}\n\\]\nNow we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that \\(-\\sigma^2/2\\).\n\n\nRefreshing the lognormal distribution.\nCheck out towardsdatascience.com for a good background read on this topic, which is not specific to recruitment deviations but rather a general property of the lognormal distribution.\nIf our deviations vector \\(\\vec r\\) is normally distributed with mean zero, \\(exp(\\vec r)\\) is lognormally distributed, shown in blue below:\n\n\n\n\n\nIf \\(r_y \\sim N(\\mu=0,\\sigma=1)\\) and \\(Y = e^{r_y}\\), the expected value of \\(r_y\\) is \\(\\mu\\) (zero), and the expected value of \\(Y\\) will be \\(e^{\\mu+\\sigma^2/2}\\) (1.6487213), not \\(e^{\\mu}\\).\nNote that \\(e^{r_y}\\), the blue values above, are 1) never less than zero and 2) asymmetrically distributed. This means mathematically is that the expected value, or mean, of \\(e^{r_y}\\) (blue dashed line above) is in fact not the same as \\(exp(mean(r_y))\\). Confirm this for yourself below.\n\n## the mean of X should be close to 0\nmean(X) \n\n[1] 0.001607768\n\n## that value exponentiated should therefore be close to 1\nexp(mean(X))\n\n[1] 1.001609\n\n## yet the mean of the blue distribution is larger than 1...\nmean(exp(X)) \n\n[1] 1.650953\n\n##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)\nexp(mu+sigma^2/2)\n\n[1] 1.648721\n\n\nThis statistical reality makes for a problem if we calculate our annual recruitment by multiplying \\(R_{det}\\) by \\(exp(r_y)\\). If we don’t account for it, we end up with inflated annual recruitments because we would be multiplying by values typically greater than 1.\n\n\nBias correction to the rescue\nThe simple idea here is that we are applying a correction to the deviations so that when a year is “average” we’ll be multiplying \\(R_{det,y}\\) (deterministic recruitment in year y) by \\(exp(0) = 1\\), and get back the mean or expected value (the black curve in the first figure). Because we’ve discovered that \\(exp(r_y)\\) is not symmetrical, we need to perform a bias correction to confirm that we approximately return the \\(R_{det,y}\\) when \\(r_y\\) is zero.\nCheck it out:\n\n\n\n\n\n\n## as above, the mean is close to 1 without adjustment\nexp(mean(X)) \n\n[1] 1.001609\n\n## ...though the actual mean is greater, by a factor of sigma^2/2\nmean(exp(X)) \n\n[1] 1.650953\n\nmean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!\n\n[1] 1.001354\n\n\nOne last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above). The horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year regardless of the stock size at hand. You can also see that the blue point we saw in the earlier figures was not actually as great of an outlier as the original plot suggested\n\n\n\n\n\n\n\nBonus: why this isn’t perfect\nThis came up on my General Exam for my PhD. I was asked to walk through the basic logic of the recruitment deviation and why we apply bias correction. Then I was asked, “why is this wrong?”.\nPart of the answer lies in the fact that this is a conscious decision made by fisheries population dynamicists to make parts of our models behave in the way we want. Specifically, after applying our bias adjustment factor, the resulting recruitment timeseries will have a median value that is less than the deterministic curve shown above. In the Methot & Taylor paper linked up top, they mention that this is sensible since the average recruitment is most representative of a recruitment event’s long-term contribution to the population, “because most of the population biomass comes from the numerous recruits in the upper tail of the lognormal distribution”. If you’re working with a different data type or hope to gain information from the median behavior of this process, such an adjustment is not for you."
  },
  {
    "objectID": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "href": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "title": "Length-based Processes in an Age-Structured Model",
    "section": "",
    "text": "This is in the category of “stuff I thought I understood” until I didn’t.\nLet’s say you have an age-based model with integer-year bins a thru A+. The model is going to keep track of the dynamics of the fish population as they step through each age, but (annoyingly, perhaps) some of your modeled processes are length-based. This can occur when you are working with lots of different fleets, for which gear selectivity is better specified by length, or even the simple inclusion of our favorite \\(w_{l} = a l^b\\) calculation, which will require a notion of a fish length-at-age to play nicely within an age-structured model.\nConverting these length-based estimates (or quantities) into age-based estimates introduces a problem because of uncertainty in length-at-age, in other words, the fact that fish of length \\(l\\) could be of various ages \\(a\\). Let’s make up some data:\n\n## check out my post \"Why do we use ln(19)?\" for more info on this syntax:\nlogistic <- function(bin, b50, b75){\n  selage <- 1/(1+exp(-(log(15))*(bin-b50)/(b75-b50)))\n  return(selage)\n} \n\nage_bins <- 0:50\nlength_bins <- 1:65\nsigma_length <- 7.16\n\n## a logistic, length-based selectivity curve\nlength_selex <- sapply(length_bins, logistic, b50 = 30, b75 = 52)\n\nHere’s our selectivity curve; there’s a vertical line at l = 28cm which we’ll use for reference throughout this post.\n\n\n\n\n\nSo let’s say we’re interested in the selectivity at l = 28cm. In a length-based population dynamics model, we’d be set to just return the selectivity at l = 28cm for any fish within that respective bin (in 1-cm bins, fish lengths greater than 27cm and less than or equal to 28cm would all be assigned this value.)\nHowever, in an age-based model, we are instead interested in translating this curve into the expected selectivity-at-age. And given age \\(a\\), a fish could be a broad range of lengths \\(l\\), making it less clear which value we would “read off” the blue curve above. This variation is caused by the variable \\(\\sigma\\), which I’ve set at ~7cm – this reflects the standard deviation in length-at-age for our stock.\nHere’s how it looks in practice. First, we can generate our vonB-based expected length-at-age (here I assume \\(t_0\\) is 0, and am plotting the observed variation for 500 datasets, which should approximate the value for sigma_length specified above).\n\nvonb <- function(age, linf, kappa, sigma_length){\n  len <- linf*(1-exp(-kappa*(age)))+rnorm(1,0,sigma_length)\n}\n\n## simulate some datasets\nlength_at_age = matrix(NA, nrow = length(age_bins), ncol = 500)\nfor(i in 1:500){\n  length_at_age[,i] <- sapply(age_bins, vonb, linf = 44,\n                              kappa = 0.15, sigma_length)\n}\nlength_at_age <- bind_cols(age = age_bins, \n                           meanL = rowMeans(data.frame(length_at_age)),\n                           sdL = apply(data.frame(length_at_age),1,sd))%>%\n  mutate(lci = meanL-1.96*sdL, uci = meanL+1.96*sdL)\n\nggplot(length_at_age,\n       aes(x = age_bins)) +\n  geom_line(lwd = 1.1, col = 'grey', aes(y = meanL)) +\n  geom_ribbon(aes(ymin = lci, ymax = uci), fill = alpha('grey',0.3))+\n  geom_hline(yintercept = 28, linetype = 'dotted')+\n  labs(x = 'age', y= 'length')\n\n\n\n\nNotice how the horizontal line at 28cm corresponds to multiple ages within the confidence interval? How do we account for the fact that there is a chance that the the selectivity applicable to a given age can be characterized by the selectivity at several lengths, and how do we weight the applicability of that selectivity value given what we know about growth? Enter \\(\\psi_{l,a}\\), the probability of being in length bin \\(l\\) at age \\(a\\), a.k.a. the normal distribution \\(\\phi\\) of size-at-age.\n\\[\n\\begin{align}\n\\psi_{al} &= \\begin{array}\n   & \\Phi (1,\\tilde l_{a}, \\sigma)) & if \\space l = 0cm \\\\\n   \\Phi (l+1,\\tilde l_{a}, \\sigma)) - \\Phi (l,\\tilde l, \\sigma))& if \\space 0cm, < l < L+ \\\\\n   1-\\Phi (l,\\tilde l_{a}, \\sigma)) & if \\space l = L+\\\\\n\\end{array}\n\\end{align}\n\\]\nWhere \\(l\\) is the length bin in question, \\(\\tilde l\\) is the observed length-at-age, and \\(\\sigma\\) is the variation in growth. If we simply used a “lookup” method (which I have certainly done in a pinch!), and assigned the selectivity at age \\(a\\) to the selectivity at the expected length at age \\(a\\), we’d be introducing bias.\n\n## this function returns the distribution of expected length-at-age \n## given a bin and observed length-at-age\ngetP_al <- function(len_bin, len_obs, length_sigma){\n  pal = NA\n  if(len_bin == 1){\n    pal <- pnorm(len_bin, len_obs, length_sigma)\n  } else if (len_bin > min(length_bins) & len_bin < max(length_bins)){\n        pal <- pnorm(len_bin+1, len_obs, length_sigma) - \n          pnorm(len_bin, len_obs, length_sigma)\n  } else{\n        pal <- 1-pnorm(len_bin, len_obs, length_sigma)\n  }\n  pal\n}\nPAL <- matrix(NA, nrow = length(age_bins), ncol =  length(length_bins))\nfor(a in age_bins){\n  for(l in length_bins){\n    ## instead of using means, could re-do vonB with error turned off\n    PAL[a,l] <- getP_al(len_bin=l,len_obs=length_at_age$meanL[a],sigma_length )\n  }\n}\nlongPal <- data.frame(PAL) %>%\n  mutate(age_bins) %>%\n  reshape2::melt(id = 'age_bins') %>% \n  mutate(len =as.numeric(gsub(\"X\",\"\",variable))) \n\nHere’s a subset of what \\(\\psi_{al}\\) looks like for ages 5-10. Notice (again) how length = 28cm crosses multiple age curves. The good news is, \\(\\psi_{al}\\) has given us a sense of the relative probability of being age \\(a\\) given length \\(l\\). (It looks like \\(l = 28cm\\) is most likely a fish of age 7 or 8).\n\n\n\n\n\nTo put this all together, we need to compute the dot-product of the selectivity-at-length and the expected distribution of length-at-age.\nIn math, we’re doing this: \\[\nSel_a = \\sum_l Sel_l \\cdot \\psi_{al}\n\\]\nWhich is really just a cheap way of approximating the correct estimator of the expected selectivity of an animal at age \\(a\\):\n\\[\nSel_a = \\int_L Sel_L  \\psi_{aL} dL\n\\]\nIn words: for each length bin \\(l\\), multiply the selectivity at that length times the probability of being at age \\(a\\) given length \\(l\\). Sum these quantities across all length bins for each age bin.\nIn pictures: let’s say we are interested in \\(a = 5\\). We are going to multiply each point on the length-selectivity by its corresponding point (by color and ID number) on the probability of length-at-age-5 plot, then sum the results.\n\n\n\n\n\n\n\n\n\n\nIn code (still using \\(a = 5\\) as an example):\n\nsel_age <- merge(longPal,data.frame(cbind(length_selex,length_bins)),\n                 by.x = 'len', by.y = 'length_bins') %>%\n  ## inner product for each age-length combo\n  mutate(prod1 = length_selex*value) %>% \n  group_by(age_bins, len) %>%\n  summarise(dotProd = sum(prod1)) %>%\n  ungroup() %>%\n  group_by(age_bins) %>%\n  summarise(dotProdsum = sum(dotProd))\n\nggplot(sel_age,\n       aes(x = age_bins, y = dotProdsum)) +\n  geom_line(lwd = 1.1, col = 'grey22') +\n  labs(x = 'age', y= 'proportion selected', \n       main = 'Converted from length-at-age')"
  },
  {
    "objectID": "posts/2020-01-01-quals/quals.html",
    "href": "posts/2020-01-01-quals/quals.html",
    "title": "How I studied for my PhD qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a free, online Coursera course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "href": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "title": "Leveraging try_again() for error catching in MSEs",
    "section": "",
    "text": "This post is not a full breakdown of how to structure code for Management Strategy Evaluation (MSE) work. I’m certainly not a pro software engineer. Instead, I was pleased to find a clean way to deal with errors in the hierarchical nature of my MSE workflow that may be of use to others, or to future-me.\nHere’s a schematic of what’s happening in my MSE:\n\n\n\n\n\nAs anyone who’s done this type of work before knows, we definitely expect a subset of our simulation runs to ‘fail’ - that is, for the unique simulated data set (which differs here based upon what I’m calling \\(r\\))in year \\(y\\), the estimation procedure doesn’t pass whatever criteria we’ve set (could be convergence, a NaN gradient, etc).\nA simple break won’t help us here, since that would kill the loop mid-run and require us to manually re-start the higher-level function. A tryCatch also seems appealing, but this would still require recursion (since we’d have to embded a call to run_MSE() in the Catch wrapper, within run_MSE() itself…not ideal).\nOur desired behavior would have us hit play once on the run_MSE() function, and trust that any estimation model that fails will be safely handled:\n\n\n\n\n\nI was able to accomplish this elegantly using testthat::try_again(). I simply indicated the number of times I wanted to attempt my run_MSE() function, and included a stop() argument inside check_model() (so that a true error is thrown when the estimator fails).\nWhen testthat::try_again() detects this failure, it restarts the run_MSE() function, which increments along a vector of random seeds, until it hits something functional. This can be adapted to lots of purposes, and one could readily keep track of the identity/quantity of estimators that failed.\nEven better, I can pop this into a one-line call to replicate, which functionally requests a total number of sucesses equal to nReplicates. A key for this to work is to ensure whatever will be incremented along for each call to run_MSE() (in my case, the vector of seeds) is long enough to account for every attempted replicate times the potential number of tries. In the example below, this would be 99 tries x 100 nReplicates = 9900 seeds.\nsim <- replicate(nReplicates, {try_again(99,run_MSE(nprojyrs, spaceID))})"
  },
  {
    "objectID": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "href": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "title": "How I Keep Up With the Literature",
    "section": "",
    "text": "My lab recently had a meeting where we discussed the nuts & bolts of performing a peer-review for a journal article, which got everyone talking about their preferred methods of reading a scientific paper. Luckily, we aren’t in bio-medicine or particle physics, but the volume of articles that emerge monthly in our field is still large, and it takes strategy to efficiently extract and scrutinize the information from a scientific article.\nOne of my favorite specimens of PhD advice comes from the http://www.chemistry-blog.com/wp-content/uploads/2010/06/Gassman-and-Meyers.pdf”, which posess the perfect melange of condecision, tough love and “kids these days” angst – the advice, nonetheless, is great. It sticks because it stings a bit. Gassman writes:\n\nIt is a consistent observation on my part that people have more ideas about their research when they are writing their thesis than at any other time. This is generally due to the fact that they are finally doing the literature work they should have done early in their thesis work only at the time that they are trying to complete their dissertations. – P.G. Gassman\n\nSo, don’t let that be you. Here is how I approach the literature, which has proved helpful for the last few years.\n\n\nHave a method to centralize new articles. I used the tips on the https://fraserlab.com/2013/09/28/The-Fraser-Lab-method-of-following-the-scientific-literature/ website to set up a Feedly account (there are lots of other options) and subscribed to the four main journals in my field, plus Science, Nature, and PNAS. The algorithm learns your preferences quite quickly, and your queue will show articles that fit your niche after you ‘check’ them off.\n\n\nCheck your centralizer every morning, and save the articles whose abstracts seem relevant as soon as you see them. It’s fine to save articles on-the-fly, but I recommend scheduling ‘deep reading’ periods of no less than one hour.\n\n\nGo beyond highlighting/sticky notes. I use a set of spreadsheets, stored in Google Drive and sorted by topic, to ensure that I’m actually engaged with what I’m reading. They all follow the same format. One example comes from my spreadsheet for studies of spatial model structure and general model mis-specification. The headings are hard to read in the screenshot, but they are Citation, Species, Mis-specification, Methods, Performance Measures, Conclusions, Similarities and Differences. These can be tailored to your field or project, but the last two are the most crucial. In those columns, I compare the study to my thesis work, making it easy as I write my introductions/discussions to state who has done similar studies, and where the research gaps lie."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "title": "Ageing Error Matrices - some notes",
    "section": "",
    "text": "I revisit the inclusion of ageing error matrices frequently enough that I wanted to consolidate some notes here. None of this is new information; it is a resource for myself to get things straight, particularly when moving between the Stock Synthesis ageing error syntax and other setups like those used in our bespoke ADMB/TMB models. For this post, \\(a\\) will index true ages, \\(\\tilde a\\) indexes read ages.\nIn this post, I will talk through the math behind the ageing error matrix, and show how one can develop such a matrix (with R code) given parameters concerning the bias and imprecision of their age reads."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "title": "Ageing Error Matrices - some notes",
    "section": "Populating the Matrix",
    "text": "Populating the Matrix\nContinuing our example, let’s pass the bias and imprecision vectors we established above to see how \\(\\mathbf P\\) responds. I’m filling each element of \\(\\mathbf P\\) one at a time. Sometimes you will need to make an assumption about the first age group to avoid a zero.\n\n\n\nYou’ll want to sanity check that the columns sum to one; in other words, are all possible true ages \\(a\\) accounted for under each read age \\(\\tilde a\\)? Normalization, and occasionally some rounding to several decimals, are reasonable steps here.\n\nfor(a_obs in 1:abins){\n  P_aa[,a_obs] <- P_aa[,a_obs]/sum(P_aa[,a_obs])\n}\nall(round(colSums(P_aa))==1)\n\n[1] TRUE\n\n\nNow let’s confirm that the shape of the probabilities looks reasonable. Generally, I expect to see what I call a “slumpy worm”: values with higher, narrow peaks at early ages, slumping to flatter, shallower peaks at older ages. The slumpy worm corresponds to the notion that it’s more likely for a reader to get a precise read on a young fish (it’s easy to count a small handful of annuli), whereas it’s harder to age an older fish (many annuli tend to get blurry and bunched up, but **check with your life history program!*).\nThere should be a spike at the ends for the plus group and age-0, if applicable.\n\n\n\n\n\nAnd here’s what the actual populated matrix looks like:"
  },
  {
    "objectID": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html",
    "href": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html",
    "title": "Unusual Grad School Advice",
    "section": "",
    "text": "I’ve been ambivalent about writing graduate school “advice”, given that 1) I’m barely out of it and 2) tons of others have compiled their own listicles, blogs, articles, and books with more numerous and practical tips than I could ever come up with.\nInstead, I wanted to share a few thoughts about the American graduate school experience, and focus on “unusual” advice – things I haven’t heard much in the discourse.\nHow to approach this list, and perhaps a piece of meta-advice in itself, is that the point is not to make you the world’s best, or even a better, graduate student. The ideas below have an undercurrent of revisiting that which enriches and preserves all aspects of you, aspects that existed before and will continue to exist after your time in school.\nI reserve the right to change my mind.\n\nDon’t abandon the “Ph” part of the “PhD”. Regardless the topic you’re doing your dissertation on, don’t neglect Philosophy. It doesn’t have to be Socrates - but give yourself the gift of reading deep thinkers (used broadly), especially those far afield. Among many, I am blessed to have spent my grad school years in the intellectual company of activists, journalists, artificial intelligence ethicists, climate lawyers, poets, ultra-runners, fantasy novelists, and many others - through their writings. Why? I hesitate to say that engaging regularly with others’ ideas will make you a better writer and critical thinker (it will), but that is not the point. The time you spend uncovering knowledge about your PhD topic should not be at the expense of your deepening relationship to your own ways of knowing, your sense of justice, creativity, and purpose. Do not let these things lie frozen during your 20’s or 30’s; let them grow with you.\nYou are aging. This is not “advice”, exactly, rather a few things to consider on at least a quarterly basis. You have precious few years on this earth. If you allow it to, the stress of your degree will accelerate the aging process. What are you willing to sacrifice on the altar of this degree? Starting a family? Your partnership? Retirement savings? Are you certain that the rewards from completing your degree will merit these sacrifices? The only guarantee is when you graduate you will be a certain number of years older than you are now. What do you want to be doing regularly in service of that future self? What if you were to receive a terminal diagnosis this month, or in the month of your defense; do you still find this endeavor worth it, in that light? Do not view your graduate education as a dalliance, a detour, or a prolonged undergrad experience. These are (likely) the healthiest, freest, most impactful years (financially and otherwise) of your singular life. If that burden discomforts you, find out why.\nEconomize, don’t capitalize. Through my extra-curricular readings, I came upon the idea of internalized capitalism: where we conceive of ourselves as efficiency machines and work to claw back productivity from every possible cranny of our lives. This is obvious in the self-help literature, with endless titles imploring us to wake up earlier, automate our mundane tasks, and hustle ourselves into “successful” oblivion. My time with thinkers old & new taught me that this is a fallacy that benefits the economy but diseases and alienates us. My experience as a mother taught me how ridiculous it is to try to milk “more” from the time given to us in a day. So, resist the temptation to spend endless hours in the office, or to implement gimmicky apps/techniques to “maximize” your productivity. The best way to get things done is to remove distractions, and I recommend you achieve this in as blunt a way as possible. I have my distracting websites fully blocked on my work computer, and there was a time I would unplug my web router at home to ensure progress was made. You will be shocked how much of a frenzy you’re in on a normal workday.\nGet your adviser relationship straight. Endless ink has been spilled about the risks of studying under a toxic PI (this was fortunately not my case). Virtually everyone I know has experienced some degree of emotional distress from their advisor and/or committee relationship, that can linger sometimes years after defending. Why is this the case? Why is it so normal to have “bad bosses” in many other professions (to the point that comedic TV shows, books and social norms are built around this assumption), but folks in the sciences find the experience so damaging and all-consuming? It has to do with structural issues, mainly the fact that under the US system the PI has basically total control over your present and future life and job prospects. This is objectively true and even more crushing if you don’t have family wealth or support, or are on a visa. What you can control is how much emotional power you hand to your supervisor."
  },
  {
    "objectID": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html#populating-the-matrix",
    "href": "posts/2023-06-21-gradschool/2023-06-08-ageingError.html#populating-the-matrix",
    "title": "Ageing Error Matrices - some notes",
    "section": "Populating the Matrix",
    "text": "Populating the Matrix\nContinuing our example, let’s pass the bias and imprecision vectors we established above to see how \\(\\mathbf P\\) responds. I’m filling each element of \\(\\mathbf P\\) one at a time. Sometimes you will need to make an assumption about the first age group to avoid a zero.\n\n\n\nYou’ll want to sanity check that the columns sum to one; in other words, are all possible true ages \\(a\\) accounted for under each read age \\(\\tilde a\\)? Normalization, and occasionally some rounding to several decimals, are reasonable steps here.\n\nfor(a_obs in 1:abins){\n  P_aa[,a_obs] <- P_aa[,a_obs]/sum(P_aa[,a_obs])\n}\nall(round(colSums(P_aa))==1)\n\n[1] TRUE\n\n\nNow let’s confirm that the shape of the probabilities looks reasonable. Generally, I expect to see what I call a “slumpy worm”: values with higher, narrow peaks at early ages, slumping to flatter, shallower peaks at older ages. The slumpy worm corresponds to the notion that it’s more likely for a reader to get a precise read on a young fish (it’s easy to count a small handful of annuli), whereas it’s harder to age an older fish (many annuli tend to get blurry and bunched up, but **check with your life history program!*).\nThere should be a spike at the ends for the plus group and age-0, if applicable.\n\n\n\n\n\nAnd here’s what the actual populated matrix looks like:"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Ageing Error Matrices - some notes\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging try_again() for error catching in MSEs\n\n\n\n\n\n\n\n\n\nMSK\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLength-based Processes in an Age-Structured Model\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Keep Up With the Literature\n\n\n\n\n\n\n\n\n\nMSK\n\n\nFeb 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do Recruitment Deviations Work?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we use ln(19)?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD General Exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Algebra for Equilibrium Recruitment\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD qualifying exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html",
    "title": "Unusual Grad School Advice",
    "section": "",
    "text": "I’ve been ambivalent about writing graduate school “advice”, given that 1) I’m barely out of it and 2) tons of others have done so. What could I add?\nInstead, I wanted to share some “unusual” advice – things I haven’t heard much in the discourse. These are more relevant to the US doctoral experience.\nHow to approach this list, and perhaps a piece of meta-advice in itself, is that the point is not to make you the world’s best, or even a better, graduate student. The ideas below have an undercurrent of revisiting that which enriches and preserves all aspects of you, aspects that existed before and will continue to exist after your time in school."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#recall-the-ph-part-of-the-phd",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#recall-the-ph-part-of-the-phd",
    "title": "Unusual Grad School Advice",
    "section": "Recall the “Ph” part of the “PhD”",
    "text": "Recall the “Ph” part of the “PhD”\nRegardless the topic you’re doing your dissertation on, don’t neglect Philosophy. It doesn’t have to be Socrates - but expose yourself to the words and works of deep thinkers (used broadly), especially those far afield. I am blessed to have spent my grad school years in the intellectual company of activists, journalists, artificial intelligence ethicists, climate lawyers, poets, ultra-runners, fantasy novelists, and many others - through their writings, performances, and YouTube videos. Why do this? I hesitate to say that engaging regularly with others’ ideas will make you a better writer and critical thinker (it will), but that is not the point. The time you spend uncovering knowledge about your PhD topic should not be at the expense of your deepening relationship to your own ways of knowing, your sense of justice, creativity, and purpose. Do not let these parts of you lie frozen during your 20’s or 30’s; let them grow with you."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-are-aging",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-are-aging",
    "title": "Unusual Grad School Advice",
    "section": "You are aging",
    "text": "You are aging\nThis is not “advice”, exactly, rather a few things to consider on at least a quarterly basis. You have precious few years on this earth. If you allow it to, the stress of your degree will accelerate the aging process. What are you willing to sacrifice on the altar of this degree? Starting a family? Your partnership? Living somewhere you like? Retirement savings? Are you certain that the rewards from completing your degree will merit these sacrifices? When you graduate, you will be a certain number of years older than you are now. What do you want to be doing regularly in service of that future self? What if you were to receive a terminal diagnosis this month, or in the month of your defense; would you still find this endeavor worth it? Do not view your graduate education as a dalliance, a detour, or a prolonged undergrad experience. These are (likely) the healthiest, freest, most impactful years (financially and otherwise) of your only life. If that burden discomforts you, find out why."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#economize-dont-capitalize",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#economize-dont-capitalize",
    "title": "Unusual Grad School Advice",
    "section": "Economize, don’t capitalize",
    "text": "Economize, don’t capitalize\nThrough my extra-curricular readings, I came upon the idea of internalized capitalism: where we conceive of ourselves as efficiency machines and work to claw back productivity from every cranny of our day. Endless self-help titles implore us to wake up earlier, automate our mundane tasks, and hustle ourselves into “successful” oblivion. My time with thinkers old & new taught me that self-capitalization is a fallacy that benefits the economy but diseases and alienates us. My experience as a mother taught me how ridiculous it is to try to milk “more” from the time given to us in a day. So, resist the temptation to spend endless hours in the office, or to implement gimmicky apps/techniques to maximize your productivity. The best and only way to get things done is to remove distractions, and I recommend you achieve this in as blunt a way as possible. I have my distracting websites fully blocked on my work computer, and there was a time I would unplug my web router at home to ensure progress was made. You will be shocked how much of a frenzy you’re in on a normal workday."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#get-your-adviser-relationship-straight",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#get-your-adviser-relationship-straight",
    "title": "Unusual Grad School Advice",
    "section": "Get your adviser relationship straight",
    "text": "Get your adviser relationship straight\nEndless ink has been spilled about the risks of studying under a toxic PI (this was fortunately not my case). Virtually everyone I know has experienced some degree of emotional distress from their advisor and/or committee relationship, pain that can linger sometimes years after defending. Why is this the case? Why is it so normal to have “bad bosses” in many other professions (to the point that comedic TV shows, books and social norms are built around this assumption), but folks in the sciences find the experience so damaging and all-consuming? It has to do with structural issues, mainly the fact that under the US system the PI has near-total control over your present work-life and job prospects. This is even more crushing if you don’t have family wealth nor support, or are on a visa. What you can control is how much emotional power you hand to your supervisor. Do not allow them to take the place of your parent or inner self-critic. How your boss treats you has no correlation with your worthiness."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#impostor-syndrome-is-a-scam",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#impostor-syndrome-is-a-scam",
    "title": "Unusual Grad School Advice",
    "section": "Impostor syndrome is a scam",
    "text": "Impostor syndrome is a scam\nThis commencement address and article by Reshma Saujani explain: Impostor Syndrome is a myth, a distraction, a “scheme”. By pathologizing our insecurities we waste too much time trying to think our way into a better state of feeling, and more importantly, direct our focus at our own shortcomings instead of the structural problems that work against our own sense of belonging. The very core of professional-level scientific inquiry is based upon an assumption of ignorance. We should feel uninformed, ignorant and curious most of the time, but the impostor syndrome industry wants that healthy awareness of our temporary intellectual shortcomings to metastasize into generalized personal insecurity and doubt. Which becomes yet another problem for us to solve. So, avoid using shorthand for your experiences. Instead of “I’m suffering from Impostor Syndrome” try “I found this problem set really hard, and didn’t feel supported by the professor when I asked for help”. Then you can disentangle the aspects you can control from those that are structural. This also enables us, future mentors, teachers, and scientific professionals, to ask ourselves: how would this institution look if it were built to support someone like me?"
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-finish-graduate-school-when-you-are-ready-to-be-done",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#you-finish-graduate-school-when-you-are-ready-to-be-done",
    "title": "Unusual Grad School Advice",
    "section": "“You finish graduate school when you are ready to be done”",
    "text": "“You finish graduate school when you are ready to be done”\nA close mentor of mine shared this with me in late 2019 at an international conference. I didn’t really get it at the time, and at first thought it was wrong-headed (i.e., “I’m ready to be done right now, but nobody else agrees!” and “Being ready isn’t the same as the analyses going well…”). In retrospect, they were right. At least 80% of my dissertation progress happened within <30% of the five years I spent in graduate school. The rest of the time, I was working on side projects, dealing with illness, raising my child, taking in family during the pandemic, or performing duties for my main full-time job. The final burst of effort that brought me over the finish line was largely inspired by the looming deadline my work had imposed, and a difficult family situation the preceding fall which reminded me how important it was that I finished, ASAP. The point is that it is a challenge of the US doctoral system that the deadlines and project goals are amorphous and flexible. Most people struggle in that context, and it’s totally unlike the rest of the working world. So, create a hard stop for yourself - “I’m moving in August” (and start making arrangements), “I want to start my family next year” (and start making arrangements), “I’m applying for my dream job in six months” (and start writing the application). This might be the nudge you need to make things happen."
  },
  {
    "objectID": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#references",
    "href": "posts/drafts_deprecated/2023-06-21-gradschool/2023-06-23-unusualGrad.html#references",
    "title": "Unusual Grad School Advice",
    "section": "References",
    "text": "References\nSome books, art, philosophy and things that moved me deeply or simply helped during graduate school\n\nHow to Do Nothing and Saving Time: Discovering a Life Beyond the Clock, both by Jenny Odell. She is an exemplary interdisciplinary thinker and opened my mind to new ways of being.\nRun to be Visible - Lydia Jennings (YouTube) A stunning representation of the difficulties of completing a degree during the pandemic, and the parallels between long-distance running and getting a PhD in a world not built for your success.\nAlma (YouTube) - Instrumental piece by Omar Sosa.\nAlma (YouTube) - Song by El Caribefunk (see translated lyrics if non-Spanish speaking)\nOn Native Land exhibit at the Tacoma Art Museum, particularly Galiano Island by Barbara Boldt (below). This seemingly straightforward landscape contains much about the passage of time and the consequences of sacrifice. Does the water shape the rocks, or do the rocks shape the water? Do the conditions of our life define us, or do we shape conditions to our subconscious selves? Could both be true?\n\n And also: Can equilibrium exist on any real timescale, whether within a human’s life, or the geological life of a beach break? How might this relate to the reliability of a population model that exchanges individuals among areas? I spent many hours on the bench before this painting in March of 2023 while my MSE simulations ran on the server. My dissertation findings regarding “cryptic depletion” (of a vulnerable fish population) was directly informed by my perception of the hidden, holey crags causing the froth at the waterside.\n\nAll books by Cal Newport, particularly “Deep Work” (see minimizing distraction above).\nGabor Maté - Chronic Illness and Stress (YouTube) Our habits program our neuro-physio-immunological experiences. See “You are aging” above."
  }
]