---
title: "Sneaking up on a Well-specified Model"
date: "15 May 2020"
author: "Maia Kapur, Hui-Hua Lee, Kevin Piner, Felipe Carvalho, André Punt"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [solarized-dark.css]
    nature:
      ratio: "17:11"
      beforeInit: "https://platform.twitter.com/widgets.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE}
library(icon)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE,fig.align='center',
                      fig.width = 4, fig.height = 5, 
                      comment = NA, rows.print = 16)
```
layout: true
.header[`r icon::fa('github')` @mkapur/sneak]

???
---
background-image: url("titleslide.png")
background-size: contain
???
Collab & 4th chapter
- Familiar faces
- Exactly 1 yo when project was starting out gave think tank on this
---
background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: contain

???

- at that time I used an analogy from this netflix documentary series to introduce concept of misspecification



--

<br><Br><bR>
# .inverse[ Mis-specification]
???
- show had rocket scientists obsessed w 20s v 2 mins on accuracy 
- which would make diff of whether the mission was accomplished or went hurtling into abyss
- I work on a diff type of model (stock ass used for mgmt.) 
- but spend a lot of time thinking about the causes and ocnsequences of misspecification, 
- for us 20s may be 20000 mt w  flow on effects for livelihoods
- Regardless of whether u saw my talk last year, hope to update you on the dev
- & findings of this work which is now in review



---

background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: contain


# .inverse[ Mis-specification]

<Br><Br><Br><br>
.darktext[ 
+ Model (wrong selectivity form)
+ Process (wrong stock-recruit)
+ Data (amount or quality)
+ And more!
]



---

## Mis-specification: What Happens?

.pull-left[
+ Estimates are biased/imprecise
]

--
.pull-right[
![](ikea.jpg)
]

???
- Why do we care -- what is the outcome?
- Not concerned w fishery/assessment specifics bc thrust @ IPM @ large
- core concern is that misspec = estimates that are biased/imprecise
- Fisheries outcomes feed into mgmt
- other systems make decisions like whether ESA or determine siting of closed area 


- pause to address non-modelers
- next slides nitty-gritty moving parts
- as i worked on this ikea analogy
- if you start to get lost, come back to think of study as all ways to wrongly assemble chair
- and how easy to sit in it
- will circle back to metaphor


---

# Research Questions

+ Do we need to fix every mis-spec before the model performs “well”? 

+ Are certain mis-specifications more or less influential on model bias? 

???
- guiding questions: quantity and identity
- second is like asking are putting the legs on right more important than the back?

---

# Simulation Study

+ Statistical catch-at-age operating model

+ Iteratively create and ‘correct’ mis-specifications

+ Calculate comparative statistics on derived quantities

???

- let's go through some schematics to illustrate how this looks

---
background-image: url("method1.png")
background-position: center
background-size: contain

???
- start with OM
- OM is like manual version of ikea, truth about pop dynamics, params, structure
- "gold standard"

---
background-image: url("method2.png")
background-position: center
background-size: contain
???
- pop quantities like biomass in system etc

---
background-image: url("method3.png")
background-position: center
background-size: contain
???
- take om, data that it generates and tweak it a little bit to build EM (green)
- the EM can be iether similar to or very different from the OM


---
background-image: url("method4.png")
background-position: center
background-size: contain
???
- but what it does do is also print population quantities
- of course the values in those vectors are diff because it underwent some misspec
- now can make a comparison and basically determine how much worse off we are

---
background-image: url("method5.png")
background-position: center
background-size: contain
???
- go thru again but more detail


---
background-image: url("method6.png")
background-position: center
background-size: contain
???
- start w OM, build em fully misspec everything wrong

---
background-image: url("method7.png")
background-position: center
background-size: contain
???
- say we build a second EM with one fewer misspec, or one correction


---
background-image: url("method8.png")
background-position: center
background-size: contain
???
- now we have three sets of pop quants
- and two relative errors, bc we always compare the chair we built (em) to the manual (OM)

---
background-image: url("method9.png")
background-position: center
background-size: contain
???
- other cool thing that arises is we keep track of IDENTITY  what we went in and corrected
- meaning we can write down the change in relative error, or improvement associated with that misspec
---
background-image: url("method10.png")
background-position: center
background-size: contain
???
- how this looks for real
- OM is just this little white box
- stack of 6 green boxes each em, numbers mean misspecs
- the sixths totally corrected
---
background-image: url("method11.png")
background-position: center
background-size: contain
???
- and get relerr on each



---

# .inverse[Misspecifications Explored]

???
let's talk about misspecs we explored
there are 380+ in a given model
and also keep combinations low bc factorial contrib
but wanted to look at those that are commonly used in IPMs, or have analogies
these terms might be fisheries specific but basically any ipm will have
params representing how animals grow, survive, and their reproductivity


for this group, generally process, misspec straightfwd, fix at wrong value




---
background-image: url("pars1.png")
background-position: center
background-size: contain
---
background-image: url("pars2.png")
background-position: center
background-size: contain

???
-final one is most interesting to me and that is sptl structure 
-om is 2 areas, with movement from one to another
-EM misspec assesses all as 1
-this is a rich and active area of research in IPMs & fisheries assessment
-we know that getting space wrong can be fairly detrimental to density dependence and other things
-but particularly interested in seeing how impactful this aspect is in context of other important processes

-NOTE COLORED SQUARES

---
<Br><Br>
##.inverse[1) Do we need to fix every mis-spec <br> before the model performs “well”? ]
???
recall, quantity
---
background-image: url("exOM1_E1.png")
background-position: center
background-size: contain
???

=This is question of quantity - do wee need too...
Now this data is not my real results, this is completely fake for illustration
y axis is relative error, x is # of misspecifications present,
good news for this fake example is that with fewer misspecs the error goes down
doesnt appear to be that great until about XX
- let's look at how this study design can enable us to answer research questions


---
background-image: url("exOM2_E1.png")
background-position: center
background-size: contain
???

This is question of quantity - do wee need too...
Now this data is not my real results, this is completely fake for illustration
y axis is relative error, x is # of misspecifications present,
good news for this fake example is that with fewer misspecs the error goes down
doesnt appear to be that great until about XX

Explain this from 0 - 5

---
background-image: url("exOM3_E1.png")
background-position: center
background-size: contain

--

# .inverse[Models do better with fewer misspecs `r emo::ji("shrug")`]
???


This is question of quantity - do wee need too...
Now this data is not my real results, this is completely fake for illustration
y axis is relative error, x is # of misspecifications present,
good news for this fake example is that with fewer misspecs the error goes down
doesnt appear to be that great until about XX

---
### .inverse[2) Are certain mis-specifications <Br> more or less influential on model performance?  ]
```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE, fig.width=8, fig.height=6, fig.align='center', fig.retina=2}
dat <-
  data.frame("SPECID" = toupper(letters[1:5]), "ABSRELE" = c(runif(2, 0.6, 0.85),
  runif(3, 0.15, 0.4)))
idrel <- ggplot(dat, aes(x = SPECID, y = ABSRELE, fill = SPECID)) +
kaplot::theme_solarized_mk(light = FALSE,base_size = 20)+
  labs(x = "What Got Corrected", y = "Decrease in Relative Error", fill = "") +
  # scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c('black','gold','orange','dodgerblue2','pink')) +
  geom_bar(stat = 'identity', color = 'white') 
idrel
```
???
recall that each EM has a correction, and that enables us to detect what happened when something was corrected
again this is fake
---

### Why this hasn't been done before...

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE,fig.show = "hold", out.width = "20%",out.height='20%', fig.align = "default"}
idrel
knitr::include_graphics("exOM3_E1.png")
```

--

<Br><Br>
.large[
.inverse[N! = all unique, ordered combinations of N misspecs]
]


---
background-image: url("exp1.png")
background-position: center
background-size: contain
???
So what i've done here is taken a figure from a few slides back
- with OM little square on left
- stack of 6 ems on right, now having colored squares representing the EMs

- rotate this graphic and showing it again in at the top
- what this row represents is the 6 ems corresponding to a unique order of correction
- with space fixed first, growth (black) second, and so on
- fixing space first every time, that leaves us with 4! combos of the remaining misspecs


---
background-image: url("exp2.png")
background-position: center
background-size: contain

--

<Br><br><br><Br><Br><Br><Br><br><br><br><Br><br>
.pull-left[
14,400 EMS= <br>
2 Experiments <Br>
x 24 "strings" <br>
x 6 EMs <Br>
x 50 bootstrapped OMs
]

???
- also looked at the same thing but space last
- the reason we did it this way is two fold; wanted to avoid going in and changing the spatial structure of the model multiple times
- also were interested in the precedence of this feautre which is so widely studied.
for example E2 allows us to ask, if you got everything else right, is it really so great to get space done?
---
# Performance Metrics
+ Estimated recruitment over time
+ Estimated stock spawning biomass over time
+ Both quantities within last 10 years

---
# .inverse[Results]
.pull-center[
![](ikea2.jpg)
]


???
- I will walk through the hilites of results, 
- as you can imagine there is a lot to wade thru with 14,000 model outputs

---
background-image: url("SSB_error_panel_E1_dark.png")
background-position: center
background-size: contain
???
- these are from E1. 
- we are seeing what we would expec
- as misspecs go down so does rel err, with the most correct model being
the most correct
---
background-image: url("SSB_Terminal_Tile_E1.png")
background-position: center
background-size: contain

```{r echo = FALSE, fig.align = 'left', out.height="10%", out.width="10%",fig.retina = 2}
knitr::include_graphics("legend3.png")
```
<Br><Br><br><br><Br><Br><br>
<Br><Br><br><br><Br><Br><br>
```{r echo = FALSE, fig.align = 'left', out.height="25%", out.width="25%",fig.retina = 2}
knitr::include_graphics("legend2.png")
```

???
- the more interesteing question is, what is the identity of those misspecs that give us the most bang for our buck?
- this plot is averaged across OMs etc and shows the abs relative error... etc
- the x axis has the # of misspecs aka the panels on previous
- and the colors are the amt of relative error for the last 10 years SSB
- the labels tell you what is misspecified in the model
- the more yellow, the better
- what we see here is that going from 5 to 4 misspecs has an outsize pos effect on rel err,
- which is interesting because in every single case that was where we fixed spatial structure
- improvements continue after that, but the biggest decline happens at that initial step
- this result is especially striking, because...
---
background-image: url("SSB_Terminal_Tile_E2.png")
background-position: center
background-size: contain
<Br><Br><br><br><Br><Br><br>
<Br><Br><br><br><Br><Br><br><Br>
```{r echo = FALSE, fig.align = 'left', out.height="25%", out.width="25%",fig.retina = 2}
knitr::include_graphics("legend2.png")
```
???


- the exact same was found for exp 2, where spatial structure wasn't corrected until after all else
- this plot is setup the same way, with the labeled letters tellin you what's wrong with this model
- as we expect, we do see the model getting more oand more correct with fewer misspecs, but we never get into the "yellow zone" until space is cleared up.

- Another interesting thing we can see from this plot is what besides space seems to have a possitive effect
- on the left side of this image we are looking at models with only one or two correctiongs
- say we were only interested in correcting one thing, moving from 5 to 4 misspecs,
- we see these clusters of lighter areas that seem to have done better
- what they have in common is these were EMs which corrected L (asymp length) or X (selex)
---
background-image: url("ID_boxplot_E2.png")
background-position: center
background-size: contain

```{r echo = FALSE, fig.align = 'left', out.height="25%", out.width="25%",fig.retina = 2}
knitr::include_graphics("legend4.png")
```
???
- can show these results from the previousin the format i introduced at the beginning, 
- where we see the spread of absolute changes in relErr accodring to identity.
- here i have two panels now, the right hand is biomass which is the same as the plot we saw before
- and right is recruitment
- What is cool is that for this experiment, space wasn't corrected till the very end but STILL had the largest impact on relative error 
- NOTE TO SELF: the variation comes from the fact that we're compiling any poss location where X,L etc were corrected, hence the spread. interesting that the trend persists even though there were equivalent # of models which were run with L last etc

---
background-image: url("Rec_error_panel_E1_dark.png")
background-position: center
background-size: contain

???
- one other interesting result to share
- here is E1 again but for recruitment
- as I'd expect, mag/var in rel err goes way up as we increase number of misspecs
- some of this is due to div small numbers, but still
- weirdest thing was this strange banding, which suggests 
- that there are clusters of EMs which are mostly pos or neg biased
- so i took this plot, and re-colored it...
---
background-image: url("Rec_error_M_E1_dark.png")
background-position: center
background-size: contain
???
- to illustrate what value of natural mortality was used within the EMs
- here, blue means the EM was misspecified using a natM above than the OM, and gold was below
- we only start to see white lines up to number 3, because models with 4 or 5 misspecs for E1 only have space misspeced
- the outcome here is that only those models with M and SPACE right are hanging out around the zero line,
- and we can effectively predict the direction of bias in your recruit estimates depending on whether natM is low or hi
---
background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: contain
## .inverse[ Making sense of a massive study I]

???
- one takeaway is my precience @ think tank last year in used the casini sp mission in intro
- because actually at the time of thinktank weren't considering space
- because too hard to implement (but alas grad school)
- proved fitting i picked space example, because..
---
background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: cover
## .inverse[ Making sense of a massive study I]

--

.pull-left[
.large[
.inverse[
+ Space matters most for both biomass & recruitment
]]]

.pull-right[
![](rocketfish.jpg)]

???
- spatial structure ended up having the most outsize influence on model estimates
- and incredibly this held true regardless of whether or not every other imporant thing was right
- we saw from the colored tileplots from both experiments
- whether space was corrected first or last, you got the most bang for your buck by correctly specifying

---
background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: cover
## .inverse[ Making sense of a massive study I]

.pull-left[
.large[
.inverse[
+ Space matters most for both biomass & recruitment
+ For biomass, growth & selectivity are early winners
+ For recruitment, Natural mortality induces systematic bias 
]
]
]

.pull-right[
![](rocketfish.jpg)]

???

- there were also interesting sub-findings of this work
- which was that even if you started with correct spatial structure,
- having misspec M will guarantee that your model outputs are biased,
- and we can even predict the direction depending whether M is hi or low.

- finally exp 2 revaled that while space is most important, the quickest reduction in
- relerr for biomass otherwise came from correcting growth and selex early on.
- so some of your prioritization among these things would depend on if you were more interested in SSB and recruitment

---

background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: contain

## .inverse[ Making sense of a complex study II]

.pull-left[
.inverse[
.large[
+ These studies are...very involved 
]
]
]

.pull-right[
![](rocketfish.jpg)]

???
- now for some more big picture takeaways
- the analysis i've done here is pretty straightforward, just summing and averaging relErr
- i feel that is the easiest to digest for broad audience
- what we've done here is produce a huge and detailed dataset
- i'll be the first to say that as i worked up results i sense that this is hungry for optimization
algo, 
- so i'd be realy happy to collab on subsequent publications with folks with experience in those areas 
- because i think there are hidden patterns that we can't do justice for


---
background-image: url("https://opuszine.us/_assets/entries/cassini-retirement.jpg")
background-size: contain
## .inverse[ Making sense of a massive study II]

.pull-left[
.inverse[
.large[
+ These studies are...very involved 
+ Basic changes = enormous consequences on model estimates
+ Empirical research is worth a lot!
]
] 
]
.pull-right[
![](rocketfish.jpg)]
???
- directed towards the folks working on the research that directly informs 
our params.
- if you are someone cutting up fish heads to nail asymp L
- or perhaps doing genetic work to determine stock structure in a managed area,
- my hope is you can point to this work to re-emphasize the direct, 
- specific contribution your findings will have on the accuracy of our management science.

---
<Br>
# .inverse[thanks]
kapurm@uw.edu

![](rocketfish.jpg)

(yes, I just googled "outer space fish")
---
# .inverse[extra slides]
---
background-image: url("SSB_error_M_E1.png")
background-position: center
background-size: contain


???
SSB vs M E1
not as prnounced
---

background-image: url("RecrTerminal_Tile_E1.png")
background-position: center
background-size: contain
???
recr e1
---

background-image: url("RecrTerminal_Tile_E2.png")
background-position: center
background-size: contain
???
recr e2
---
background-image: url("Screenshot (22).png")
background-position: center
background-size: contain
---
background-image: url("Screenshot (23).png")
background-position: center
background-size: contain
---
background-image: url("Screenshot (24).png")
background-position: center
background-size: contain
---

background-image: url("Screenshot (25).png")
background-position: center
background-size: contain
---
# .inverse[More OM Variation Slides]
---
background-image: url("S3_recdevs_mainpd.png")
background-position: center
background-size: contain

---
background-image: url("S4_Surveys_2Areas_Error.png")
background-position: center
background-size: contain

---
background-image: url("selex_compare_1.png")
background-position: center
background-size: contain

---
background-image: url("summaryF_All.png")
background-position: center
background-size: contain
