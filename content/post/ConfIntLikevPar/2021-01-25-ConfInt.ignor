---
title: Likelihood Profiles and the Conf-Int Confusion
author: ~
date: '2020-12-15'
slug: confint-like
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-12-15T15:36:27-07:00'
featured: no
image:
  caption: ''m
  focal_point: ''
  preview_only: no
projects: []
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 'latex' else 'pandoc'
})
```

One time, during crunch time, I received the following email from a colleague:
"So, I went to get the confidence intervals for the parameters, and multiplied by 1.92 as usual...and they look wrong". My colleague even attached a powerpoint slide from a class which (unclearly) seemed to say that 1.92 was the appropriate multiplier assuming a normal distribution. But it's always 1.96, I thought, feeling confused...and kind of dumb. To avoid falling into this trap in the future, let me clear up the confusion once and for all.

# A Brief Profile of Likelihood Profiles
Here we are discussing confidence intervals in a maximum likelihood context. I'm going to pretend we're evaluating (minimizing) the negative log-likelihood for a model with parameter vector $\bar\theta$. We might be curious about how well the data inform us about the estimates of a given parameter, call it $\theta_i$. This can be visualized by constructing a likelihood profile, which shows the "best" (smallest) NLL obtained if $\theta_i$ is fixed at one of many values.

```{r echo = FALSE, warning = FALSE, message = FALSE}

theta0 <- -20:20
good <- (2*theta0^2)/100
bad <- (0.5*theta0^2)/100
prof = data.frame(theta = (theta0+40)/15,good,bad)

black_nll = good


require(ggplot2)
ggplot(reshape2::melt(prof, id = 'theta'), 
       aes(x = theta, 
           y = value,
           col = variable)) +
  theme_sleek()+
  theme(legend.position = 'none') +
  labs(x = expression(theta~i), y = 'NLL') +
  geom_line(lwd = 1) 
```


One useful bit of information here is the "steepness" of the profile. If it's steeper, like the black curve above, it means the data are pretty informative on $\theta_i$ and we are pretty confident about our MLE for that parameter. But why? What is it about the shape of the profile that means there aren't many other candidate values for $\theta_i$ which are very good? *Enter the 1.92 confuse-maker*. 

# Crashing into the chi-squared distribution
What you need to know here is that if we are interested in comparing models in terms of likelihood differences, we are really comparing ratios (since the difference between logged values is a quotient). For this reason we need to employ the $\chi^2$ distribution, and can ask a question like, which values of $\theta_i$ compose the 95% confidence interval? The answer is given by the range of $\theta_i$ for which the *difference* between $NLL_{\theta_i}$ and$NLL_{\theta_i}min$ is at or below the $\chi^2$ test value for a 95% interval for comparing two models. This value is $\frac{\chi^2_{(0.95,1)}}{2}$:

```{r echo = T}
qchisq(0.95,1)/2
```

In our example from above, we can leverage this value to evaluate which values of $\theta_i$ return NLLs within 1.92 of the minimum NLL (which, by definition, is obtained by our MLE). For simplicity, my example has the MLE value for with an NLL of zero, but it's possible that your value is different. Below, I'm using the NLL values from the steeper black curve in the first figure.


$$
LL_\theta - LL_{\theta min} \leq \frac{\chi^2_{(0.95,1)}}{2}
$$

This code will return the actual values of  $\theta_i$ which satisfy these conditions:
```{r echo = T}
chisq_black <- theta[which(black_nll - min(black_nll) <= qchisq(0.95,1)/2)]
```

Which we can then add to our plot:
```{r echo = FALSE}
profm <- reshape2::melt(prof, id = 'theta')

ggplot(profm, 
       aes(x = theta, 
           y = value,
           col = variable)) +
  theme_sleek()+
  theme(legend.position = 'none') +
  labs(x = expression(theta~i), y = 'NLL') +
  geom_line(lwd = 1) +
  geom_point(data = subset(profm, value <= 1.92 & variable == 'good'), col = 'blue') +
  geom_hline(yintercept = 1.92, col = 'blue')
```

Two things to notice here. One is that we didn't identify exactly which values of $\theta_i$ returned an NLL of 1.92 exctly, which has to do with the granularity of our search. We only did evaluations where the points are, so can only filter for points that fell above or below that line (more on that in a sec). Secondly, you'll notice that you can roughly identify the range of $\theta_i$ which met our critera. If you repeat the same exercise for the shallower gold line, you'll find that there are in fact more $\theta_i$ values which meet the critera. Another way of saying this is that in the "shallower profile" case, we are less certain that the best estimate of $\theta_i$ falls within a narrow range, and instead have a broad set of candidate $\theta_i$ which constitute the 95% CI.

```{r echo = FALSE}
ggplot(profm, 
       aes(x = theta, 
           y = value,
           col = variable)) +
  theme_sleek()+
  theme(legend.position = 'none') +
  labs(x = expression(theta~i), y = 'NLL') +
  geom_line(lwd = 1) +
  geom_point(data = subset(profm, value <= 1.92 & variable == 'good'), col = 'black') +
    geom_point(data = subset(profm, value <= 1.92 & variable == 'bad'), col = 'blue') +
  geom_hline(yintercept = 1.92, col = 'blue')
```

# What about 1.96?
The good news is, still assuming we are working with these symmetrical, normally-distributed parameters, the bridge to understanding 1.96 is not far off. In the previous section, we used the log-ratio aspect of comparing NLLs to determine which parameter values fell below the cutoff defined by the chi-squared distribution. However, this only enabled us to filter amidst parameter values we'd already evaluated, and didn't provide the exactl value of theta i which represents the 95% confidnce interval. For that, we use the idea that the 2.5th percentile and 97.5th percentile are each within 1.96 standard deviations of the mean (the MLE, in this case). 1.96 comes from evaluating the normal distribution at the 0.025 and 0.975 confidence level:

```{r}
qnorm(0.025,0,1)
qnorm(0.975,0,1)
```
Since that calculation is performed assuming a standard deviation of 1, we can multiply it by the standard deviation of our parameters and add to the MLE to get the precice value of the paraemter at the interval:
```{r}
sd_theta <- sd(theta)
lci <- qnorm(0.025,0,1)*sd_theta + theta[which.min(black_nll)]
uci <- qnorm(0.975,0,1)*sd_theta + theta[which.min(black_nll)]
```

Adding these values to our plot, we'd expect them to encompass the points we identified earlier but intercept precicely with the horizontal line representing the chi-sq test from before:

```{r echo = FALSE}
ggplot(profm, 
       aes(x = theta, 
           y = value,
           col = variable)) +
  theme_sleek()+
  theme(legend.position = 'none') +
  labs(x = expression(theta~i), y = 'NLL') +
  geom_line(lwd = 1) +
  geom_point(data = subset(profm, value <= 1.92 & variable == 'good'), col = 'blue') +
  geom_vline(xintercept = c(lci, uci), col= 'blue') +
    geom_point(data = subset(profm, value <= 1.92 & variable == 'bad'), col = 'blue') +
  geom_hline(yintercept = 1.92, col = 'blue')
```