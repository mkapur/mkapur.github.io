---
title: "How Do Recruitment Deviations Work?"
author: "MSK"
editor: visual
date: '2021-01-25'
slug: how-rec-devs
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-01-20T15:36:27-07:00'
featured: yes
image: recdev-img.jpg
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 'latex' else 'pandoc'
})
```

For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out: Methot, R.D., Taylor, I.G., Chen, Y., 2011. *Adjusting for bias due to variability of estimated recruitments in fishery assessment models.* Can. J. Fish. Aquat. Sci. 68, 1744--1760. <https://doi.org/10.1139/f2011-092>.

Here I want to jot down the practical implementation behind the recruitment deviation. The topics covered here are 1) The meaning of a recruitment deviation (abbreviated here to "rec-dev"), 2) the common mathematical syntax for these values, and 3) the practical meaning of lognormal bias correction. This post will *not* address advanced implementation topics, such as likelihood penalties and bias correction ramps.

# What is a rec-dev?

A recruitment deviation is how much a given year's recruitment *deviates* from what you'd expect from average. This could be a static value (e.g., unfished recruitment), or the deterministic value given by a stock-recruit relationship (in this post I'll use the Beverton-Holt [1957] as an example). The idea is that stock productivity is likely not the same every single year, and we need a flexible method to capture that variation in our model fitting procedures. Reasons for these *deviations* could be environmental, and are generally unexplained -- which is why the time series of rec-devs is often interpreted as process error in your system (extra noise in the system not captured by other processes).

Let's quickly refresh what we're talking about when we say the mean or expected recruitment level. Again, using the Bev-Holt as an example, this equation describes the expected number of recruits given a stock spawning biomass (SSB). (This syntax is what's commonly used in Stock Synthesis models: check out my post on [recruitment algebra](https://mkapur.github.io/posts/2020-05-21-recAlg/2020-05-21-recAlg.html#bonus-round-stock-synthesis-recruitment-syntax) for the derivation).

$$ 
R_{expected} = \frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)} 
$$

$h$ is steepness, or the expected proportion of $R_0$ anticipated at $0.2SSB_0$; $R_0$ and $SSB_0$ are unfished recruitment and unfished spawning biomass, respectively. Given values for these parameters, the "mean" or "expected recruitment" level is what you'd get by plugging in those numbers to the values and reading off the curve:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
bh <- function(ssb,h,r0,ssb0){
  num = ssb*4*h*r0
  denom1 = ssb0*(1-h)
  denom2 = ssb*(5*h-1)
  ans = num/(denom1+denom2)
  return(ans)
}
require(ggplot2)
require(ggsidekick)

sd_log <- log(0.8)
mu_log <- 0
recdf <- data.frame()
for(b in 1:100){
  recdf[b,'b'] <- b
  recdf[b,'detRec'] <- bh(ssb = b, 
                          h = 0.6,
                      r0 = 150, ssb0 = 75)
  
  ## non-corrected errors around the curve
  ## recall that logN errors are multiplicative
  # https://influentialpoints.com/Training/log_normal_confidence_intervals.htm
  # recdf[b,'lci'] <- recdf[b,'detRec']+1.96*exp(SDR)
  # recdf[b,'uci'] <- recdf[b,'detRec']-1.96*exp(SDR)
  # C <- exp(1.96*SDR*log(recdf[b,'detRec']))
  # C = 1.96*sqrt(exp(sd_log^2-1)*exp(2*mu_log+sd_log^2))
  # recdf[b,'lci'] <- recdf[b,'detRec']-C ## otherwise D/C assuming D is in log space
  # recdf[b,'uci'] <- recdf[b,'detRec']+C ## otherwise DC if D in log space
  # Generate error values following a lognormal distribution
  dev_raw<- rnorm(1, mean = mu_log, sd = exp(sd_log))
  error_values <- dev_raw*exp(-sd_log^2/2)
  unadjusted_error_values <- dev_raw
  # Calculate confidence interval
  z_value <- qnorm(1 - (1 - 0.5) / 2)
  error_multiplier <- exp(z_value * sd_log)  # Multiplier for error values
  
  # Calculate upper and lower bounds of confidence interval
  recdf[b,'lci']  <- recdf[b,'detRec'] * error_multiplier
  recdf[b,'uci'] <- recdf[b,'detRec'] / error_multiplier
  
  ## draw from a normal distribution with mean detRec and sigma
  # recdf[b,'rand'] <- ifelse(b %% 5 == 0, rnorm(1,mean = recdf[b,'detRec'], sd = SDR  ),NA)
  # recdf[b,'dev'] <- recdf[b,'rand']-recdf[b,'detRec'] ## raw difference in values
  # recdf[b,'corr'] <- recdf[b,'detRec']*exp( recdf[b,'dev']-SDR^2/2) ## should return rand
  
  ## draw from a normal distribution with mean detRec and sigma
  # recdf[b,'dev'] <- ifelse(b %% 5 == 0, rnorm(1,mean = mu_log, sd = sd_log),NA)
    # recdf[b,'corr'] <- recdf[b,'detRec']*exp(recdf[b,'dev']-sd_log^2/2) 
   recdf[b,'dev'] <- error_values
     recdf[b,'dev_wrong'] <- unadjusted_error_values
  # recdf[b,'dev'] <- ifelse(b %% 5 == 0, rlnorm(1, meanlog = mu_log, sdlog = sd_log),NA)
  recdf[b,'corr'] <- recdf[b,'detRec'] * exp(  recdf[b,'dev'] )

  
}

ggplot(recdf, aes(x = b, y = detRec)) +
  geom_line(lwd = 1) +
  theme_sleek() +
  labs(x = 'SB', y = 'Recruitment')

```

<!-- Cool. Now let's consider that we have some notion of error around this deterministic value; this error could arise from uncertainty in the parameters of the stock recruit curve. Here, the shaded zone is the 95% confidence interval around our expected recruitment. 95% of recruitment values in a given year at a given SB should fall within that zone. -->

<!-- ```{r, echo = FALSE, message = FALSE, warning = FALSE} -->

<!-- ggplot(recdf, aes(x = b, y = detRec)) + -->

<!--   geom_line(lwd = 1) + -->

<!--   theme_sleek() + -->

<!--   labs(x = 'SB', y = 'Recruitment') + -->

<!--   geom_ribbon(aes(ymin = lci, ymax = uci), alpha = 0.2) -->

<!-- ``` -->

However, the recruitment in a given year may *deviate* from the black curve. We can visualize these in two ways.  On the bottom left, I've invented a series of new recruitment values shown as gold points; the dotted line separating each point from the expected value is the **raw** value of the deviation itself (in numbers). A time series of these values is shown on the right. The blue point on both plots is the highest year -- and you can see that since this is plotted in natural (not log) space it's fairly hard to distinguish the smaller points from one another.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
p1 <- ggplot(recdf, aes(x = b, y = detRec)) +
  geom_line(lwd = 1) +

  theme_sleek() +
  labs(x = 'SB', y = 'Recruitment') +
  # geom_ribbon(aes(ymin = lci, ymax = uci), alpha = 0.2) +
  geom_point(data = recdf[-which.max(recdf$corr),], aes(y = corr), col = 'goldenrod') +
  geom_point(data = recdf[which.max(recdf$corr),], aes(y = corr), pch =8, col = 'blue') +
  geom_segment(aes(xend = b, yend = corr),
               linetype = 'dotted')

p2 <- ggplot(recdf, aes(x = b, y =corr)) + 
  theme_sleek() +
  labs(x = 'SB', y = 'Recruitment') +
  # scale_y_continuous(limits = c(0,300))+
  geom_line(aes(y = corr), col = 'goldenrod', lwd = 1) +
  geom_point(data = recdf[which.max(recdf$corr),], aes(y = corr), pch =8, col = 'blue') +
  geom_segment(aes(xend = b, yend = corr),
               linetype = 'dotted')

Rmisc::multiplot(plotlist = list(p1,p2),cols = 2)
```

Let's think for a second. If we deal in absolutes, the raw (absolute) value of a deviation at high biomass is going to be much larger than the deviation at low biomass. This isn't very helpful if we're trying to quantify the degree to which our stock's recruitment in year $y$ is diverging from expectation. For that reason, we are interested in modeling the errors, or deviations, as *lognormally distributed*. Equation-wise, here's what that looks like. Note that $R_{det}$ is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.

$$
R_{expected,y} = R_{det,y}e^{(dev_y-\sigma^2/2)}
$$

Now we'll quickly refresh the idea behind the lognormal distribution and explain what's up with that $-\sigma^2/2$.

# Refreshing the lognormal distribution.

Check out (towardsdatascience.com)\[https://towardsdatascience.com/log-normal-distribution-a-simple-explanation-7605864fb67c\] for a good background read.

If our deviations vector $\vec r$ is normally distributed with mean zero, $exp(\vec r)$ is *lognormally* distributed, shown in blue below:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
sigma = 1
mu = 0
X <- rnorm(1000,mu,sigma)
hist(X, ylim = c(0,300), 
     main = '',
     xlab = '',
     xlim = c(-5,30),
     col  = alpha('grey22',0.5))
hist(exp(X), add = TRUE,
     breaks = seq(0,100,0.5),
     col = alpha('blue',0.5))
abline(v =exp(mean(X)) , lty = 2, lwd = 2, col = 'grey22', add = T )
abline(v =mean(exp(X)) , lty = 2, lwd = 2, col = 'blue', add = T )
legend('topright',
       lty = 1, lwd = 5,
       col = c(alpha('grey22',0.5),
               alpha('blue',0.5),
               'grey22', 'blue'),
       legend = c('X~N(0,1)','Y = exp(X)',
                  'exp(mean(X))','mean(Y)'))
```

If $r_y \sim N(\mu=0,\sigma=1)$ and $Y = e^{r_y}$, the expected value of $r_y$ is $\mu$ (zero), and the expected value of $Y$ will be $e^{\mu+\sigma^2/2}$ (`r exp(0+sigma^2/2)`), *not* $e^{\mu}$.

Note that $e^{r_y}$, the blue values above, are 1) never less than zero and 2) asymmetrically distributed. This means mathematically is that the expected value, or mean, of $e^{r_y}$ (blue dashed line above) *is in fact not the same* as $exp(mean(r_y))$. Confirm this for yourself below.

```{r, echo = TRUE, warning = FALSE}
## should be close to 0
mean(X) 
## should be close to 1
exp(mean(X))
## yet the mean of the blue distribution is larger...
mean(exp(X)) 
##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)
exp(mu+sigma^2/2)
```

That's fine statistically, but makes for a problem if we calculate our annual recruitment by multiplying $R_{det}$ by $exp(r_y)$: we would end up with inflated average recruitments because we would be multiplying by values *typically* greater than 1.

# Bias correction to the rescue

We set the mean for $X$ to 0 in hopes that, on an "average" year, we'll be multiplying $R_{det,y}$ (deterministic recruitment in year *y*) by $exp(0) = 1$, and get the expected value back. However, because we've discovered that $exp(r_y)$ is not symmetrical, we need to perform a *bias correction* to confirm that we *approximately* return the $R_{det,y}$ when $r_y$ is zero. In other words, we back-transform the deviations to confirm that the mean of those deviations is roughly one, on a normal scale (after exponentiation).

Check it out:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
hist(X, ylim = c(0,500), 
     xlim = c(-5,30),
     xlab = '',
     main = '',
     col  = alpha('grey22',0.5))
hist(exp(X), add = TRUE,
     breaks = seq(0,100,0.5),
     col = alpha('blue',0.5))
hist(exp(X-sigma^2/2), add = TRUE,
     breaks = seq(0,100,0.5),
     col = alpha('red',0.5))
legend('topright',
       lty = 1, lwd = 5,
       col = c(alpha('grey22',0.5),
               alpha('blue',0.5),
               alpha('red',0.5)),
       legend = c('X~N(0,1)','Y=exp(X)', 'Bias Adjusted'))
```

```{r, echo = T, warning = FALSE}
## as above, the mean is close to 1 without adjustment
exp(mean(X)) 
## ...though the actual mean is greater, by a factor of sigma^2/2
mean(exp(X)) 
mean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!
```

One last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above).

Imagining we had a time series of data which encompassed all the different $SB$ values corresponding to the gold points above, we could plot the deviations through time (I randomly reordered the values):

```{r, echo = FALSE, message = FALSE, warning = FALSE}
require(dplyr)

recdfsamp <- 
  sample_n(data.frame(recdf[!is.na(recdf$dev),]),
           size = nrow(recdf[!is.na(recdf$dev),]),
           replace = TRUE) %>%
  mutate(Year = 1:nrow(recdf[!is.na(recdf$dev),]))

ggplot(recdfsamp, aes(x = Year)) +
  theme_sleek() +
  labs(x = 'Year', y = 'log Recruitment deviation') +
  # geom_point(aes(y = dev_wrong), color = 'red') +
  geom_point(aes(y = dev)) +
  geom_line(linetype = 'dotted', aes(y = dev)) +
  # geom_segment(aes(xend = Year, yend = 0),linetype = 'dotted') +
  geom_hline(yintercept = 0)

```

So now, the horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were "worse than average years", and vice versa. What's more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year *regardless of the stock size at hand*.

# Bonus: why this isn't perfect

This came up on my General Exam for my PhD. I was asked to walk through the basic logic of the recruitment deviation and why we apply bias correction. Then I was asked, "why is this wrong?".

Part of the answer lies in the fact that this is a conscious decision made by *fisheries population dynamicists* to make parts of our models behave in the way we want. Specifically, after applying our bias adjustment factor, the resulting recruitment timeseries will have a median value that is less than the deterministic curve shown above. In the Methot & Taylor paper linked up top, they mention that this is sensible since the average recruitment is most representative of a recruitment event's long-term contribution to the population, "because most of the population biomass comes from the numerous recruits in the upper tail of the lognormal distribution". If you're working with a different data type or hope to gain information from the *median* behavior of this process, such an adjustment is not for you.

# Bonus 2: A note on lognormal CVs

This came up during some work on VAST. f you have low log-SDs (i.e., \<0.1), then the log-SD is basically identical to CV -- log-SD is the SD of the index in log-space, and makes sense if you're fitting the index as a lognormal distribution. **Here's how to convert a log-normal mean and standard deviation to a CV on the normal scale**, otherwise.

Consider a random variable X that follows a lognormal distribution: $X \sim Lognormal(μ, σ)$

We know that when converting into non-log space, the mean of $X$ should be $μ_X = e(μ + (σ^2)/2)$ and the standard deviation is $σ_X = \sqrt{(e^{σ^2}-1)} e^{μ + σ^2/2}$ (check the [lognormal wiki](https://en.wikipedia.org/wiki/Log-normal_distribution#Probability_in_different_domains) page under "Arithmetic Moments" for details).

The coefficient of variation (CV) on the original scale is defined as the ratio of the standard deviation to the mean of the random variable X: $CV_{original} = \frac{σ_X}{μ_X}$

Via substitution:

$\frac{\sqrt{(e^{σ^2}-1)}e^{μ + σ^2/2}} {e^{μ + σ^2/2}}$

Cancel out the common term ($e^{μ + σ^2/2}$) in the numerator and denominator, leaving:

$\sqrt{e^{σ^2}-1}$
