[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Maia Sosa Kapur",
    "section": "",
    "text": "Ageing Error Matrices - some notes\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging try_again() for error catching in MSEs\n\n\n\n\n\n\n\n\n\nMSK\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLength-based Processes in an Age-Structured Model\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Keep Up With the Literature\n\n\n\n\n\n\n\n\n\nMSK\n\n\nFeb 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do Recruitment Deviates Work?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we use ln(19)?\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD General Exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Algebra for Equilibrium Recruitment\n\n\n\n\n\n\n\n\n\nMSK\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I studied for my PhD qualifying exam\n\n\n\n\n\n\n\n\n\nMSK\n\n\nJan 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "href": "posts/2021-01-25_why_ln19/2021-01-25-ln19.html",
    "title": "Why do we use ln(19)?",
    "section": "",
    "text": "Have you ever noticed that sneaky \\(log(19)\\) in your logistic equation for maturity or selectivity? What is up with that?\nLet’s take the example of a basic selectivity-at-age function, which uses the parameters \\(a_{50}\\) and \\(a_{95}\\). These represent the ages where a fish has a probability 50% or 95% chance of being captured. Read that again carefully. I took these parameters for granted, but the meaning of them is intertwined with the use of \\(ln(19)\\), and if they change, so does the logged value.\nHere’s a selectivity plot with some made up numbers. The selectivity function I’m using is:\n\\[\nS_a = \\langle 1+exp(-log(19)*\\frac{a-a_{50}}{a_{95}-a_{50}}) \\rangle ^{-1}\n\\]\n\n\n\n\n\nNote how nicely these values extend between zero and one (on the y axis)? That’s because we’re really interested in working with the odds of being captured. Based on our parameter definitions, we need to ensure that the logistic function equals 0.95 when \\(a = a_{95}\\) (and similarly equals 0.5 at \\(a_{50}\\)). To achieve this, we recognize the exponent of the log odds equals the odds ratio. You can think of the odds ratio as the probability of your event (getting captured) happening over the probability of it not happening. We can leverage this to force the logistic equation to scale the way we want.\nSince we’ve already defined \\(a_{95}\\) as the age at which we have a 95% probability of getting captured, the log-odds can be defined as \\(log(\\frac{0.95}{0.05})\\). This could be rewritten as \\(log(\\frac{19/20}{1/20})\\) and simplifies to \\(log(19)\\). This ensures that when \\(a = a_{95}\\), the equation up top indeed returns 0.95, which is the logistic function evaluated at an odds ratio of 95%.\nGiven this syntax you’ll see how we would never really need to correct for \\(a_{50}\\), since the odds ratio of two events with 50% probability is actually zero (plug in 0.5 to see for yourself).\nTwo notes:\n\nSometimes to ease model fitting, we will simplify the above equation by substituting \\(\\delta\\) in place of the denominator (\\(a_{95} - a_{50}\\)), or estimate \\(\\delta\\) as \\(\\frac{ln(19)}{a_{95} - a_{50}}\\). (Specifically, if you have the \\(a_{50}\\) and \\(\\delta\\) values from, say, a Stock Synthesis model, you can solve for \\(a_{95}\\) via \\(a_{95} = \\frac{-ln(19)}{\\delta}+a_{50}\\)).\nIf you’d want to use the something like \\(a_{75}\\), you’d need to update the log odds via \\(log(\\frac{0.75}{0.25})\\) or ~ \\(log(3)\\). Note that this ensures that you are getting a 75% selectivity at the corresponding age, but provides a distinct curve from before."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently both a federal Research Mathematical Statistician at the Alaska Fisheries Science Center (NOAA, Seattle) and a PhD Candidate at the School of Aquatic and Fisheries Science at the University of Washington in the Punt Lab. My research was supported by a NMFS-Sea Grant Population Dynamics Fellowship awarded in 2019.\nPreviously I worked as an Assessment Specialist at the Joint Institute for Marine and Atmospheric Research at PIFSC (NOAA) in Honolulu and completed my MSc at the Hawaii Institute of Marine Biology. I have also worked in the Florida Keys and studied at the Smithsonian Tropical Research Institute in Bocas Del Toro, Panama while completing my BSc in Environmental Science at UC Berkeley."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "You can find an up-to-date list of my peer-reviewed publications on Google Scholar.\nBecause much of my work directly informs the management of industrial fisheries in the United States and international waters, a large body of my research is contained in grey literature: technical memoranda and formal stock assessment reports submitted to regional fishery management organizations. A list of these publications, and links (where available), is provided below.\nStock Assessments Used for Fisheries Management\n\nKapur, M. and Monnohan, C. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2022. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2022/BSAIflathead.pdf\nKapur, M. Assessment of the Flathead Sole Stock in the Gulf of Alaska. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/GOAflathead.pdf\nKapur, M. Assessment of the Flathead Sole-Bering flounder Stock in the Bering Sea and Aleutian Islands. North Pacific Fishery Management Council, 2021. https://apps-afsc.fisheries.noaa.gov/Plan_Team/2021/BSAIflathead.pdf\nKapur, M., Lee, Q., Haltuch, M., Gertserva, V., and Hamel, O. Status update of the U.S. sablefish stock in 2021. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. https://www.pcouncil.org/documents/2021/06/g-5-attachment-5-draft-status-of-sablefish-anoplopoma-fimbria-along-the-us-west-coast-in-2021-electronic-only.pdf/ 108 p.\nHaltuch, M., Johnson, K., Tolimieri, N, Kapur, M., and Castillo-Jordán, C. Status of the sablefish stock in U.S. waters in 2019. Pacific Fisheries Management Council, 7700 Ambassador Place NE, Suite 200, Portland, OR. 398 p. https://www.pcouncil.org/wp-content/uploads/2019/10/sablefish_20191022.pdf\nKapur, M., Hamel, O. Catch-only update for China Rockfish in 2019.\nAdams, G. D., Kapur, M., McQuaw, K., Thurner, S., Hamel, O.S., Stephens, A. & Wetzel, C. R. 2019. Stock Assessment Update: Status of Widow Rockfish (Sebastes entomelas) Along the U.S. West Coast in 2019. Pacific Fishery Management Council, Portland, Oregon. https://www.pcouncil.org/wp-content/uploads/2019/10/2019WidowUpdate_final_clean.pdf\nHamel, O. Kapur, M., Catch-only update for Blackgill Rockfish in 2019.\nKapur, M., Fitchett, M., Yau, A., Carvalho F. 2019. 2018 Benchmark Stock Assessment of Main Hawaiian Islands Kona Crab. NOAA Tech Memo. NMFS-PIFSC-77, 114 p. https://doi.org/10.25923/7wf2-f040 Available at: https://www.fisheries.noaa.gov/resource/document/2018-benchmark-stock-assessment-main-hawaiian-islands-kona-crab\nISC, 2019 (contributor). 2019 Benchmark stock assessment and future projections for the Western and Central North Pacific striped marlin. Honolulu, Hawaii, USA. Available at:http://isc.fra.go.jp/pdf/ISC19/ISC19_ANNEX11_Stock_Assessment_Report_for_Striped_Marlin.pdf\nISC, 2018 (contributor). Stock Assessment of Shortfin Mako Shark in the North Pacific Ocean Through 2016 (No. ISC/18/ANNEX/15). Yeosu, Republic of Korea. Available at: http://isc.fra.go.jp/pdf/ISC18/ISC_18_ANNEX_15_Shortfin_Mako_Shark_Stock_Assessment_FINAL.pdf\nLangseth, B., Syslo, J., Yau, A., Kapur, M., Brodziak, J., 2018. Stock assessment for the main Hawaiian Islands Deep 7 bottomfish complex in 2018, with catch projections through 2022. NOAA Tech. Memo. NMFS-PIFSC 69, 217. Available at: https://repository.library.noaa.gov/view/noaa/17252\nWinker, H., Carvalho, F., Kapur, M., Parker, D., Kerwath, S., 2017. JABBA goes IOTC: Just Another Bayesian Biomass Assessment for Indian Ocean Blue shark and swordfish (No. IOTC-2017-WPB15-INF02). Available at: https://www.iotc.org/sites/default/files/documents/2017/10/IOTC-2017-WPM08-11_Rev_1.pdf\n\nTechnical Documents Submitted to Fisheries Management Organizations\n\nHamel, O., Punt, A.E., Kapur, M. and Maunder, M. 2023. Natural Mortality: Theory, Estimation, and Application in Fishery Stock Assessment Models. U.S. Department of Commerce, NOAA Processed Report NMFSNWFSC-PR-2023-02. https://doi.org/10.25923/xqv2-2g73\nKapur, M., Connors, B., DeVore, J., Fenske, K., Haltuch, M., Key, M. 2021. Transboundary Sablefish Management Strategy Evaluation (MSE) Workshop Report. Pacific Fishery Management Council, Supplemental Informational Report 5. https://www.pcouncil.org/documents/2021/06/supplemental-informational-report-5-council-sponsors-successful-sablefish-management-strategy-evaluation-workshop.pdf/\nCarvalho, F., Kapur, M., Ijima, H. 2019. Comparison between Preliminary Stock Synthesis Models for the 2019 WCNPO Striped Marlin Stock Assessment. http://isc.fra.go.jp/pdf/BILL/ISC19_BILL_2/ISC19_BILLWG2_WP4.pdf\nFenske, K. H., Berger, A. M., Connors, B., Cope, J.M., Cox, S. P., Haltuch, M. A., Hanselman, D. H., Kapur, M., Lacko, L., Lunsford , C., Rodgveller, C., and Williams, B. 2019. Report on the 2018 International Sablefish Workshop. U.S. Dep. Commer., NOAA Tech. Memo. NMFS-AFSC-387, 107 p. Available at: https://www.afsc.noaa.gov/Publications/AFSC-TM/NOAA-TM-AFSC-387.pdf\nSculley, M., Kapur, M., Yau, A., 2018. Size composition for swordfish Xiphias gladius in the Hawaii-based pelagic longline fishery for 1995-2016 (No. WP-18-003). https://doi.org/10.7289/V5/WP-PIFSC-18-003\nSculley, M., Yau, A., Kapur, M., 2018. Standardization of the swordfish Xiphias gladius catch per unit effort data caught by the Hawaii-based longline fishery from 1994-2016 using generalized linear models. (No. WP-18-001). https://doi.org/10.7289/V5/WP-PIFSC-18-001\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. Performance evaluation of JABBA-Select against an age-structured simulator and estimation model (No. MARAM/IWS/2017/Linefish/P3 Performance). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P3.pdf\nWinker, H., Carvalho, F., Thorson, J.T., Kapur, M., Parker, D., Kerwath, S., Booth, A.J., Kell, L., 2017. JABBA-Select: an alternative surplus production model to account for changes in selectivity and relative mortality from multiple fisheries (No. MARAM/IWS/2017/Linefish/P2). Cape Town, South Africa. Available at: https://drupalupload.uct.ac.za/maram/Documents/pub/2017/IWS 2017/MARAM_IWS_2017_Linefish_P2.pdf\nKapur, M., Yau, A., 2018. Size Compositions and Sex Ratios of Oceanic Whitetip Sharks and Giant Manta Rays for Longline Fisheries in the Pacific Islands Region (DR-18-012). Honolulu, HI. https://doi.org/10.25923/67hf-bn93\nKapur, M., Brodziak, J.A., Fletcher, E.J., Yau, A.J., 2017. Summary of Life History and Stock Assessment Results for Pacific Blue Marlin, Western and Central North Pacific Striped Marlin, and North Pacific Swordfish. (No. WP-17-004). 10.7289/V5/WP-PIFSC-17-004 Available at: http://www.pifsc.noaa.gov/library/pubs/WP-17-004.pdf\nSculley, M.S., Brodziak, J.A., Yau, A.J., Kapur, M., 2017. An Exploratory Analysis of Trends in Swordfish (Xiphias gladius) Length Composition Data from the Hawaiian Longline Fishery (No. WP-17-002). 10.7289/V5/WP-PIFSC-17-002. Available at: https://repository.library.noaa.gov/view/noaa/14862/noaa_14862_DS1.pdf"
  },
  {
    "objectID": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "href": "posts/2020-01-01_how-i-studied-for-quals/quals.html",
    "title": "How I studied for my qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a https://www.coursera.org/learn/bayesian-statistics (free, online) course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2021-01-25_recDevs/recdevs.html",
    "href": "posts/2021-01-25_recDevs/recdevs.html",
    "title": "How Do Recruitment Deviates Work?",
    "section": "",
    "text": "For an in-depth description of the theory and math behind recruitment deviations in assessment models, check out Methot, R.D., Taylor, I.G., Chen, Y., 2011. Adjusting for bias due to variability of estimated recruitments in fishery assessment models. Can. J. Fish. Aquat. Sci. 68, 1744–1760. https://doi.org/10.1139/f2011-092.\nHere I want to jot down the practical implementation behind the recruitment deviate for those working in Stock Synthesis or building their own models from scratch. The main topics to discuss are 1) The meaning of a recruitment deviate (abbreviated here to “rec-dev”), 2) how it plays in your dynamics equations, and 3) keeping an eye out for lognormal bias correction.\n\nWhat is a rec-dev?\nA recruitment deviate is how much a given year’s recruitment deviates from what you’d expect given a stock-recruit relationship (in this post I’ll use the Beverton-Holt as an example). The idea is that there could be many reasons for a stock to produce a number of recruits which is different from the deterministic value defined by the current stock size (among other parameters). Reasons for this deviation could be environmental, and are generally unexplained – which is why the time series of rec devs is often interpreted as process error in your system (all the extra natural noise our model can’t capture).\nLet’s quickly refresh what we’re talking about when we say the “mean” or expected recruitment level. Again, using the Bev-Holt as an example:\n\\[\nR_{expected} = \\frac{SSB 4hR_0}{SSB_0(1-h)+ SSB(5h-1)}\n\\]\n\\(h\\) is steepness, or the expected proportion of \\(R_0\\) anticipated at \\(0.2SSB_0\\); \\(R_0\\) and \\(SSB_0\\) are virgin recruitment and spawning biomass, respectively.\nIf we make up some numbers for these parameters and plug them in, the resultant relationship might look something like this:\n\n\n\n\n\nCool. Now let’s consider that we have some notion of error around this deterministic value; this error could arise from uncertainty in the parameters of the stock recruit curve. Here, the shaded zone is the 95% confidence interval around our expected recruitment. 95% of recruitment values in a given year at a given SB should fall within that zone.\n\n\n\n\n\nGiven this error, your observed recruitment in a given year may deviate from the black curve somewhat. The new observed recruitments are shown as gold points; the line separating each point from the expected value is the raw value of the deviate itself.\n\n\n\n\n\nLet’s think for a second. If we deal in absolutes, the raw (absolute) value of a deviate at high biomass is going to be much larger than the deviate at low biomass. This isn’t very helpful if we’re trying to quantify the degree to which our stock’s recruitment in year \\(y\\) is diverging from expectation. For that reason, we are interested in modeling the errors, or deviates, as lognormally distributed. Equation-wise, here’s what that looks like. Note that \\(R_{det}\\) is simply the deterministic value of recruitment, which could come from a Bev-Holt, Ricker, you name it.\n\\[\nR_{expected,y} = R_{det,y}exp(dev_y-\\sigma^2/2)\n\\]\nNow we’ll quickly refresh the idea behind the lognormal distribution and explain what’s up with that \\(-\\sigma^2/2\\).\n\n\nRefreshing the lognormal distribution.\nIf our devs are normally distributed with mean zero, \\(exp(\\mu = 0)\\) is lognormally distributed, shown in blue below:\n\n\n\n\n\nIf \\(X ~ N(\\mu,\\theta)\\) and \\(Y = e^X\\), the expected value of \\(X\\) is \\(\\mu\\), and the expected value of \\(Y\\) will be \\(e^{\\mu+\\theta/2}\\), not \\(e^{\\mu}\\).\nNote that \\(exp(X)\\) is 1) never less than zero and 2) pretty asymmetrical. What this means mathematically is that the mean of \\(exp(X)\\) is in fact not the same as \\(exp(mean(X))\\). Confirm this for yourself by testing the following. They’ve been added to the plots above as vertical dashed lines.\n\nmean(X) ## should be close to 0\n\n[1] 0.01049011\n\nexp(mean(X)) ## should be close to 1\n\n[1] 1.010545\n\nmean(exp(X)) ## uh-oh! It's way bigger...\n\n[1] 1.631989\n\nexp(mu+sigma/2) ##... in fact, the expected value of exp(X) is ~exp(mu+sigma/2)\n\n[1] 1.648721\n\n\n\n\nBias correction to the rescue\nBack to the recruitment example, we can pretend that \\(X\\) here is our vector of recruitment deviates. We set the mean for \\(dev_y\\) to 0 in hopes that, on an “average” year, we’ll be multiplying \\(R_{det,y}\\) (deterministic recruitment) by \\(exp(0) = 1\\), and get the expected value back. However, because we’ve discovered that \\(exp(dev_y)\\) is not symmetrical, we need to perform a bias correction to confirm that we approximately return the deterministic value when \\(dev_y\\) is zero. In other words, we are adjusting how we back-transform the deviates to confirm that the mean of those deviates is roughly one, on a normal scale.\nCheck it out:\n\n\n\n\n\n\nexp(mean(X)) ## should be close to 1, as above\n\n[1] 1.010545\n\nmean(exp(X-1^2/2)) ## with bias correction, closer to 1 again!\n\n[1] 0.9898515\n\n\nOne last thing. Normally we present a time series of rec-devs, and leave them in log space (to get around the scale issue I mentioned above).\nImagining we had a time series of data which encompassed all the different \\(SB\\) values corresponding to the gold points above, we could plot the deviates through time (I randomly reordered the values):\n\n\n\n\n\nSo now, the horizontal line at zero represents the deterministic or expected recruitment, and the points are the log deviations from that mean. Now you can interpret this plot intuitively, where values below zero were “worse than average years”, and vice versa. What’s more, because we are working in log space you can also get a quantitative sense of how divergent from expectation the recruits were in this year regardless of the stock size at hand."
  },
  {
    "objectID": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "href": "posts/2022-05-02-len-age-conv/2022-05-02-len-age-conv.html",
    "title": "Length-based Processes in an Age-Structured Model",
    "section": "",
    "text": "This is in the category of “stuff I thought I understood” until I didn’t.\nLet’s say you have an age-based model with integer-year bins a thru A+. The model is going to keep track of the dynamics of the fish population as they step through each age, but (annoyingly, perhaps) some of your modeled processes are length-based. This can occur when you are working with lots of different fleets, for which gear selectivity is better specified by length, or even the simple inclusion of our favorite \\(w_{l} = a l^b\\) calculation, which will require a notion of a fish length-at-age to play nicely within an age-structured model.\nConverting these length-based estimates (or quantities) into age-based estimates introduces a problem because of uncertainty in length-at-age, in other words, the fact that fish of length \\(l\\) could be of various ages \\(a\\). Let’s make up some data:\n\n## check out my post \"Why do we use ln(19)?\" for more info on this syntax:\nlogistic <- function(bin, b50, b75){\n  selage <- 1/(1+exp(-(log(15))*(bin-b50)/(b75-b50)))\n  return(selage)\n} \n\nage_bins <- 0:50\nlength_bins <- 1:65\nsigma_length <- 7.16\n\n## a logistic, length-based selectivity curve\nlength_selex <- sapply(length_bins, logistic, b50 = 30, b75 = 52)\n\nHere’s our selectivity curve; there’s a vertical line at l = 28cm which we’ll use for reference throughout this post.\n\n\n\n\n\nSo let’s say we’re interested in the selectivity at l = 28cm. In a length-based population dynamics model, we’d be set to just return the selectivity at l = 28cm for any fish within that respective bin (in 1-cm bins, fish lengths greater than 27cm and less than or equal to 28cm would all be assigned this value.)\nHowever, in an age-based model, we are instead interested in translating this curve into the expected selectivity-at-age. And given age \\(a\\), a fish could be a broad range of lengths \\(l\\), making it less clear which value we would “read off” the blue curve above. This variation is caused by the variable \\(\\sigma\\), which I’ve set at ~7cm – this reflects the standard deviation in length-at-age for our stock.\nHere’s how it looks in practice. First, we can generate our vonB-based expected length-at-age (here I assume \\(t_0\\) is 0, and am plotting the observed variation for 500 datasets, which should approximate the value for sigma_length specified above).\n\nvonb <- function(age, linf, kappa, sigma_length){\n  len <- linf*(1-exp(-kappa*(age)))+rnorm(1,0,sigma_length)\n}\n\n## simulate some datasets\nlength_at_age = matrix(NA, nrow = length(age_bins), ncol = 500)\nfor(i in 1:500){\n  length_at_age[,i] <- sapply(age_bins, vonb, linf = 44,\n                              kappa = 0.15, sigma_length)\n}\nlength_at_age <- bind_cols(age = age_bins, \n                           meanL = rowMeans(data.frame(length_at_age)),\n                           sdL = apply(data.frame(length_at_age),1,sd))%>%\n  mutate(lci = meanL-1.96*sdL, uci = meanL+1.96*sdL)\n\nggplot(length_at_age,\n       aes(x = age_bins)) +\n  geom_line(lwd = 1.1, col = 'grey', aes(y = meanL)) +\n  geom_ribbon(aes(ymin = lci, ymax = uci), fill = alpha('grey',0.3))+\n  geom_hline(yintercept = 28, linetype = 'dotted')+\n  labs(x = 'age', y= 'length')\n\n\n\n\nNotice how the horizontal line at 28cm corresponds to multiple ages within the confidence interval? How do we account for the fact that there is a chance that the the selectivity applicable to a given age can be characterized by the selectivity at several lengths, and how do we weight the applicability of that selectivity value given what we know about growth? Enter \\(\\psi_{l,a}\\), the probability of being in length bin \\(l\\) at age \\(a\\), a.k.a. the normal distribution \\(\\phi\\) of size-at-age.\n\\[\n\\begin{align}\n\\psi_{al} &= \\begin{array}\n   & \\Phi (1,\\tilde l_{a}, \\sigma)) & if \\space l = 0cm \\\\\n   \\Phi (l+1,\\tilde l_{a}, \\sigma)) - \\Phi (l,\\tilde l, \\sigma))& if \\space 0cm, < l < L+ \\\\\n   1-\\Phi (l,\\tilde l_{a}, \\sigma)) & if \\space l = L+\\\\\n\\end{array}\n\\end{align}\n\\]\nWhere \\(l\\) is the length bin in question, \\(\\tilde l\\) is the observed length-at-age, and \\(\\sigma\\) is the variation in growth. If we simply used a “lookup” method (which I have certainly done in a pinch!), and assigned the selectivity at age \\(a\\) to the selectivity at the expected length at age \\(a\\), we’d be introducing bias.\n\n## this function returns the distribution of expected length-at-age \n## given a bin and observed length-at-age\ngetP_al <- function(len_bin, len_obs, length_sigma){\n  pal = NA\n  if(len_bin == 1){\n    pal <- pnorm(len_bin, len_obs, length_sigma)\n  } else if (len_bin > min(length_bins) & len_bin < max(length_bins)){\n        pal <- pnorm(len_bin+1, len_obs, length_sigma) - \n          pnorm(len_bin, len_obs, length_sigma)\n  } else{\n        pal <- 1-pnorm(len_bin, len_obs, length_sigma)\n  }\n  pal\n}\nPAL <- matrix(NA, nrow = length(age_bins), ncol =  length(length_bins))\nfor(a in age_bins){\n  for(l in length_bins){\n    ## instead of using means, could re-do vonB with error turned off\n    PAL[a,l] <- getP_al(len_bin=l,len_obs=length_at_age$meanL[a],sigma_length )\n  }\n}\nlongPal <- data.frame(PAL) %>%\n  mutate(age_bins) %>%\n  reshape2::melt(id = 'age_bins') %>% \n  mutate(len =as.numeric(gsub(\"X\",\"\",variable))) \n\nHere’s a subset of what \\(\\psi_{al}\\) looks like for ages 5-10. Notice (again) how length = 28cm crosses multiple age curves. The good news is, \\(\\psi_{al}\\) has given us a sense of the relative probability of being age \\(a\\) given length \\(l\\). (It looks like \\(l = 28cm\\) is most likely a fish of age 7 or 8).\n\n\n\n\n\nTo put this all together, we need to compute the dot-product of the selectivity-at-length and the expected distribution of length-at-age.\nIn math, we’re doing this: \\[\nSel_a = \\sum_l Sel_l \\cdot \\psi_{al}\n\\]\nWhich is really just a cheap way of approximating the correct estimator of the expected selectivity of an animal at age \\(a\\):\n\\[\nSel_a = \\int_L Sel_L  \\psi_{aL} dL\n\\]\nIn words: for each length bin \\(l\\), multiply the selectivity at that length times the probability of being at age \\(a\\) given length \\(l\\). Sum these quantities across all length bins for each age bin.\nIn pictures: let’s say we are interested in \\(a = 5\\). We are going to multiply each point on the length-selectivity by its corresponding point (by color and ID number) on the probability of length-at-age-5 plot, then sum the results.\n\n\n\n\n\n\n\n\n\n\nIn code (still using \\(a = 5\\) as an example):\n\nsel_age <- merge(longPal,data.frame(cbind(length_selex,length_bins)),\n                 by.x = 'len', by.y = 'length_bins') %>%\n  ## inner product for each age-length combo\n  mutate(prod1 = length_selex*value) %>% \n  group_by(age_bins, len) %>%\n  summarise(dotProd = sum(prod1)) %>%\n  ungroup() %>%\n  group_by(age_bins) %>%\n  summarise(dotProdsum = sum(dotProd))\n\nggplot(sel_age,\n       aes(x = age_bins, y = dotProdsum)) +\n  geom_line(lwd = 1.1, col = 'grey22') +\n  labs(x = 'age', y= 'proportion selected', \n       main = 'Converted from length-at-age')"
  },
  {
    "objectID": "posts/2020-01-01-quals/quals.html",
    "href": "posts/2020-01-01-quals/quals.html",
    "title": "How I studied for my PhD qualifying exam",
    "section": "",
    "text": "Yay, I got a high pass on my exam (now back to the things I neglected Autumn quarter). Before I disappear, I wanted to share how I went about studying for my exam, in hopes this can help other PhD students. Disclaimer: this is likely UW/SAFS specific, and the exam is a 5-day written format where each committee member gave me a set of readings and/or topics from which I was supposed to prepare to answer any long-format question they could give me in a single day.\n\nAcquire materials early. I actually got my readings over the summer, well before I knew exactly when I was taking the exam. This gave me a birds-eye view of the magnitude of the readings and helped me triage how much time I’d spend on each. For example, my advisor assigned two textbooks – I started reading these on the bus so when crunch-time came I was really reading them for the 2nd or 3rd time.\nMake a weekly gameplan. This is extremely valuable. I set my exam date about 2.5 months out, and drew up a timetable of what readings or topics I was to accomplish each week, with the final two weeks dedicated to review.\nHave a hardcore paper-organization system. If you can do this with printed sheets, I’m in awe. Otherwise, especially since I was required to synthesize and cite papers on the fly, I developed three digital tracking regimes for my readings. Three sounds like a lot, but I found using the following method enabled me to read each paper multiple times, extract the valuable information from it, and have pre-written summaries ready to go come exam day. The basic system was this:\n\nMendeley (any bib software should do): Load in all the readings, sorted into folders based on which committee member assigned it. This gives you a gestalt for the topics they’re leaning towards. Inside the software, I will read, highlight, and give brief annotations to the text itself. If there are equations or figures, I will paste a comment and summarize what is being said in my own words.\nExcel (see my How I keep up with the Literature post). I continued my table-style tracking, but included a column for keywords that I thought may come up in the exam. For example, one comm. member gave me readings about catch-only methods, MSY, data-poor methods, and global fisheries; each of these became a keyword flag so I could sort the citations quickly depending on the question. Other columns included “claim”, “methods”, “major takeaways” and “larger scale conclusions”, where I connected that paper’s findings with others in that subsection.\nWord. This is where the magic happened. For each committee member, I would make up some generalized questions and try to answer them in a loose, outline style referring to my Excel sheet. I normally did this a day or two after my first Mendeley-Excel pass of the articles, which meant I got to do a second reading of the papers and ensured I understood them enough to comment intelligently. Good questions are things that would open a review paper or Op-Ed, not things like “What did Sekkar et al. say about X?”. Synthetic questions, such as “what have we learned about the influence of X on Y” or “Are Z considered good metrics of K?” are fruitful and will allow you to connect the readings.\n\nGo beyond. One of the best things I did during the exam prep was take a free, online Coursera course in Bayesian statistics. In add’n to the Bayes textbook I was assigned, the course let me get “tutored” in a condensed, low-pressure way AND the quizzes helped me test my knowledge. It wouldn’t have been realistic for me to take a formal Bayes class in the stats department this quarter (midterms and all). It’s easy to fool yourself that you “get it” after highlighting a textbook (for which the previous steps aren’t as applicable), but MOOCs like Coursera enable you to test yourself in real time.\nPace thyself. I read a quote last fall that said, “you can either contribute to the literature, or keep up with the literature”. Some days I would be deep in my Excel-Word mode for 9+ hours, and feel strangely like I hadn’t really “accomplished” anything, but this thought must be banished. Firstly, having a foundation in the literature of your field is something you’ll always need, and if you don’t do it now, you won’t have any more time when gainfully employed."
  },
  {
    "objectID": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "href": "posts/2022-12-10-leveraging-testthat-for-my-mse-simulations/2022-12-10-tryagain.html",
    "title": "Leveraging try_again() for error catching in MSEs",
    "section": "",
    "text": "This post is not a full breakdown of how to structure code for Management Strategy Evaluation (MSE) work. I’m certainly not a pro software engineer. Instead, I was pleased to find a clean way to deal with errors in the hierarchical nature of my MSE workflow that may be of use to others, or to future-me.\nHere’s a schematic of what’s happening in my MSE:\n\n\n\n\n\nAs anyone who’s done this type of work before knows, we definitely expect a subset of our simulation runs to ‘fail’ - that is, for the unique simulated data set (which differs here based upon what I’m calling \\(r\\))in year \\(y\\), the estimation procedure doesn’t pass whatever criteria we’ve set (could be convergence, a NaN gradient, etc).\nA simple break won’t help us here, since that would kill the loop mid-run and require us to manually re-start the higher-level function. A tryCatch also seems appealing, but this would still require recursion (since we’d have to embded a call to run_MSE() in the Catch wrapper, within run_MSE() itself…not ideal).\nOur desired behavior would have us hit play once on the run_MSE() function, and trust that any estimation model that fails will be safely handled:\n\n\n\n\n\nI was able to accomplish this elegantly using testthat::try_again(). I simply indicated the number of times I wanted to attempt my run_MSE() function, and included a stop() argument inside check_model() (so that a true error is thrown when the estimator fails).\nWhen testthat::try_again() detects this failure, it restarts the run_MSE() function, which increments along a vector of random seeds, until it hits something functional. This can be adapted to lots of purposes, and one could readily keep track of the identity/quantity of estimators that failed.\nEven better, I can pop this into a one-line call to replicate, which functionally requests a total number of sucesses equal to nReplicates. A key for this to work is to ensure whatever will be incremented along for each call to run_MSE() (in my case, the vector of seeds) is long enough to account for every attempted replicate times the potential number of tries. In the example below, this would be 99 tries x 100 nReplicates = 9900 seeds.\nsim <- replicate(nReplicates, {try_again(99,run_MSE(nprojyrs, spaceID))})"
  },
  {
    "objectID": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "href": "posts/2021-02-10-how-i-keep-up-with-the-literature/2021-02-10-literature.html",
    "title": "How I Keep Up With the Literature",
    "section": "",
    "text": "My lab recently had a meeting where we discussed the nuts & bolts of performing a peer-review for a journal article, which got everyone talking about their preferred methods of reading a scientific paper. Luckily, we aren’t in bio-medicine or particle physics, but the volume of articles that emerge monthly in our field is still large, and it takes strategy to efficiently extract and scrutinize the information from a scientific article.\nOne of my favorite specimens of PhD advice comes from the http://www.chemistry-blog.com/wp-content/uploads/2010/06/Gassman-and-Meyers.pdf”, which posess the perfect melange of condecision, tough love and “kids these days” angst – the advice, nonetheless, is great. It sticks because it stings a bit. Gassman writes:\n\nIt is a consistent observation on my part that people have more ideas about their research when they are writing their thesis than at any other time. This is generally due to the fact that they are finally doing the literature work they should have done early in their thesis work only at the time that they are trying to complete their dissertations. – P.G. Gassman\n\nSo, don’t let that be you. Here is how I approach the literature, which has proved helpful for the last few years.\n\n\nHave a method to centralize new articles. I used the tips on the https://fraserlab.com/2013/09/28/The-Fraser-Lab-method-of-following-the-scientific-literature/ website to set up a Feedly account (there are lots of other options) and subscribed to the four main journals in my field, plus Science, Nature, and PNAS. The algorithm learns your preferences quite quickly, and your queue will show articles that fit your niche after you ‘check’ them off.\n\n\nCheck your centralizer every morning, and save the articles whose abstracts seem relevant as soon as you see them. It’s fine to save articles on-the-fly, but I recommend scheduling ‘deep reading’ periods of no less than one hour.\n\n\nGo beyond highlighting/sticky notes. I use a set of spreadsheets, stored in Google Drive and sorted by topic, to ensure that I’m actually engaged with what I’m reading. They all follow the same format. One example comes from my spreadsheet for studies of spatial model structure and general model mis-specification. The headings are hard to read in the screenshot, but they are Citation, Species, Mis-specification, Methods, Performance Measures, Conclusions, Similarities and Differences. These can be tailored to your field or project, but the last two are the most crucial. In those columns, I compare the study to my thesis work, making it easy as I write my introductions/discussions to state who has done similar studies, and where the research gaps lie."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html",
    "title": "Ageing Error Matrices - some notes",
    "section": "",
    "text": "I revisit the inclusion of ageing error matrices frequently enough that I wanted to consolidate some notes here. None of this is new information; it is a resource for myself to get things straight, particularly when moving between the Stock Synthesis ageing error syntax and other setups like those used in our bespoke ADMB/TMB models. For this post, \\(a\\) will index true ages, \\(\\tilde a\\) indexes read ages.\nIn this post, I will talk through the math behind the ageing error matrix, and show how one can develop such a matrix (with R code) given parameters concerning the bias and imprecision of their age reads."
  },
  {
    "objectID": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "href": "posts/2023-06-08-ageingError/2023-06-08-ageingError.html#populating-the-matrix",
    "title": "Ageing Error Matrices - some notes",
    "section": "Populating the Matrix",
    "text": "Populating the Matrix\nContinuing our example, let’s pass the bias and imprecision vectors we established above to see how \\(\\mathbf P\\) responds. I’m filling each element of \\(\\mathbf P\\) one at a time. Sometimes you will need to make an assumption about the first age group to avoid a zero.\n\n\n\nYou’ll want to sanity check that the columns sum to one; in other words, are all possible true ages \\(a\\) accounted for under each read age \\(\\tilde a\\)? Normalization, and occasionally some rounding to several decimals, are reasonable steps here.\n\nfor(a_obs in 1:abins){\n  P_aa[,a_obs] <- P_aa[,a_obs]/sum(P_aa[,a_obs])\n}\nall(round(colSums(P_aa))==1)\n\n[1] TRUE\n\n\nNow let’s confirm that the shape of the probabilities looks reasonable. Generally, I expect to see what I call a “slumpy worm”: values with higher, narrow peaks at early ages, slumping to flatter, shallower peaks at older ages. The slumpy worm corresponds to the notion that it’s more likely for a reader to get a precise read on a young fish (it’s easy to count a small handful of annuli), whereas it’s harder to age an older fish (many annuli tend to get blurry and bunched up, but **check with your life history program!*).\nThere should be a spike at the ends for the plus group and age-0, if applicable.\n\n\n\n\n\nAnd here’s what the actual populated matrix looks like:"
  }
]