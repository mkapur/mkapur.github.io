---
title: The Simulation Pinball Machine
author: ~
date: '2022-07-20'
slug: simulation-pinball
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2022-07-20T15:36:27-07:00'
featured: false
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
math: true
---

```{r, setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "static")
options(knitr.table.format = function() {
  if (knitr::is_latex_output()) 'latex' else 'pandoc'
})
``` 

# The pinball machine
Simulation is the bread-and-butter of my worklife. I use it to create data for research questions, to explore the behavior of the models I build to assess fishery populations, and to convince myself whether certain truisms are really true. I don't hope to give a full review of fisheries simulation here, but wanted to make a note about how two of the key "switches" we regularly use within the simulation context to represent process error and fishing impacts interact.

Specifically, I think of a well-built simulation like a pinball machine: the ball is your state-space self running downhill (through time), and the various switches/lights/buttons are all the parameters and model structures that you've established in hopes of shooting that ball on a nice, high-scoring trajectory. A good simulation will reliably 1) follow the same trajectory each time things are run the same way, and 2) only vary among trajectories in the manner you've asked it to.

This is easier said than done, if you haven't given careful thought to how to represent uncertainty in your simulation framework. There also is a nuanced difference between the type of simulation you'd build for *forecasting* (aka projecting) a population, versus *bootstrapping* (or "resampling") a historical population. There's also what I consider a hybrid of the two, where you will use the data & parameters corresponding to a historical population to simulate a new, historical trajectory, which is otherwise independent of the original data.

This post concerns the fisheries scientest more-or-less fluent in Stock Synthesis but also eager to generate their own simulated pouplations (i.e., in TMB). I will admit up-front that I have not much experience with `ss3sim` but will be discussing some principles of Synthesis population generation here. Please tag me on Twitter or email me below if you spot any gross misrepresentations.

# Background Reading
Some helpful and can't-miss resources on this topic are the following:

Methot, R.D., Wetzel, C.R., 2013. Stock synthesis: A biological and statistical framework for fish stock assessment and fishery management. Fish. Res. 142, 86â€“99. https://doi.org/10.1016/j.fishres.2012.10.012, **Particularly** the section on p94 regarding the bootstrap routine. This paragraph makes it clear that if you're invoking the bootstrap functionality within SS (where a new data file gets written at the bottom of `data.ss_new`), these bootstrapped data series are dependent on the *expected* values from your original assessment model. This is done purposefully, to not propagate data conflicts, autocorrelations, and/or residual patterns in your original data into the new simulated data. However, it's pretty important that your original model actually fits the data pretty well, if you expect to turn around and use that original model to do some sort of re-estimation on the bootstrapped data. Related to the discussion below, this paper mentions that folks will often combine the SS3 Bootstrap routine with other sampling methods to introduce both process and observation error into their simulations (see below).


**This [simple](https://www.google.com/search?q=tmb+simulate&rlz=1C1CHBF_enUS814US814&oq=tmb+simulate&aqs=chrome..69i57j69i64.2880j0j7&sourceid=chrome&ie=UTF-8) overview** of `TMB::simulate()`, where the authors remind us the effects of estimation error even in a dead-simple linear regression model. This is why a perfectly-specified estimation model is not going to exactly recover your index of abundance! Rick himself has a few thoughts about why the distribution of parameter estimates from bootstrapped models is narrower than one might expect from the original data on the vLab site [here](https://vlab.noaa.gov/web/stock-synthesis/public-forums/-/message_boards/message/5517207).

Finally, the old-ish paper that inspired this blog post:
Magnusson, A., Punt, A.E. and Hilborn, R. (2013), Measuring uncertainty in fisheries stock assessment: the delta method, bootstrap, and MCMC. Fish and Fisheries, 14: 325-342. https://doi.org/10.1111/j.1467-2979.2012.00473.x. This paper does a great job talking through how to represent uncertainty in simulated data. Figure 2 in this paper is what got me thinking about the various buttons of the pinball machine, and how changing them can provide subtly different outcomes in your simulated populations.

# Quick refresher/glossary

Important to our discussion today, especially in the context of state-space model simulation, are the ideas of *process* vs *observation* error. My non-technical way of thinking about this is a paraphrase of Andre's explanation: 

*Process error* is the vague, multi-forced, often unexplained system variability that makes the world diverse, stochastic and beautiful. The fact that some recruitment years are big and others are small -- the basic idea behind [recruitment deviates](https://maiakapur.netlify.app/post/recdevs/how-rec-devs/) -- is often equated with process error. It's random (but might follow an underlying distribution). **Process error is the variation you would see if you could observe the system perfectly**.

Observation error is basically the goofs introduced by your faulty instruments, your sleepy technician, your failings as a mortal human. It's the discrepancy between the truth in the water and what you write down in your lab notebook because of your inability to take perfect measurements; this is a simple fact of reality (see: Uncertainty Principle). **Observation error is the variation obtained as a result of imperfect tools**.

Other key words I'm using today:

**Simulation**: Inventing data in a systematic way, using pre-set functional forms and statistical distributions to represent a set of natural processes and the collection of information from them.

**Forecasting/Projecting**: Taking information (parameters, time series) from an existing model (such as an assessment model), and marching that population forward in time. Requires, at minimum, some idea of how that population will reproduce (recruitment deviates) and die (natural mortality, and perhaps catches) in the future.

**Bootstrapping**: Fisheries folks use this somewhat loosely to refer to the process of generating new datasets, typically over a historical time period, from an existing model. Generally, these approaches look at your parameter distributions, the expected values from your datasets, and how you've specified error within them to create a new batch of datasets and the accompanying population. This is subtly different from forecasting, because we don't necessarily need to tell the model anything about the future; it can instead use the historical recruitment deviates and catch information to drive the population -- but more on this shortly.

# Le Confusion
Let's take a look at Figure 2 from Magnusson et. al. The text explains that these populations were simulating using totally random recruitment deviations, which you can see on the top-right:

```{r, echo=FALSE, fig.cap="Magnusson et. al. 2013", out.width = '100%'}
knitr::include_graphics("C:/Users/mkapur/Dropbox/other projects/mkapur.github.io/content/post/2022-07-20-simulation/fig2.png")
```

Simulating recruitment deviates, either for the past or future, from a lognormal distribution with $\sigma_r$ from your base model is a good call. This is how we introduce *process error* into the system, by pretending that the ups & downs in population productivity happen at random, with a certain level of spread. (Technically, a correct way to do this is to simulate $\tilde R_y \sim N(-\sigma_R^2/2,\sigma_R))$ so that the resultant vector is bias-corrected). Lots of cool studies have been and will be done by examining what happens if that process error (those rec-devs) are instead influenced by, say, a temperature index. But anyways, Arni's paper does it the random way.

Now, if you had set up a random cloud of recruitment deviates, how would you expect the SSB trajectory to look? ([Phd Generals](https://maiakapur.netlify.app/post/generals/how-studied-general/) question alert). You might be inclined to say "it would depend on the catches". That is true in an absolute sense, and thinking carefully about this dependency will help you understand the different ways of setting up your simulation to do what you'd like. 

I titled this section "Le Confusion" because an inattentive viewer of Fig. 2 (cough) might not get why the SSB trajectory on the bottom-right looks like a smooth, bendy trend, given the fact that the rec devs are a random cloud. Shouldn't it be a cloud as well?

After all (brute coding ahead):

```{r, echo=TRUE}
## population setup
sigmaR <- 0.4
init_pop <- 1000; 
rec_mult <- 0.2 ## let's pretend 20% of the population reproduces
natM <- 0.2

## make fifty, 50-year population trajectories
pop_big <- matrix(NA, nrow = 50, ncol = 50)
for(i in 1:50){
  rec_devs <- rnorm(50,mean = -sigmaR^2/2,sigmaR) ## simulate our devs as a cloud
  pop = NULL; pop[1] <- init_pop ## store a population with initial size 1000
  for(y in 2:50){
    recruits <- exp(rec_devs[y])*pop[y-1]*rec_mult## in lieu of an SRR, just a simple function of current biomass
    pop[y] <- exp(-natM)*(pop[y-1]+recruits) ## pretending they die after spawning
  } ## end years
  pop_big[,i] <- pop
} ## end pop replicate

## do the same thing but without recdevs
pop_big_zero <- matrix(NA, nrow = 50, ncol = 50)
for(i in 1:50){
  rec_devs <- rep(0, 50)
  pop = NULL; pop[1] <- init_pop 
  for(y in 2:50){
    recruits <- exp(rec_devs[y])*pop[y-1]*rec_mult
    pop[y] <- exp(-natM)*(pop[y-1]+recruits) 
  } ## end years
  pop_big_zero[,i] <- pop
} ## end pop replicate
 

```

Like we'd expect, the rec-devs introduce variety both through time and among population replicates:

```{r, echo=F, fig.cap="Simulated Populations with random rec-devs (left) or rec devs = 0 (right)", out.width = '100%'}
par(mfrow = c(1,2))
plot(pop_big[,1], type = 'l', ylim = c(0,max(pop_big)), xlab= 'year',ylab = 'simulated population size'); for(i in 2:50)lines(pop_big[,i], col = gray(i/50))

plot(pop_big_zero[,1], type = 'l', ylim = c(0,max(pop_big)), xlab= 'year',ylab = 'simulated population size')
```
That looks pretty different from the figure in the paper -- and you might suspect it has something to do with fishery removals. So let's introduce fishery removals in a careful way.
First we can specify a time series of known catches. This is probably what you have within your basic Stock Synthesis model you'd like to simulate with, or in the `data` list passed to your TMB objective function. When you invoke `TMB::SIMULATE()` given a rec-dev vector and known catches, or ask SS to bootstrap a population with a fixed rec-dev vector, this is what you're getting. Notice how below I back out the $F$ rate in order to realize the catches exactly.

```{r, echo=T}
## build the catches
known_catch <- c(sort(runif(25, 1,50 )), sort(runif(25, 1, 40), decreasing = T))/5

## make fifty, 50-year population trajectories
pop_big <-rec_devs <-  Frates <- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] <- rnorm(50,mean = -sigmaR^2/2,sigmaR) 
  
  pop[1] <- init_pop 
  Frates[1,] <-  known_catch[1]/pop[1]
  for(y in 2:50){
    Frates[y,i] = known_catch[y-1]/pop[y-1]
    recruits <- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] <- exp(-(natM))*(pop[y-1]+recruits)*(1- Frates[y,i]) 
  } ## end years
  pop_big[,i] <- pop
} ## end pop replicate

```

```{r, echo=FALSE, fig.cap="Simulated population with catches known exactly", out.width = '100%'}
par(mfrow = c(2,2))
plot(Frates[,1], type = 'l', ylim = c(0,max(Frates)), xlab= 'year',ylab = 'F Rates'); for(i in 2:50)lines(Frates[,i], col = gray(i/50))
plot(rec_devs[,1], ylim = c(-6,6), type = 'l', xlab= 'year',ylab = 'log recruitment deviate'); for(i in 2:50)lines(rec_devs[,i], col = gray(i/50))
plot(known_catch, xlab = 'year', ylab = 'known catches', type = 'l')
plot(pop_big[,1], type = 'l', ylim = c(0,max(pop_big)), xlab= 'year',ylab = 'simulated population size'); for(i in 2:50)lines(pop_big[,i], col = gray(i/50))

```


# Le Difference

The population size trends from the above example do decline, but they are all over the place! (They don't go up and down systematically). It's responding to your random rec dev vector you created, and the underlying harvest rate which *adjusts* to take the known catches from your population. **If you pass a random vector of recruitment devs to Stock Synthesis with a time series of historical catch, the SSB timeseries derived from your bootstrapped dataset will vary along with your devs**. It won't look like your original population series, and might be quite high/low in given years. This could be just what you want! But it's not what's in Arni's paper, which is distinct because there the $F$s are instead known exactly, and landings are allowed to vary. Let's show what happens with that small change, noting how much less variability you see in the SSB trend:

```{r, echo = T}
## specify the Fs -- 
known_F <- known_catch/apply(pop_big,1,mean)

## make fifty, 50-year population trajectories
pop_big <-rec_devs <-  catches<- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] <- rnorm(50,mean = -sigmaR/2,sigmaR) 
  
  pop[1] <- init_pop 
  catches[1,] <-  known_F[1]*init_pop
  for(y in 2:50){
    recruits <- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] <- exp(-(natM))*(pop[y-1]+recruits)*(1-known_F[y])
    catches[y,i] <- known_F[y]*(pop[y-1]+recruits)
  } ## end years
  pop_big[,i] <- pop
} ## end pop replicate

```


```{r, echo=FALSE, fig.cap="Simulated population with F known exactly", out.width = '100%'}
par(mfrow = c(2,2))
plot(known_F , type = 'l',  xlab= 'year',ylab = 'F Rates') 
plot(rec_devs[,1], ylim = c(-6,6), type = 'l', xlab= 'year',ylab = 'log recruitment deviate'); for(i in 2:50)lines(rec_devs[,i], col = gray(i/50))
plot(catches[,1],  ylim = c(0,max(catches)), type = 'l', xlab= 'year',ylab = 'simulated catches'); for(i in 2:50)lines(catches[,i], col = gray(i/50))
plot(pop_big[,1], type = 'l', ylim = c(0,max(pop_big)), xlab= 'year',ylab = 'simulated population size'); for(i in 2:50)lines(pop_big[,i], col = gray(i/50))

```


# Le workaround

This is all well and good -- but what if you (like me) are on a time crunch, perhaps aren't super familiar with [`ss3sim`](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0092725), and/or don't have time to code up a version of your model which accepts fixed inputs for $F$ (instead of solving for them)? In the Synthesis framework, to my knowledge, we're pretty much stuck with treating the vector of catches in the `dat` file as known, so the model $F$s are indeed going to vary with each population replicate. A way to generate a smooth band a la Magnusson at al is to  pass a random vector of recruitment deviates that are instead **centered at the annual values from the original model**.



```{r, echo=T}
rec_dev_mean <-  rnorm(50,mean = -sigmaR^2/2,sigmaR) 
## make fifty, 50-year population trajectories
pop_big <-rec_devs <-  catches<- matrix(NA, nrow = 50, ncol = 50);  
for(i in 1:50){
  pop = NULL
  rec_devs[,i] <- rnorm(50,mean = rec_dev_mean-sigmaR^2/2,sigmaR) ## devs are now mean-centered, still bias corrected
  
  pop[1] <- init_pop 
  Frates[1,] <- known_catch/init_pop
  for(y in 2:50){
    Frates[y,i] = known_catch[y-1]/pop[y-1]
    recruits <- exp(rec_devs[y,i])*pop[y-1]*rec_mult
    pop[y] <- exp(-(natM))*(pop[y-1]+recruits)*(1- Frates[y,i])
  } ## end years
  pop_big[,i] <- pop
} ## end pop replicate

```


```{r, echo=FALSE, include = TRUE, fig.cap="Simulated population with recdevs recentered", out.width = '100%'}
par(mfrow = c(2,2))
plot(Frates[,1], type = 'l', ylim = c(0,max(Frates)), xlab= 'year',ylab = 'F Rates'); for(i in 2:50)lines(Frates[,i], col = gray(i/50))
plot(rec_devs[,1], ylim = c(-3,3), type = 'l', xlab= 'year',ylab = 'log recruitment deviate'); for(i in 2:50)lines(rec_devs[,i], col = gray(i/50))
plot(known_catch, xlab = 'year', ylab = 'known catches', type = 'l')
plot(pop_big[,1], type = 'l', ylim = c(0,max(pop_big)), xlab= 'year',ylab = 'simulated population size'); for(i in 2:50)lines(pop_big[,i], col = gray(i/50))

```
Now we're able to realize the known catches exactly while preserving the historical population trend to a greater degree than before. Do use this with caution; you are treating the model outputs (recruitment devs from your original assessment) as a new input, so inheriting any trends in the process error your original model might have missed.
