---
title: Leveraging try_again() for error catching in MSEs
author: ~
date: '2022-12-10'
slug: testthat-mse
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2022-12-10T15:36:27-07:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>This post is not a full breakdown of how to structure code for Management Strategy Evaluation (MSE) work. I’m certainly not a pro software engineer. Instead, I was pleased to find a clean way to deal with errors in the hierarchical nature of my MSE workflow that may be of use to others, or to future-me.</p>
<p>Here’s a schematic of what’s happening in my MSE:
<img src="mse_schematic.jpg" width="960" /></p>
<p>As anyone who’s done this type of work before knows, we definitely expect a subset of our simulation runs to ‘fail’ - that is, for the unique simulated data set (which differs here based upon what I’m calling <span class="math inline">\(r\)</span>)in year <span class="math inline">\(y\)</span>, the estimation procedure doesn’t pass whatever criteria we’ve set (could be convergence, a <code>NaN</code> gradient, etc).</p>
<p>A simple <code>break</code> won’t help us here, since that would kill the loop mid-run and require us to manually re-start the higher-level function. A <code>tryCatch</code> also seems appealing, but this would still require recursion (since we’d have to embded a call to <code>run_MSE()</code> in the <code>Catch</code> wrapper, within <code>run_MSE()</code> itself…not ideal).</p>
<p>Our desired behavior would have us hit play once on the <code>run_MSE()</code> function, and trust that any estimation model that fails will be safely handled:</p>
<pre class="r"><code>knitr::include_graphics(&#39;C:/users/mkapur/dropbox/other projects/mkapur.github.io/content/post/2022-12-10-leveraging-testthat-for-my-mse-simulations/mse_schematic2.jpg&#39;)</code></pre>
<p><img src="mse_schematic2.jpg" width="960" /></p>
<p>I was able to accomplish this elegantly using <code>testthat::try_again()</code>. I simply indicated the number of times I wanted to attempt my <code>run_MSE()</code> function, and included a <code>stop()</code> argument inside <code>check_model()</code> (so that a true error is thrown when the estimator fails).</p>
<p>When <code>testthat::try_again()</code> detects this failure, it restarts the <code>run_MSE()</code> function, which increments along a vector of random seeds, until it hits something functional. This can be adapted to lots of purposes, and one could readily keep track of the identity/quantity of estimators that failed.</p>
<p>Even better, I can pop this into a one-line call to <code>replicate</code>, which functionally requests a total number of sucesses equal to <code>nReplicates</code>. A key for this to work is to ensure whatever will be incremented along for each call to <code>run_MSE()</code> (in my case, the vector of seeds) is long enough to account for every attempted replicate times the potential number of tries. In the example below, this would be 99 tries x 100 <code>nReplicates</code> = 9900 seeds.</p>
<p><code>sim &lt;- replicate(nReplicates, {try_again(99,run_MSE(nprojyrs, spaceID))})</code></p>
